{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Big Data Aplicado - CE IABD - IES Gran Capit\u00e1n","text":""},{"location":"index.html#indice","title":"\u00cdndice","text":"<ol> <li>Gesti\u00f3n de Soluciones I</li> <li>Gesti\u00f3n de Soluciones II</li> <li>Gesti\u00f3n de Soluciones III</li> <li>Gesti\u00f3n de Soluciones IV</li> <li>Introducci\u00f3n a la computaci\u00f3n paralela y distribuida</li> <li>Apache Hadoop</li> </ol>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones%20I/index.html","title":"UD 1 - Gesti\u00f3n de Soluciones","text":"<p>ver presentaci\u00f3n BDA1.1</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones%20I/index.html#11-introduccion-de-los-datos-al-conocimiento","title":"1.1. Introducci\u00f3n de los datos al conocimiento","text":"<p>El dato es una representaci\u00f3n sint\u00e1ctica, generalmente num\u00e9rica, que puede manejar un dispositivo electr\u00f3nico - normalmente un ordenador - sin significado por s\u00ed solo. Sin embargo, el dato es a su vez el ingrediente fundamental y el elemento de entrada necesario en cualquier sistema y/o proceso que pretenda extraer informaci\u00f3n o conocimiento sobre un dominio determinado. En este sentido, 7 es un dato, como tambi\u00e9n lo es \u03c0 o como son los t\u00e9rminos aprobado o suspenso.</p> <p>Por su parte, la informaci\u00f3n es el dato interpretado, es decir, el dato con significado. Para obtener informaci\u00f3n, ha sido necesario un proceso en el que, a partir de un dato como elemento de entrada, se realice una interpretaci\u00f3n de ese dato que permita obtener su significado, es decir, informaci\u00f3n a partir de \u00e9l. La informaci\u00f3n es tambi\u00e9n el elemento de entrada y de salida en cualquier proceso de toma de decisiones. Partiendo de los datos del ejemplo anterior, informaci\u00f3n obtenida a partir de los mismos puede ser: El 7 es un n\u00famero primo, \u03c0 es una constante cuyo valor es 3, 141592653..., Mar\u00eda ha aprobado el examen de conducir, Pablo est\u00e1 suspenso en matem\u00e1ticas.</p> <p>A partir de informaci\u00f3n, es posible construir conocimiento. El conocimiento es informaci\u00f3n aprendida, que se traduce a su vez en reglas, asociaciones, algoritmos, etc. que permiten resolver el proceso de toma de decisiones. As\u00ed pues, la informaci\u00f3n obtenida a partir de los datos permite generar conocimiento, es decir, aprender. El conocimiento no es est\u00e1tico, como tampoco lo es siempre el aprendizaje. Aprender, construir conocimiento, implica necesariamente contrastar y validar el conocimiento construido con nueva informaci\u00f3n que permita, a su vez, guiar el aprendizaje y construir conocimiento nuevo. Siguiendo con los ejemplos anteriores, el conocimiento que permite obtener que el 7 es un n\u00famero primo puede ser el algoritmo de Erat\u00f3stenes. Por otra parte, el conocimiento que permite obtener el valor del n\u00famero \u03c0 puede extraerse de los resultados de los trabajos de Jones, Euler o Arqu\u00edmedes, mientras que el aprobado de Mar\u00eda en el examen de conducir y el suspenso de Pablo en matem\u00e1ticas, se pueden obtener de la regla que en una escala de diez asigna el aprobado a notas mayores o iguales que 5 y el suspenso a notas menores.</p> <p></p> <p>Figura 1: Relaci\u00f3n entre datos, informaci\u00f3n y conocimiento en el proceso de toma de decisiones</p> <p>Por tanto, datos, informaci\u00f3n y conocimiento est\u00e1n estrechamente relacionados entre s\u00ed y dirigen cualquier proceso de toma de decisiones  La figura1.1 muestra la relaci\u00f3n entre datos, informaci\u00f3n y conocimiento, en un proceso gen\u00e9rico de toma de decisiones. M\u00e1s concretamente, en el ejemplo del suspenso de Pablo en matem\u00e1ticas, el proceso de toma de decisi\u00f3n acerca de la calificaci\u00f3n de Pablo se estructurar\u00eda de la siguiente forma:</p> <ol> <li> <p>El profesor corrige el examen de Pablo, que ha sacado un 3. Esta calificaci\u00f3n, por s\u00ed sola, es simplemente un dato.</p> </li> <li> <p>A continuaci\u00f3n, el profesor calcula la calificaci\u00f3n final de Pablo, en base a la nota del examen, sus trabajos y pr\u00e1cticas de laboratorio. La nota final de Pablo es un 4. Esto \u00faltimo es informaci\u00f3n.</p> </li> <li> <p>\u00bfHa aprobado Pablo? La informaci\u00f3n de entrada al proceso de decisi\u00f3n es su calificaci\u00f3n final de 4 puntos, obtenida en el paso anterior. El conocimiento del profesor sobre el sistema de calificaci\u00f3n le indica que una nota menor a 5 puntos se corresponde con un suspenso y, en caso contrario, con un aprobado.</p> </li> <li> <p>La informaci\u00f3n de salida tras este proceso de decisi\u00f3n es que Pablo est\u00e1 suspenso en matem\u00e1ticas.</p> </li> </ol> <p>Question</p> <p>\u2753Pregunta. Siguiendo el ejemplo anterior \u00bfC\u00f3mo se produce el proceso de toma de decisiones para determinar si un n\u00famero es primo?</p> <p>Aunque se trate de un ejemplo trivial, la importancia del proceso de toma de decisiones no lo es. En marketing, por ejemplo, se analizan bases de datos de clientes para identificar distintos grupos e intentar predecir el comportamiento de estos. En el mundo de las finanzas, las inversiones realizadas por grandes empresas responden a un proceso complejo de toma de decisiones donde los datos son el eje fundamental de este proceso. En medicina, existe una gran cantidad de sistemas de ayuda a la decisi\u00f3n que permiten a los doctores contrastar y validar sus diagn\u00f3sticos de forma precoz. En definitiva, no hay \u00e1rea de conocimiento ni \u00e1mbito de aplicaci\u00f3n que escape al proceso de toma de decisiones.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones%20I/index.html#12-la-carrera-entre-los-datos-y-la-tecnologia","title":"1.2 La carrera entre los datos y la tecnolog\u00eda","text":"<p>Que los datos son el elemento fundamental en cualquier proceso y/o sistema de toma de decisiones no es algo nuevo. Sin embargo, los datos no siempre han estado al alcance de los expertos y no siempre ha sido posible ni sencillo procesarlos seg\u00fan las necesidades concretas de cada caso de aplicaci\u00f3n.</p> <p>La informaci\u00f3n, por tanto, siempre ha sido poder y el gran reto ha sido y sigue siendo extraer informaci\u00f3n a partir de datos para generar conocimiento. Para ello, es necesario contar con dos factores que deben estar alineados: datos y tecnolog\u00eda.</p> <p>Obtener datos no ha sido siempre una tarea f\u00e1cil. Esto es debido principalmente a que la gran cantidad de sensores disponibles en la actualidad, que permiten registrar magnitudes de cualquier proceso, no exist\u00eda como a d\u00eda de hoy. Adem\u00e1s, los sensores existentes en esta \u00e9poca (finales del siglo XX y comienzos del siglo XXI) no estaban ampliamente extendidos, ya que sus prestaciones estaban lejos de las que ofrecen hoy y sus precios no estaban al alcance de cualquier usuario. Por tanto, los procesos que se monitorizaban y de los cuales se recog\u00edan datos eran, sobretodo, procesos industriales realizados en grandes empresas. Por todos estos motivos, tradicionalmente se recurr\u00eda a modelos de simulaci\u00f3n que, a trav\u00e9s de la implementaci\u00f3n de un modelo matem\u00e1tico, permit\u00edan generar datos realistas de un proceso.</p> <p>Los datos generados mediante simulaci\u00f3n son conocidos como datos sint\u00e9ticos mientras que los datos provenientes de las lecturas de un sensor se conocen como datos reales.</p> <p>Pero con los datos no es suficiente. Es necesario tambi\u00e9n contar con la tecnolog\u00eda necesaria para su procesamiento. Generar, almacenar y procesar todos estos datos no es una tarea trivial, y plantea una serie de ptoblemas tecn\u00f3logicos a resolver.</p> <ul> <li>Primer problema tecnol\u00f3gico a resolver, el almacenamiento. Algunas soluciones propuestas pasan por los sistemas de informaci\u00f3n distribuida, entendidos como un conjunto de ordenadores separados f\u00edsicamente y conectados en red destinados al almacenamiento de datos o por los sistemas de informaci\u00f3n en la nube, que permiten adquirir espacio de almacenamiento en servidores privados, dejando la gesti\u00f3n de estos servidores en manos del proveedor.</li> <li> <p>El segundo problema tecnol\u00f3gico es el procesamiento de los datos almacenados. Este aspecto cobra especial relevancia en funci\u00f3n del caso de aplicaci\u00f3n, pudiendo distinguirse entre procesamiento on-line (en l\u00ednea) y procesamiento off-line (fuera de l\u00ednea).</p> </li> <li> <p>\ud83d\udcbb En el caso del procesamiento on-line, los datos son procesados a medida que son generados, ya que se requiere una respuesta en tiempo real. Por ejemplo, en un sistema de control del tr\u00e1fico que permite regular los sem\u00e1foros en funci\u00f3n del tr\u00e1fico actual, el sistema debe regular el sem\u00e1foro a medida que se van generando e interpretando los datos del tr\u00e1fico en un instante de tiempo dado.</p> </li> <li>\u274e Por otra parte, en el caso del procesamiento off-line, no es necesario que los datos se procesen a medida que se generan. Por ejemplo, en un sistema de detecci\u00f3n del fraude bancario, comprobar si un cliente ha realizado alg\u00fan movimiento fraudulento es una tarea que puede llevarse a cabo off-line, por ejemplo, haciendo un an\u00e1lisis de los movimientos del cliente en un momento dado, sin tener por qu\u00e9 diagnosticar cada movimiento que este va realizando.</li> </ul> <p>En este sentido, la computaci\u00f3n distribuida, en donde m\u00faltiples m\u00e1quinas realizan el procesamiento optimizando el rendimiento o la computaci\u00f3n en la nube, que permite adquirir recursos de procesamiento al igual que se puede adquirir espacio de almacenamiento, son dos soluciones al problema del procesamiento.</p> <p>Otras alternativas son la programaci\u00f3n paralela y la programaci\u00f3n multi-procesador, que permiten, respectivamente, aprovechar el paralelismo de m\u00faltiples hilos de ejecuci\u00f3n dentro de un procesador y realizar el procesamiento dividi\u00e9ndolo en m\u00faltiples hilos en diferentes procesadores</p> <p>Question</p> <p>\u2753 Pregunta. Piensa en procesos cotidianos que requieran un procesamiento on-line y en otros que requieran un procesamiento off-line.</p> <p>\u23f3 En la actualidad, la proliferaci\u00f3n de una gran cantidad de sensores con altas prestaciones y precios asequibles que permiten monitorizar y generar datos sobre cualquier proceso ha supuesto un incremento exponencial en la cantidad de datos generados. Es posible monitorizar casi cualquier proceso, incluyendo los dom\u00e9sticos como el consumo el\u00e9ctrico de un hogar, la presencia dentro del mismo o procesos cotidianos como la actividad f\u00edsica, entre otros muchos. Hoy, los datos llevan la delantera en la carrera entre datos y tecnolog\u00eda. Si bien es cierto que la tecnolog\u00eda ha experimentado grandes avances en los \u00faltimos a\u00f1os, la cantidad de datos generada no deja de crecer. Esto supone un reto permanente para la tecnolog\u00eda, que sigue evolucionando a nivel hardware con la aparici\u00f3n de arquitecturas con mayores posibilidades procesamiento y almacenamiento y a nivel software, con la aparici\u00f3n de modelos de programaci\u00f3n que optimizan el procesamiento de los datos.</p> <p>ver presentaci\u00f3n BDA1.2</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones%20I/index.html#13-los-datos-los-de-ayer-y-los-de-hoy","title":"1.3 Los datos: los de ayer y los de hoy","text":"<p>Al igual que la tecnolog\u00eda ha ido evolucionando para dar respuesta a la ingente cantidad de datos que ha comenzado a generarse, estos \u00faltimos tambi\u00e9n han experimentado una gran evoluci\u00f3n. Esta evoluci\u00f3n, o revoluci\u00f3n, no est\u00e1 \u00fanicamente relacionada con la cantidad de datos (como se expuso en el anterior apartado) sino tambi\u00e9n con el tipo y el formato de los mismos.</p> <p>\ud83d\udcbe Tradicionalmente, el tipo y formato de datos con el que se ha trabajado para extraer informaci\u00f3n y conocimiento a partir de ellos era ciertamente limitado. En muchas ocasiones se trataba de ficheros de datos estructurados de forma tabular, donde cada fila del conjunto de datos representaba una instancia del mismo y cada columna una variable o atributo de la instancia. El formato de archivo que se manejaba sol\u00edan ser formatos de hojas de c\u00e1lculo (.xlsx, .ods, .numbers etc) o ficheros separados por comas (.csv). Muy pocos eran los procesos en los que se trabajaba con otros tipos de datos como texto, im\u00e1genes, audio e incluso v\u00eddeos, ya que los formatos de estos tipos de datos eran limitados hace unos a\u00f1os, su procesamiento m\u00e1s complejo y la tecnolog\u00eda para ello a\u00fan en desarrollo.</p> <p>Aunque a d\u00eda de hoy tambi\u00e9n se sigue trabajando con archivos de datos en forma de hojas de c\u00e1lculo y/o archivos tradicionales para generar conocimiento a partir de ellos, las posibilidades actuales son pr\u00e1cticamente ilimitadas.</p> <ul> <li> <p>\u270f\ufe0f En cuanto al texto, las t\u00e9cnicas de inteligencia artificial y procesamiento del lenguaje natural hacen posible la extracci\u00f3n de conocimiento a partir de grandes vol\u00famenes de textos, que pueden provenir de p\u00e1ginas web, archivos .pdf, redes sociales, etc.</p> </li> <li> <p>\ud83d\udcf9 El desarrollo de hardware con mejores prestaciones y los nuevos modelos de programaci\u00f3n permiten procesar en la actualidad grandes cantidades de im\u00e1genes, audios y v\u00eddeos con una gran variedad de t\u00e9cnicas de inteligencia artificial en tiempos razonables.</p> </li> <li> <p>\u26ab Finalmente, han aparecido nuevos tipos y formatos de datos, como por ejemplo, aquellos datos generados a partir de grafos, los cuales se tratar\u00e1n en pr\u00f3ximas secciones y cap\u00edtulos con m\u00e1s detenimiento. Estos datos se corresponden, por ejemplo, con datos geogr\u00e1ficos obtenidos a partir de mapas como los generados en aplicaciones como Google Maps u Open Street Maps o datos de seguimiento y actividad en redes sociales de gran valor en campa\u00f1as publicitarias entre otros muchos.</p> </li> </ul> <p>Question</p> <p>\u2753Pregunta. Haz una b\u00fasqueda y elabora un listado con distintos tipos de datos y los formatos de almacenamiento m\u00e1s utilizados con los que se trabaja en ciencia de datos y big data.</p> <p>Los diferentes tipos y formatos de datos, los de ayer y los de hoy, son la materia b\u00e1sica fundamental en cualquier proceso de extracci\u00f3n de informaci\u00f3n y de conocimiento. Despu\u00e9s, las metodolog\u00edas empleadas para ello y arquitecturas hardware sobre las que se realice el procesamiento de los mismos, permitir\u00e1n definir procesos y metodolog\u00edas de big data, aplicadas a un \u00e1mbito concreto.</p> <p>ver presentaci\u00f3n BDA1.3</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones%20I/index.html#14-soluciones-big-data","title":"1.4 Soluciones Big Data","text":"<p>En esta nueva era tecnol\u00f3gica en la que nos hayamos inmersos, a diario se generan enormes cantidades de datos, del orden de petabytes (m\u00e1s de un mill\u00f3n de gigabytes) en muy cortos peri\u00f3dos de tiempo. Hoy en d\u00eda, cualquier dispositivo como puede ser un reloj, un coche, un smartphone, etc est\u00e1 conectado a Internet generando, enviando y recibiendo una gran cantidad de datos. Tanto es as\u00ed, que se estima que el 90 % de los datos disponibles en el mundo ha sido generado en los \u00faltimos a\u00f1os. Sin lugar a dudas, esta y las pr\u00f3ximas generaciones ser\u00e1n las generaciones del big data.</p> <p>Esta realidad descrita anteriormente demanda la capacidad de enviar y recibir datos e informaci\u00f3n a gran velocidad, as\u00ed como la capacidad de almacenar tal cantidad de datos y procesarlos en tiempo real. As\u00ed pues, la gran cantidad de datos disponibles junto con las herramientas, tanto hardware como software, que existen a disposici\u00f3n para analizarlos se conoce como big data.</p> <p>\u270c\ufe0f No existe una definici\u00f3n precisa del t\u00e9rmino big data, ni tampoco un t\u00e9rmino en castellano que permita denominar este concepto. A veces se usan en castellano los t\u00e9rmino datos masivos o grandes vol\u00famenes de datos para hacer referencia al big data. Por este motivo, a menudo el concepto de big data es definido en funci\u00f3n de las caracter\u00edsticas que poseen los datos y los procesos que forman parte de este nuevo paradigma de computaci\u00f3n. Esto es lo que se conoce como \u270c\ufe0f las Vs del big data \u270c\ufe0f.</p> <p>Algunos autores coinciden en que big data son datos cuyo volumen es demasiado grande como para procesarlos con las tecnolog\u00edas y t\u00e9cnicas tradicionales, requiriendo nuevas arquitecturas hardware, modelos de programaci\u00f3n y algoritmos para su procesamiento. Adem\u00e1s, se trata de datos que se presentan en una gran variedad de estructuras y formatos: datos sint\u00e9ticos, provenientes de sensores, num\u00e9ricos, textuales, im\u00e1genes, audio, v\u00eddeo... Finalmente, se trata de datos que requieren ser procesados a gran velocidad para poder extraer valor y conocimiento de ellos. Esta concepci\u00f3n se conoce como las tres Vs del big data (ver figura1.2).</p> <p></p> <p>Figura 1.2: Definici\u00f3n de big data en base a \u201cLas tres Vs del big data</p> <p>Otros autores ampl\u00edan las caracter\u00edsticas que han de tener los datos que forman parte del big data, incluyendo \u201cotras Vs\u201d como lo son:    - \u270c\ufe0f Volatilidad, referida al tiempo durante el cual los datos recogidos son v\u00e1lidos y a durante cu\u00e1nto tiempo deber\u00e1n ser almacenados.    - \u270c\ufe0f Valor, referido a la utilidad de los datos obtenidos para extraer conocimiento y tomar decisiones a partir de ellos.    - \u270c\ufe0f Validez, referida a lo precisos que son los datos para el uso que se pretende darles. El uso de datos validados permitir\u00e1 ahorrar tiempo en etapas como la limpieza y el preprocesamiento de los datos.    - \u270c\ufe0f Veracidad, relacionada con la confiabilidad del origen del cual provienen los datos con los que se trabajar\u00e1 as\u00ed como la incertidumbre o el ruido que pudiera existir en ellos.    - \u270c\ufe0f Variabilidad, frente a la variedad de estructuras y formatos, hace referencia a la complejidad del conjunto de datos, es decir, al n\u00famero de variables que contiene. Estas caracter\u00edsticas, unidas a las tres Vs descritas anteriormente, se conocen como las ocho Vs del big data (ver figura1.3).</p> <p></p> <p>Figura 1.3: Definici\u00f3n de big data en base a \u201cLas ocho Vs del big data\u201d</p> <p>Dado que no existe una definici\u00f3n uniforme para el t\u00e9rmino big data, muchos autores definen el t\u00e9rmino en funci\u00f3n de aquellas caracter\u00edsticas que consideran m\u00e1s relevantes, por lo que es com\u00fan encontrar \u201clas cinco Vs del big data\u201d, \u201clas siete Vs del big data\u201d o \u201clas diez Vs del big data\u201d seg\u00fan cada autor, apareciendo distintos t\u00e9rminos para describir el big data, como tambi\u00e9n pueden ser visualizaci\u00f3n o vulnerabilidad, entre otros. </p> <p>Son muchas las soluciones a nivel hardware y software, que se han propuesto a los problemas derivados del almacenamiento y el procesamiento de big data. A continuaci\u00f3n, se describen los fundamentos de tres de ellas, las cuales ser\u00e1n desarrolladas a nivel te\u00f3rico, tecnol\u00f3gico y pr\u00e1ctico en los siguientes cap\u00edtulos.</p> <p>ver presentaci\u00f3n BDA1.4</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones%20I/index.html#141-almacenes-de-datos","title":"1.4.1 Almacenes de datos","text":"<p>\ud83d\udcbe Tradicionalmente hablando, cuando nos referimos a almacenes de datos, podemos hablar de las bases de datos relacionales son colecciones de datos integrados, almacenados en un soporte secundario no vol\u00e1til y con redundancia controlada. La definici\u00f3n de los datos y la estructura de la base de datos debe estar basada en un modelo de datos que permita captar las interrelaciones y restricciones existentes en el dominio que se pretende modelizar. A su vez, un Sistema Gestor de Bases de Datos (SGBD) se compone de una colecci\u00f3n de datos estructurados e interrelacionados (una base de datos) as\u00ed como de un conjunto de programas para acceder a dichos datos.</p> <p>\ud83d\udcbe Las bases de datos tradicionales, siguiendo la definici\u00f3n anterior, est\u00e1n basadas generalmente en sistemas relacionales u objeto-relacionales. Para el acceso, procesamiento y recuperaci\u00f3n de los datos, se sigue el modelo Online Transaction Processing (OLTP). Una transacci\u00f3n es una interacci\u00f3n completa con un sistema de base de datos, que representa una unidad de trabajo. As\u00ed pues, una transacci\u00f3n representa cualquier cambio que se produzca en una base de datos.</p> <p>\ud83d\udcbe El modelo OLTP, traducido al castellano como procesamiento de transacciones en l\u00ednea, permite gestionar los cambios de la base de datos mediante la inserci\u00f3n, actualizaci\u00f3n y eliminaci\u00f3n de informaci\u00f3n de la misma a trav\u00e9s de transacciones b\u00e1sicas que son procesadas en tiempos muy peque\u00f1os.</p> <p>\ud83d\udcbe Con respecto a la recuperaci\u00f3n de informaci\u00f3n de la base de datos, se utilizan operadores cl\u00e1sicos (concatenaci\u00f3n, proyecci\u00f3n, selecci\u00f3n, agrupamiento...) para realizar consultas b\u00e1sicas y sencillas (realizadas, mayoritariamente, en lenguaje SQL y extensiones del mismo).</p> <p>\ud83d\udcbe Finalmente, las opciones de visualizaci\u00f3n de los datos recuperados son limitadas, mostr\u00e1ndose fundamentalmente los resultados de forma tabular y requiriendo un procesamiento adicional y m\u00e1s complejo en caso de querer presentar datos complejos.</p> <p>\ud83d\udd01 La revoluci\u00f3n en la generaci\u00f3n, almacenamiento y procesamiento de los datos, as\u00ed como la irrupci\u00f3n del big data, han puesto a prueba el modelo de funcionamiento, rendimiento y escalabilidad de las bases de datos relacionales tradicionales. En la actualidad, se requiere de soluciones integradas que a\u00fanen datos y tecnolog\u00eda para almacenar y procesar grandes cantidades de datos con diferentes estructuras y formatos con el objetivo de facilitar la consulta, el an\u00e1lisis y la toma de decisiones sobre los mismos. En este sentido, la inteligencia de negocio, m\u00e1s conocida por el t\u00e9rmino ingl\u00e9s business intelligence, investiga en el dise\u00f1o y desarrollo de este tipo de soluciones. La inteligencia de negocio puede definirse como la capacidad de una empresa de estudiar sus acciones y comportamientos pasados para entender d\u00f3nde ha estado la empresa, determinar la situaci\u00f3n actual y predecir o cambiar lo que suceder\u00e1 en el futuro, utilizando las soluciones tecnol\u00f3gicas m\u00e1s apropiadas para optimizar el proceso de toma de decisiones.</p> <p>Estas nuevas soluciones requerir\u00e1n un modelo de procesamiento diferente a OLTP. Esto es as\u00ed, ya que el objetivo perseguido por la inteligencia de negocio est\u00e1 menos orientado al \u00e1mbito transaccional y m\u00e1s enfocado al \u00e1mbito anal\u00edtico. Las nuevas soluciones utilizan el modelo Online analytical processing (OLAP).</p> <p>La principal diferencia entre OLTP y OLAP estriba en que mientras que el primero es un sistema de procesamiento de transacciones en l\u00ednea, el segundo es un sistema de recuperaci\u00f3n y an\u00e1lisis de datos en l\u00ednea. Por tanto, OLAP complementa a SQL aportando la capacidad de analizar datos desde distintas variables y dimensiones, mejorando el proceso de toma de decisiones. Para ello, OLAP permite realizar c\u00e1lculos y consolidaciones entre datos de distintas dimensiones, creando modelos que no presentan limitaciones conceptuales ni f\u00edsicas, presentando y visualizando la informaci\u00f3n de forma flexible, esto es, en diferentes formatos.</p> <p>Los sistemas OLAP est\u00e1n basados, generalmente, en sistemas o interfaces multidimensionales que proporcionan facilidades para la transformaci\u00f3n de los datos, permitiendo obtener nuevos datos m\u00e1s combinados y agregados que los obtenidos mediante las consultas simples realizadas por OLTP. Al contrario que en OLTP, las unidades de trabajo de OLAP son m\u00e1s complejas que en OLTP y consumen m\u00e1s tiempo.</p> <p>Finalmente, en cuanto a la visualizaci\u00f3n de los mismos, los sistemas OLAP permiten la visualizaci\u00f3n y el an\u00e1lisis multidimensional a partir de diferentes vistas de los datos, presentando los resultados en forma matricial y con mayores posibilidades est\u00e9ticas y visuales. La tabla 1.1 muestra un resumen con las principales diferencias entre los sistemas OLTP y OLAP.</p> Bases de datos relacionales(OLTP) Soluciones Business Intelligence(OLAP) Concepto Sistema de procesamiento de transacciones en l\u00ednea Sistema de recuperaci\u00f3n y an\u00e1lisis de datos en l\u00ednea Funciones Gesti\u00f3n de transacciones: inserci\u00f3n, actualizaci\u00f3n, eliminaci\u00f3n... An\u00e1lisis de datos para dar soporte a la toma de decisiones Procesamiento Transacciones cortas Procesamientos de an\u00e1lisis complejos Tiempo Las transacciones requieren poco tiempo de ejecuci\u00f3n Los an\u00e1lisis requieren mayor tiempo de ejecuci\u00f3n Consultas Simples, utilizando operadores b\u00e1sicos tradicionales Complejas, permitiendo analizar los datos desde m\u00faltiples dimensiones Visualizaci\u00f3n B\u00e1sica. Muestra los datos en forma tabular Muestra los datos en forma matricial. Mayores posibilidades gr\u00e1ficas <p>Tabla 1.1: Tabla resumen y comparativa entre OLTP y OLAP</p> <p>\u2757Por tanto, un almac\u00e9n de datos, m\u00e1s conocido por el t\u00e9rmino data warehouse (en ingl\u00e9s), es una soluci\u00f3n de business intelligence que combina tecnolog\u00edas y componentes con el objetivo de ayudar al uso estrat\u00e9gico de los datos por parte de una organizaci\u00f3n. Esta soluci\u00f3n debe proveer a la empresa, de forma integrada, de capacidad de almacenamiento de una gran cantidad de datos as\u00ed como de herramientas de an\u00e1lisis de los mismos que, frente al procesamiento de transacciones, permita transformar los datos en informaci\u00f3n para ponerla a disposici\u00f3n de la organizaci\u00f3n y optimizar el proceso de toma de decisiones.</p> <p>ver presentaci\u00f3n BDA1.5</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones%20I/index.html#142-bases-de-datos-documentales-u-orientadas-a-documentos","title":"1.4.2 Bases de datos documentales u orientadas a documentos","text":"<p>La gran variedad y heterogeneidad en los tipos de datos almacenados y procesados en los \u00faltimos a\u00f1os ha puesto sobre la mesa la cuesti\u00f3n de si las bases de datos relacionales son el modelo m\u00e1s \u00f3ptimo para trabajar con seg\u00fan qu\u00e9 tipos de datos. Como alternativa a ellas, en los \u00faltimos a\u00f1os han proliferado las bases de datos NoSQL.</p> <p>NoSQL es el t\u00e9rmino utilizado para referirse a un tipo de bases de datos que permiten almacenar y gestionar tipos de datos que tradicionalmente han sido dif\u00edciles de gestionar por parte de las bases de datos relacionales. As\u00ed pues, NoSQL hace referencia a bases de datos documentales, bases de datos orientadas a grafos, buscadores, etc. En contraposici\u00f3n a las bases de datos relacionales, las NoSQL se caracterizan principalmente por:  - Independencia del esquema: Al contrario que en las bases de datos relacionales, no es necesario dise\u00f1ar un esquema para definir los tipos y estructura de los datos almacenados, permitiendo acortar el tiempo de desarrollo y facilitando las modificaciones de la estructura interna de la base de datos.</p> <ul> <li> <p>No relacionales: El concepto de relaci\u00f3n de las bases de datos relacionales no existe en NoSQL. Por tanto, se trabaja con datos que no est\u00e1n normalizados, lo cual aporta flexibilidad en relaci\u00f3n a los tipos y estructuras de datos que pueden ser almacenados.</p> </li> <li> <p>Distribuidas: La cantidad de datos almacenados requiere de su almacenamiento en m\u00faltiples servidores, ya que un \u00fanico servidor por potente que sea no podr\u00e1 procesar en tiempos razonables tal cantidad de informaci\u00f3n. Este hecho permite utilizar hardware sencillo, ya que al utilizar m\u00faltiples servidores no es necesario que todos ellos tengan grandes prestaciones.</p> </li> </ul> <p>Las bases de datos documentales trabajan con documentos, entendidos como una estructura jer\u00e1rquica de datos que, a su vez, puede contener subestructuras. En una primera aproximaci\u00f3n, es f\u00e1cil pensar en que estos documentos pueden ser documentos de Microsoft Word, documentos pdf, p\u00e1ginas web... Las bases de datos documentales pueden, efectivamente, trabajar con estos tipos de documentos. Sin embargo, el t\u00e9rmino documento en este contexto posee un mayor nivel de abstracci\u00f3n.</p> <p>\ud83d\udd16 Los documentos pueden consistir en datos binarios o texto plano. Es posible que se traten de datos semiestructurados, cuando aparecen en formatos como JavaScript Object Notation (JSON) o Extensible Markup Language (XML). Por \u00faltimo, tambi\u00e9n pueden ser datos estructurados conforme a un modelo de datos particular como, por ejemplo, XML Schema Definition (XSD).</p> <p>\ud83d\udd16 Actualmente, XML y JSON son los formatos de intercambio de datos m\u00e1s utilizados en el desarrollo de aplicaciones web. Sin embargo, existen importantes diferencias entre ellos: XML es una extensi\u00f3n del lenguaje Standard Generalized Markup Language (SGML) el cual es un lenguaje que permite la creaci\u00f3n, organizaci\u00f3n y etiquetado de documentos. Por su parte, JSON es una extensi\u00f3n del lenguaje JavaScript. XML tiene una notaci\u00f3n m\u00e1s pesada que JSON, siendo este \u00faltimo un lenguaje m\u00e1s ligero que admite tipos de datos y matrices. Por su parte, XML no proporciona tipos ni estructuras de datos, sino que contiene un conjunto de reglas que, mediante el uso de atributos y elementos, permite codificar un documento. Finalmente, JSON es un lenguaje orientado a datos mientras que XML es un lenguaje orientado a documentos. La tabla 1.2 muestra una comparativa de estos y otros aspectos de ambos lenguajes.</p> XML JSON Lenguaje fuente SGML JavaScript Tipo Lenguaje Orientado a datos Orientado a documentos Notaci\u00f3n Pesada Ligera Etiquetas inicio y fin S\u00ed No Comentarios S\u00ed No Espacios de nombres S\u00ed No Soporte tipo de datos No S\u00ed <p>Tabla 1.2: Tabla comparativa entre XML y JSON</p> <p>\ud83d\udd16 En XML, la estructura principal de un documento est\u00e1 formada por dos elementos: el pr\u00f3logo (opcional) y el cuerpo. El pr\u00f3logo contiene a su vez dos partes: la declaraci\u00f3n XML que establece la versi\u00f3n del lenguaje, el tipo de codificaci\u00f3n y si se trata de un documento aut\u00f3nomo y la declaraci\u00f3n del tipo de documento. El cuerpo, por su parte, contiene la informaci\u00f3n del documento</p> <p>\ud83d\udd16 Supongamos que Carlos ha enviado un mensaje de whatsapp a Javier dici\u00e9ndole que han quedado con los compa\u00f1eros de trabajo a las diez de la noche en la puerta del Sol. Un documento XML que representa este mensaje como un documento, podr\u00eda ser el mostrado en el listado 1.</p> <p><pre><code>&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\n&lt;whatsapp&gt;\n&lt;para&gt; Javier &lt;/para&gt;\n&lt;de&gt; Carlos &lt;/de&gt;\n&lt;titulo&gt; Quedada &lt;/titulo&gt;\n&lt;contenido&gt; A las 22:00 pm en la puerta del sol &lt;/contenido&gt;\n&lt;/whatsapp&gt;\n</code></pre> Listado 1: Quedada en la puerta del sol</p> <p>\ud83d\udd16 El listado 1 incluye en la primera l\u00ednea el pr\u00f3logo del documento, definiendo la versi\u00f3n y el tipo de codificaci\u00f3n utilizada. A partir de la segunda l\u00ednea, se define el cuerpo del documento que contiene, mediante etiquetas de apertura &lt;&gt;y cierre , los distintos atributos de los que se compone el documento.</p> <p>ver presentaci\u00f3n BDA1.6</p> <p>\u303d\ufe0f En JSON, la sintaxis del lenguaje tiene las mismas reglas que el lenguaje JavaScript, del cual proviene. Sin embargo, los archivos JSON deben cumplir tambi\u00e9n otras reglas sint\u00e1cticas adicionales.</p> <ul> <li>En primer lugar, un archivo JSON representar\u00e1 o bien un objeto, es decir, una tupla de pares clave-valor o bien una colecci\u00f3n de elementos, es decir, un vector o array.</li> <li>Por otra parte, los archivos JSON que representan objetos comienzan con una llave de inicio { y terminan con una llave de cierre }. Cuando se representa un vector, sus elementos se encierran entre corchetes [].</li> <li>Las cadenas y nombres de atributos del objeto deber\u00e1n encerrarse entre comillas, as\u00ed como todos los nombres de los atributos del objeto, separ\u00e1ndose cada elemento del siguiente con una coma (,) no habiendo una coma despu\u00e9s del \u00faltimo elemento.</li> </ul> <p>\u303d\ufe0f As\u00ed pues, si se pretende representar en formato JSON el mensaje que ha enviado Carlos a Javier, el fichero JSON resultante ser\u00eda el mostrado en el listado 2. Este fichero define un objeto JSON que contiene una serie de atributos entrecomillados cuyo valor asociado son cadenas de caracteres que, por tanto, tambi\u00e9n van entrecomilladas.</p> <p><pre><code>{\n\"para\": \"Javier\",\n\"de\": \"Carlos\",\n\"titulo\": \"Quedada\",\n\"contenido\": \"A las 22:00 pm en la puerta del sol\"\n}\n</code></pre> Listado 2: Quedada en la puerta del sol</p> <p>Note</p> <p>\ud83d\udd16 Existen en la web m\u00faltiples aplicaciones online que permiten convertir archivos XML en JSON y viceversa \u303d\ufe0f. En sucesivos cap\u00edtulos, se profundizar\u00e1 m\u00e1s sobre la sintaxis de estos dos lenguajes as\u00ed como la equivalencia entre los mismos a la hora de definir documentos que ser\u00e1n tratados posteriormente por una base de datos documental. Tambi\u00e9n se describir\u00e1n las tecnolog\u00edas m\u00e1s utilizadas en este \u00e1mbito, haciendo especial \u00e9nfasis en MongoDB, uno de los sistemas de bases de datos NoSQL orientado a documentos m\u00e1s utilizado a d\u00eda de hoy.</p> <p>ver presentaci\u00f3n BDA1.7</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones%20I/index.html#143-bases-de-datos-sobre-grafos","title":"1.4.3 Bases de datos sobre grafos","text":"<p>Tal y como se expuso anteriormente, en los \u00faltimos a\u00f1os han aparecido nuevos tipos de datos como aquellos que vienen dados en forma de grafo. Algunos de estos datos son los provenientes de interacciones en redes sociales, datos geogr\u00e1ficos expresados en forma de mapas, etc.</p> <p>Un grafo es un ente matem\u00e1tico compuesto por un conjunto de nodos o v\u00e9rtices y un conjunto de enlaces o aristas. Matem\u00e1ticamente puede ser expresado por medio de la ecuaci\u00f3n siguiente</p> <pre><code>G = {V, E}\n</code></pre> <p>Donde V representa el conjunto de nodos o v\u00e9rtices y E representa el conjunto de enlaces o aristas. As\u00ed pues, sea el grafo de la figura 4 que representa un conjunto de ciudades conectadas por autov\u00edas, el conjunto V de nodos ser\u00eda V = {La Coru\u00f1a, Madrid, San Sebasti\u00e1n, Barcelona, Valencia, Sevilla, C\u00e1diz}, mientras que el conjunto de enlaces E vendr\u00eda dado por</p> <pre><code>E = {A-1, A-2, A-3, A-4, A-4-I, A-4-II, A-6, A-7}\n</code></pre> <p></p> <p>Figura 1.4: Grafo que representa las principales autov\u00edas de Espa\u00f1a</p> <p>Una base de datos orientada a grafos es, por tanto, un sistema de bases de datos que implementa m\u00e9todos de creaci\u00f3n, lectura, actualizaci\u00f3n y eliminaci\u00f3n de datos en un modelo expresado en forma de grafo. Existen dos aspectos fundamentales en este tipo de sistemas: el primero de ellos hace referencia al almacenamiento de los datos. En una base de datos orientada a grafos, los datos pueden almacenarse siguiendo el modelo relacional, lo que implica mapear la estructura del grafo a una estructura relacional, o bien, almacenarse de forma nativa utilizando modelos de datos propios para almacenar estructuras de tipo grafo. La ventaja de mapear los grafos a una estructura relacional radica en que la gesti\u00f3n y consulta de los datos se realizar\u00e1 de forma tradicional a trav\u00e9s de un backend conocido como, por ejemplo, MySQL. Por su parte, la ventaja del almacenamiento nativo de grafos radica es que existen modelos de datos e implementaciones que aseguran y garantizan el buen rendimiento y la escalabilidad del sistema.</p> <p>El segundo aspecto importante es el procesamiento de los datos. En este sentido, tambi\u00e9n es posible distinguir el procesamiento no nativo de los grafos, lo cual implica el tratamiento de estos datos siguiendo las t\u00e9cnicas tradicionales utilizadas en los lenguajes de modificaci\u00f3n de datos de las bases de datos relacionales, o el procesamiento nativo de los datos de grafos, el cual es beneficioso porque optimiza los recorridos del grafo cuando se realizan consultas aunque, en ocasiones, invierta demasiado tiempo y memoria en consultas que no requieren de recorridos complejos.</p> <p>Si bien es cierto que cualquier dominio puede ser modelado como un grafo, esta no es una raz\u00f3n de peso como para cambiar o migrar de un esquema y modelo de datos bien conocido e investigado, como el esquema relacional, a un modelo orientado a grafos. Sin embargo, la motivaci\u00f3n para requerir de sistemas de bases de datos espec\u00edficos, orientados a trabajar con este tipo de datos, radica en tres aspectos principales:</p> <ul> <li> <p>\u26ab Rendimiento: Los sistemas de bases de datos orientados a grafos optimizan el rendimiento de las consultas sobre este tipo de datos con respecto a las bases de datos relacionales. Si bien es cierto que las consultas en el modelo relacional se vuelven m\u00e1s complejas y el rendimiento disminuye a medida que el conjunto de datos crece, esto no es as\u00ed cuando se trabaja con grafos, donde complejidad y rendimiento permanecen constantes. Esto es as\u00ed, ya que las consultas se localizan en una porci\u00f3n del grafo y, por tanto, solo es necesario explorar la parte del grafo afectada para resolver la consulta.</p> </li> <li> <p>\u26ab Flexibilidad: Los sistemas de bases de datos orientados a grafos son m\u00e1s flexibles que los sistemas de bases de datos relacionales. Mientras que los primeros requieren la modelizaci\u00f3n exhaustiva a priori de todo el dominio, esto no es necesario en los segundos, donde la naturaleza aditiva de los grafos permite a\u00f1adir nuevos nodos, relaciones, etiquetas y subgrafos a uno dado sin necesidad de modificar todo el modelo de datos, facilitando la implementaci\u00f3n y reduciendo el riesgo a corromper el modelo de datos. </p> </li> <li>\u26ab Agilidad: A partir de las dos caracter\u00edsticas anteriores, es posible deducir que el trabajo con sistemas de bases de datos orientados a grafos es m\u00e1s \u00e1gil que la gesti\u00f3n de estos datos a trav\u00e9s de sistemas de bases de datos relacionales. Esta r\u00e1pida gesti\u00f3n permitir\u00e1 utilizar tecnolog\u00eda alineada con las nuevas metodolog\u00edas de desarrollo de software \u00e1gil, permitiendo dise\u00f1ar y desarrollar software que utilice este tipos de datos.</li> </ul> <p>Existen distintos lenguajes de marcado de grafos, a partir de los cuales es posible generar archivos con formato de grafo para ser tratados por una base de datosde este tipo. Algunos de los lenguajes m\u00e1s utilizados son GraphML, eXtensibleGraph Markup and Modeling Language (XGMML), Graph Exchange Language (GXL) o Graph Modelling Language (GML), entre otros. La mayor\u00eda deellos son variantes o extensiones del lenguaje XML para el modelado de grafos.</p> <p>En la actualidad, GraphML es uno de los lenguajes m\u00e1s extendidos para el modelado de datos en forma de grafo. Se trata de un lenguaje sencillo, general, extensible y robusto que est\u00e1 compuesto por un n\u00facleo del lenguaje que define las propiedades estructurales del grafo y un mecanismo flexible de extensiones que permite a\u00f1adir informaci\u00f3n espec\u00edfica del mismo. La notaci\u00f3n es muy similar a XML y se profundizar\u00e1 en ella en cap\u00edtulos posteriores. A modo de ejemplo, el grafo mostrado en la figura 1.4 podr\u00eda representarse en GraphML seg\u00fan se muestra en el siguiente listado.</p> <p><pre><code>&lt;graph id=\"Grafo_Autovias\" edgedefault=\"undirected\"&gt;\n&lt;node id=\"La Coru\u00f1a\"/&gt;\n&lt;node id=\"San Sebasti\u00e1n\"/&gt;\n&lt;node id=\"Madrid\"/&gt;\n&lt;node id=\"Barcelona\"/&gt;\n&lt;node id=\"Valencia\"/&gt;\n&lt;node id=\"Sevilla\"/&gt;\n&lt;node id=\"Cadiz\"/&gt;\n&lt;edge id = \"A-6\" source=\"La Coru\u00f1a\" target=\"Madrid\"/&gt;\n&lt;edge id = \"A-1\" source=\"Madrid\" target=\"San Sebasti\u00e1n\"/&gt;\n&lt;edge id = \"A-2\" source=\"Madrid\" target=\"Barcelona\"/&gt;\n&lt;edge id = \"A-7\" source=\"Barcelona\" target=\"Valencia\"/&gt;\n&lt;edge id = \"A-3\" source=\"Madrid\" target=\"Valencia\"/&gt;\n&lt;edge id = \"A-4\" source=\"Madrid\" target=\"Sevilla\"/&gt;\n&lt;edge id = \"A-4-I\" source=\"Sevilla\" target=\"C\u00e1diz\"/&gt;\n&lt;edge id = \"A-4-II\" source=\"Madrid\" target=\"C\u00e1diz\"/&gt;\n&lt;/graph&gt;\n</code></pre> Listado 3: Grafo Autov\u00edas</p> <p>Como se puede apreciar en el fragmento de c\u00f3digo, en primer lugar se define un grafo cuyo identificador es Grafo_Autov\u00edas. El tipo de aristas que incluye este grafo son aristas no dirigidas, puesto que los enlaces del grafo no presentan flechas que indiquen direcci\u00f3n. Por tanto, los enlaces son bidireccionales. En caso de definir un grafo dirigido, el valor del atributo edgedefault ser\u00eda directed. A continuaci\u00f3n, se definen los distintos nodos que contendr\u00e1 el grafo y despu\u00e9s las aristas o enlaces, definiendo el identificador de cada una de ellas as\u00ed como su origen y destino. Al tratarse de un grafo no dirigido, origen y destino son intercambiables, no siendo as\u00ed en caso de que se trate de un grafo dirigido.</p> <p>Los sistemas de bases de datos orientados a grafos han proliferado mucho en los \u00faltimos a\u00f1os, existiendo numerosas tecnolog\u00edas que dan soporte a ellos, como pueden ser Microsoft Infinite Graph, Titan, OrientDB, FlockDB, AllegroGraph o Neo4j que se estudiar\u00e1 con m\u00e1s profundidad posteriormente.</p> <p>ver presentaci\u00f3n BDA1.8</p>"},{"location":"UD2%20-%20Gesti%C3%B3n%20de%20soluciones%20II/index.html","title":"UD 2 - Gesti\u00f3n de Soluciones II","text":"<p>En este cap\u00edtulo se profundizar\u00e1 en los almacenes de datos o Data Warehouses (DW) como soluci\u00f3n propia de la inteligencia de negocio o Business Intelligence (BI). Esta soluci\u00f3n aparece como respuesta al crecimiento de datos digitales almacenados por las empresas y la necesidad de extraer informaci\u00f3n y generar conocimiento a partir de ellos para optimizar el proceso de toma de decisiones en cada uno de los departamentos o divisiones de la empresa. Para ello, en este cap\u00edtulo se aborda el concepto de almac\u00e9n de datos, su arquitectura as\u00ed como su dise\u00f1o e implementaci\u00f3n.</p>"},{"location":"UD2%20-%20Gesti%C3%B3n%20de%20soluciones%20II/index.html#21-sistemas-de-ayuda-a-la-decision","title":"2.1. Sistemas de ayuda a la decisi\u00f3n","text":"<p>En una empresa u organizaci\u00f3n, los datos generados a diario son, principalmente, aquellos derivados de las operaciones rutinarias de la empresa. Estos datos, tradicionalmente, se almacenaban en bases de datos relacionales y su manipulaci\u00f3n se correspond\u00eda con transacciones realizadas sobre la base de datos. Sin embargo, el objetivo de cualquier organizaci\u00f3n es seleccionar esos datos para realizar estudios y an\u00e1lisis que permitan generar informes que, a su vez, permitan a la empresa extraer informaci\u00f3n para tomar decisiones estrat\u00e9gicas que conduzcan a la organizaci\u00f3n al \u00e9xito.</p> <p>El crecimiento exponencial de los datos manejados por una organizaci\u00f3n ha hecho que los computadores sean las \u00fanicas herramientas capaces de procesar estos datos para obtener informaci\u00f3n y ofrecer ayuda en la toma de decisiones. En este contexto, aparecen los sistemas de ayuda a la decisi\u00f3n o Decision Support Systems (DSS) que ayudan a quienes ocupan puestos de gesti\u00f3n a tomar decisiones o elegir entre diferentes alternativas. </p> <p>Sistema de ayuda a la decisi\u00f3n</p> <p>\ud83d\udcc4 Sistema de ayuda a la decisi\u00f3n: Conjunto de t\u00e9cnicas y herramientas tecnol\u00f3gicas desarrolladas para procesar y analizar datos para ofrecer soporte en la toma decisiones a quienes ocupan puestos de gesti\u00f3n o direcci\u00f3n en una organizaci\u00f3n. Para ello, el sistema combina los recursos de los gestores junto con los recursos computacionales para optimizar el proceso de toma de decisiones.</p> <p>Mientras que las bases de datos relacionales han sido tradicionalmente el componente del back-end en el dise\u00f1o de sistemas de ayuda a la decisi\u00f3n, los almacenes de datos se han convertido en una opci\u00f3n mucho m\u00e1s competitiva como elemento back-end al mejorar el rendimiento de \u00e9stas.</p> <p>Los campos de aplicaci\u00f3n de los almacenes de datos no se reducen \u00fanicamente al \u00e1mbito empresarial, sino que cubren multitud de dominios como las ciencias naturales, demograf\u00eda, epidemiolog\u00eda o educaci\u00f3n, entre otros muchos. La propiedad com\u00fan a todos estos campos y que hace de los almacenes de datos una adecuada soluci\u00f3n en estos \u00e1mbitos es la necesidad de almacenamiento y herramientas de an\u00e1lisis que permitan obtener en tiempos razonables informaci\u00f3n y conocimiento \u00fatiles para mejorar el proceso de toma de decisiones.</p> <p>ver presentaci\u00f3n BDA2.1</p>"},{"location":"UD2%20-%20Gesti%C3%B3n%20de%20soluciones%20II/index.html#22-almacenes-de-datos-concepto","title":"2.2. Almacenes de datos: Concepto","text":"<p>La aparici\u00f3n de los almacenes de datos est\u00e1 ligada, principalmente, a una serie de retos que es necesario abordar para convertir los datos transaccionales con los que trabaja una base de datos relacional en informaci\u00f3n para generar conocimiento y dar soporte al proceso de toma de decisiones</p> <ul> <li> <p>Accesibilidad: Desde cualquier dispositivo, a cualquier tipo de usuario y a gran cantidad de informaci\u00f3n que no puede ser almacenada de forma centralizada. La accesibilidad, en este sentido, debe hacer frente al problema de la escalabilidad del sistema y de los datos que este maneja.</p> </li> <li> <p>Integraci\u00f3n: Referente a la gesti\u00f3n de datos heterog\u00e9neos, con distintos formatos, y provenientes de distintos \u00e1mbitos de la organizaci\u00f3n. Una correcta integraci\u00f3n debe garantizar a su vez la correcci\u00f3n y completitud de los datos integrados.</p> </li> <li> <p>Consultas mejoradas: Permitiendo incluir operadores avanzados y dar soporte a herramientas y procedimientos que posibiliten obtener el m\u00e1ximo partido de los datos existentes. De este modo, ser\u00e1 posible obtener informaci\u00f3n precisa para realizar un an\u00e1lisis eficiente.</p> </li> <li> <p>Representaci\u00f3n multidimensional: Proporciona herramientas para analizar de forma multi-dimensional los datos del sistema, incluyendo datos de diferentes unidades de la organizaci\u00f3n con el objetivo de proporcionar herramientas de an\u00e1lisis y visualizaci\u00f3n multi-dimensional para mejorar el proceso de toma de decisiones.</p> </li> </ul> <p>A continuaci\u00f3n, se muestra una definici\u00f3n de almac\u00e9n de datos muy extendida, dada por W. Inmon, quien es conocido por ser el \u201cpadre\u201d del concepto de almac\u00e9n de datos.</p> <p>Almac\u00e9n de datos (Data Warehouse)</p> <p>\ud83d\udcc4 Almac\u00e9n de datos (Data Warehouse): Colecci\u00f3n de datos orientados a temas, integrados, variante en el tiempo y no vol\u00e1til que da soporte al proceso de toma de decisiones de la direcci\u00f3n.</p> <p>Para entender correctamente esta definici\u00f3n, es necesario ahondar en las caracter\u00edsticas que incluye la misma.</p> <ul> <li> <p>Orientados a temas: Es decir, no orientado a procesos (transacciones), sino a entidades de mayor nivel de abstracci\u00f3n como \u201cart\u00edculo\u201d o \u201cpedido\u201d.</p> </li> <li> <p>Integrados: Almacenados en un formato uniforme y consistente, lo que implica depurar o limpiar los datos para poder integrarlos.</p> </li> <li> <p>Variante en el tiempo: Asociados a un instante de tiempo (mes, trimestre, a\u00f1o...)</p> </li> <li> <p>No vol\u00e1tiles: Se trata de datos persistentes que no cambian una vez se incluyen en el almac\u00e9n de datos.</p> </li> </ul> <p>El dise\u00f1o y funcionamiento de los almacenes de datos se basa en el sistema de procesamiento anal\u00edtico en-l\u00ednea, OLAP. Este sistema se encarga del an\u00e1lisis, interpretaci\u00f3n y toma de decisiones acerca del negocio, en contraposici\u00f3n a los sistemas de procesamiento de transacciones en l\u00ednea, OLTP.</p> <p>As\u00ed pues, los sistemas OLTP est\u00e1n dirigidos por la tecnolog\u00eda y orientados a automatizar las operaciones del d\u00eda a d\u00eda de la organizaci\u00f3n, mientras que los sistemas OLAP est\u00e1n dirigidos por el negocio y proporcionan herramientas para tomar decisiones a largo plazo, mejorando la estrategia y la competitividad de la organizaci\u00f3n. La tabla 2.1 muestra una comparativa entre las principales caracter\u00edsticas de las bases de datos operacionales (OLTP) y los almacenes de datos (OLAP).</p> Caracter\u00edstica BBDD Operacionales(OLTP) Almac\u00e9n Datos(OLAP) Objetivo Depende de la aplicaci\u00f3n Toma de decisiones Usuarios Miles Cientos Trabajo con... Transacciones predefinidas Consultas y an\u00e1lisis espec\u00edficos Acceso Lectura y escritura a cientos de registros Principalmente lecutra. Miles de registros Datos Detallados, num\u00e9ricos y alfanum\u00e9ricos Agregados, principalmente num\u00e9ricos Integraci\u00f3n En funci\u00f3n de la aplicaci\u00f3n Basados en temas, con mayor nivel de abstracci\u00f3n Calidad Medida en t\u00e9rminos de integridad Medida en t\u00e9rminos de consistencia Temporalidad Datos Solo datos actuales Datos actuales e hist\u00f3ricos Actualizaciones Continuas Peri\u00f3dicas Modelo Normalizado Desnormalizado, multidimensional Optimizaci\u00f3n Para acceso OLTP a parte de la BBDD Para acceso OLAP a gran parte de la BBDD <p>Tabla 2.1. Diferencias entre BBDD Operacionales y Almacenes de Datos</p>"},{"location":"UD2%20-%20Gesti%C3%B3n%20de%20soluciones%20II/index.html#23-almacenes-de-datos-arquitectura","title":"2.3 Almacenes de datos: Arquitectura","text":"<p>Las arquitecturas disponibles para el dise\u00f1o de almacenes de datos se basan, principalmente, en garantizar que el sistema cumpla una serie de propiedades esenciales para su \u00f3ptimo funcionamiento</p> <ul> <li> <p>Separaci\u00f3n: De los datos transaccionales y los datos estrat\u00e9gicos que sirven como punto de partida a la toma de de decisiones.</p> </li> <li> <p>Escalabilidad: A nivel hardware y software, para actualizarse y garantizar el correcto funcionamiento del sistema a medida que el n\u00famero de datos y usuarios aumenta.</p> </li> <li> <p>Extensiones: Permitiendo integrar e incluir nuevas aplicaciones sin necesidad de redise\u00f1ar el sistema completo.</p> </li> <li> <p>Seguridad: Monitorizando el acceso a los datos estrat\u00e9gicos guardados en el almac\u00e9n de datos.</p> </li> </ul> <p>Almac\u00e9n de datos</p> <p>\ud83d\udcc4 Las arquitecturas de almacenes de datos se clasifican, fundamentalmente, en dos tipos: arquitecturas orientadas a la estructura y arquitecturas orientadas a la empresa.</p>"},{"location":"UD2%20-%20Gesti%C3%B3n%20de%20soluciones%20II/index.html#231-arquitecturas-orientadas-a-la-estructura","title":"2.3.1 Arquitecturas orientadas a la estructura","text":"<p>Las arquitecturas orientadas a la estructura reciben su nombre debido a que est\u00e1n dise\u00f1adas poniendo especial \u00e9nfasis en el n\u00famero de capas y elementos que componen la arquitectura del sistema de almac\u00e9n de datos. De acuerdo con este criterio, es posible distinguir las siguientes arquitecturas.</p> <p>Arquitectura de una capa</p> <p>El objetivo principal de esta arquitectura, poco utilizada en la pr\u00e1ctica, es minimizar la cantidad de datos almacenados eliminando para ello los datos redundantes. La figura2.1 muestra un esquema de este tipo de arquitectura. En ella, el almac\u00e9n de datos creado es virtual, existiendo un middleware que interpreta los datos operacionales y ofrece una vista multidimensional de ellos.</p> <p>El principal inconveniente de esta arquitectura es que su simplicidad hace que el sistema no cumpla la propiedad de separaci\u00f3n, ya que los procesos de an\u00e1lisis se realizan sobre los datos operacionales.</p> <p></p> <p>Figura 2.1. Almac\u00e9n de datos. Arquitectura de una capa.</p> <p>Arquitectura de dos capas</p> <p>Fue dise\u00f1ada con el objetivo de solucionar el problema de la separaci\u00f3n que presentaba la arquitectura de una capa. Este esquema consigue subrayar la separaci\u00f3n entre los datos disponibles y el almac\u00e9n de datos a trav\u00e9s de los siguientes componentes (ver figura2.2[figura2.2):</p> <ul> <li> <p>Capa de origen (fuente): Se corresponde con los or\u00edgenes y fuentes de los datos heterog\u00e9neos que se pretenden incorporar al almac\u00e9n de datos.</p> </li> <li> <p>Puesta a punto: Proceso por el cual se utilizan herramientas de Extracci\u00f3n, Transformaci\u00f3n y Carga (ETL) para extraer, limpiar, filtrar, validar y cargar datos en el almac\u00e9n de datos.</p> </li> <li> <p>Capa de almac\u00e9n de datos: Almacenamiento centralizado de la informaci\u00f3n en el almac\u00e9n de datos, el cual puede ser utilizado para crear data marts o repositorios de metadatos.</p> </li> <li> <p>An\u00e1lisis: Conjunto de procesos a partir de los cuales los datos son eficientemente y flexiblemente analizados, generando informes y simulando escenarios hipot\u00e9ticos para dar soporte a la toma de decisiones.</p> </li> </ul> <p>Data mart</p> <p>\ud83d\udccb Data mart es un subconjunto o agregaci\u00f3n de los datos almacenados en un almac\u00e9n de datos primario que incluye informaci\u00f3n relevante sobre un \u00e1rea espec\u00edfica del negocio.</p> <p></p> <p>Figura 2.2. Almac\u00e9n de datos. Arquitectura de dos capas.</p> <p>Arquitectura de tres capas</p> <p>Este tercer tipo de arquitectura incluye una capa llamada de datos reconciliados o almac\u00e9n de datos operativos. Con esta capa, los datos operativos obtenidos tras la limpieza y depuraci\u00f3n son integrados y validados, proporcionando un modelo de datos de referencia para toda la organizaci\u00f3n.</p> <p>De este modo, el almac\u00e9n de datos no se nutre de los datos de origen directamente, sino de los datos reconciliados generados, los cuales tambi\u00e9n son utilizados para realizar de forma m\u00e1s eficiente tareas operativas, como la realizaci\u00f3n de informes o la alimentaci\u00f3n de datos a procesos operativos.</p> <p>Esta capa de datos reconciliados tambi\u00e9n puede implementarse de forma virtual en una arquitectura de dos capas, ya que se define como una vista integrada y coherente de los datos de origen. La figura2.3 muestra de forma gr\u00e1fica este tipo de arquitectura</p> <p></p> <p>Figura 2.3. Almac\u00e9n de datos. Arquitectura de tres capas.</p>"},{"location":"UD2%20-%20Gesti%C3%B3n%20de%20soluciones%20II/index.html#232-arquitecturas-orientadas-a-la-empresa","title":"2.3.2. Arquitecturas orientadas a la empresa","text":"<p>Esta clasificaci\u00f3n distingue cinco tipos de arquitecturas que combinan las capas mencionadas en la primera clasificaci\u00f3n para dise\u00f1ar almacenes de datos.</p> <p>1. Arquitectura de data marts independientes</p> <p>Arquitectura preliminar en la que los distintos data marts son dise\u00f1ados de forma independiente y construidos de forma no integrada. Suele utilizarse en los inicios de implementaci\u00f3n de proyectos de almacenes de datos y reemplazada a medida que el proyecto va creciendo.</p> <p>2. Arquitectura en bus</p> <p>Similar a la anterior, asegura la integraci\u00f3n l\u00f3gica de los data marts creados, ofreciendo una visi\u00f3n amplia de los datos de la empresa y permitiendo realizar an\u00e1lisis rigurosos de los procesos que en ella se llevan a cabo.</p> <p>3. Arquitectura hub-and-spoke (centro y radio)</p> <p>Esta arquitectura es muy utilizada en almacenes de datos de tama\u00f1os medio y grande. Su dise\u00f1o pone especial \u00e9nfasis en garantizar la escalabilidad del sistema y permitir a\u00f1adir extensiones al mismo.</p> <p>Para ello, los datos se almacenan de forma at\u00f3mica y normalizada en una capa de datos reconciliados que alimenta a los data marts construidos que contienen, a su vez, los datos agregados de forma multidimensional. Los usuarios acceden a los data marts, si bien es cierto que tambi\u00e9n pueden hacer consultas directamente sobre los datos reconciliados.</p> <p>4. Arquitectura centralizada</p> <p>Se trata de un caso particular de la arquitectura hub-and-spoke. En ella, la capa de datos reconciliados y los data marts se almacenan en un \u00fanico repositorio f\u00edsico.</p> <p>5. Arquitectura federada</p> <p>Se trata de un tipo de arquitectura muy utilizada en entornos din\u00e1micos, cuando se pretende integrar almacenes de datos o data marts existentes con otros para ofrecer un entorno \u00fanico e integrado de soporte a la toma de decisiones. De esta forma, cada almac\u00e9n de datos y cada data mart es integrado virtual o f\u00edsicamente con lo dem\u00e1s. Para ello, se utilizan una serie de t\u00e9cnicas y herramientas avanzadas como son las ontolog\u00edas, consultas distribuidas e interoperabilidad de metadatos, entre otras.</p>"},{"location":"UD2%20-%20Gesti%C3%B3n%20de%20soluciones%20II/index.html#24-almacenes-de-datos-diseno-e-implementacion","title":"2.4. Almacenes de datos: Dise\u00f1o e implementaci\u00f3n","text":"<p>En esta secci\u00f3n se profundizar\u00e1 en cada una de las etapas necesarias para dise\u00f1ar e implementar un almac\u00e9n de datos. Para ello, en primer lugar se describir\u00e1 el proceso de Extracci\u00f3n, Transformaci\u00f3n y Carga (ETL), fundamental para construir y alimentar un almac\u00e9n de datos. Despu\u00e9s, se desarrollar\u00e1n dos de los dise\u00f1os m\u00e1s extendidos de almac\u00e9n de datos: el dise\u00f1o en estrella y el dise\u00f1o en copo de nieve.</p> <p>ver presentaci\u00f3n BDA2.2</p>"},{"location":"UD2%20-%20Gesti%C3%B3n%20de%20soluciones%20II/index.html#241-el-proceso-de-extraccion-transformacion-y-carga-etl","title":"2.4.1 El proceso de Extracci\u00f3n, Transformaci\u00f3n y Carga (ETL)","text":"<p>El proceso ETL es el encargado de extraer, limpiar e integrar los datos provenientes de las fuentes de datos para alimentar el almac\u00e9n de datos. Este proceso tambi\u00e9n es el encargado de alimentar la capa de datos reconciliados en la arquitectura de tres capas. El proceso ETL tiene lugar cuando se puebla el almac\u00e9n de datos y se lleva a cabo cada vez que el almac\u00e9n de datos se actualiza. A continuaci\u00f3n, se describen detalladamente cada una de las fases de las que consta este proceso</p> <p>\u2b06\ufe0f Extracci\u00f3n</p> <p>Etapa que consiste en la lectura de los datos de las distintas fuentes de las que provienen. Cuando un almac\u00e9n de datos se rellena por primera vez, se suele utilizar la t\u00e9cnica de extracci\u00f3n est\u00e1tica, la cual consiste en extraer una instant\u00e1nea de los datos operacionales. A partir de entonces, se utiliza la extracci\u00f3n incremental para actualizar peri\u00f3dicamente los datos del almac\u00e9n de datos, recogiendo los cambios aplicados desde la \u00faltima extracci\u00f3n. Para ello, se utiliza el registro mantenido por el SGBD que, por ejemplo, asocia una marca de tiempo (timestamp) a los datos operacionales para registrar cuando fueron modificados y agilizar el proceso de extracci\u00f3n.</p> <p>En la actualidad, existe una gran cantidad de conjuntos de datos o data sets p\u00fablicos, conocidos bajo el nombre de Open Data, que abarcan una gran cantidad de dominios y con los que es posible trabajar para construir soluciones big data.  </p> <p>Open Data</p> <p>\ud83d\udccb Open Data: Se trata de datos que han sido generados por una fuente en particular, que abarcan un dominio tem\u00e1tico o disciplinar y tienen atributos, dentro de los cuales est\u00e1 la frecuencia de actualizaci\u00f3n. Adem\u00e1s, cuentan con una licencia espec\u00edfica que indica las condiciones de reutilizaci\u00f3n de los mismos.</p> <p>La fuente de los datos es en muchos de los casos el estado nacional, provincial, municipal u organizaciones comerciales. En otras ocasiones, la fuente de los datos es fruto del estudio o medici\u00f3n por parte de particulares. Los atributos de los conjuntos de datos deben especificar c\u00f3mo fueron obtenidos, incluyendo fechas de obtenci\u00f3n, actualizaci\u00f3n y validez, as\u00ed como el p\u00fablico involucrado, la metodolog\u00eda de recogida o muestreo, etc.</p> <p>Algunas de las fuentes m\u00e1s utilizadas en la actualidad para la obtenci\u00f3n de datos abiertos provienen de los centros nacionales e internacionales de estad\u00edsticas, como son el Instituto Nacional de Estad\u00edstica de Espa\u00f1a (INE) 1 , eurostat 2 , la oficina europea de estad\u00edsticas, la Organizaci\u00f3n Mundial de la Salud (OMS) 3 ... Por otra parte, existen repositorios p\u00fablicos de datos abiertos y a disposici\u00f3n de los usuarios como UCI4 4 o Kaggle 5 , que es una comunidad de cient\u00edficos de datos donde empresas y organizaciones suben sus datos y plantean problemas que son resueltos por los miembros de la comunidad. La figura 2.4 muestra las fuentes de datos que se acaban de mencionar.</p> <p></p> <p>Figura 2.4: Fuentes para la obtenci\u00f3n de datos abiertos</p> <p>ver presentaci\u00f3n BDA2.3</p> <p>\ud83d\udd04 Transformaci\u00f3n</p> <p>La etapa de transformaci\u00f3n es la fase clave para transformar los datos operativos en datos con un formato espec\u00edfico para alimentar un almac\u00e9n de datos. En esta etapa, los datos se limpian y se transforman, a\u00f1adi\u00e9ndoles contexto y significado. En caso de implementar un almac\u00e9n de datos siguiendo una arquitectura de tres capas, el proceso de transformaci\u00f3n es el encargado de obtener la capa de datos reconciliados. Si bien es cierto que algunos autores separan la limpieza y la transformaci\u00f3n de los datos en dos etapas distintas, en este cap\u00edtulo se considerar\u00e1n ambas dentro de la fase de transformaci\u00f3n.</p> <p>Transformaci\u00f3n</p> <p>\ud83d\udcc4 La etapa de transformaci\u00f3n engloba todos los procesos de limpieza y manipulaci\u00f3n de los datos, con el objetivo de transformar los datos operativos propios de sistemas relacionales (OLTP) en datos preparados para ser incluidos dentro del almac\u00e9n de datos (OLAP).</p> <p>La limpieza de los datos o data cleaning engloba todos aquellos procedimientos necesarios para detectar y resolver situaciones problem\u00e1ticas con los datos de partida que pudieran suponer problemas potenciales a la hora de analizarlos. As\u00ed pues, los datos de partida pueden ser incompletos, es decir, pueden contener atributos sin valor o valores agregados, incorrectos que incluyan errores o valores sin ning\u00fan significado, lo cual es com\u00fan cuando los datos se introducen manualmente en el sistema, o inconsistentes cuando los cambios no son propagados a todos los m\u00f3dulos del sistema, los rangos de un determinado atributo son cambiantes, existen datos duplicados...</p> <p>Otra tarea muy importante que se debe abordar en la fase de limpieza es la gesti\u00f3n de los datos perdidos. Se trata de datos que no est\u00e1n disponibles para algunas de las variables en alguna instancia. Esto puede ser debido, por ejemplo, a que no se registraran las modificaciones sufridas por los datos, se produjera un mal funcionamiento del equipo, los datos nunca fueron rellenados o no exist\u00edan en el momento en que fueron completados, se eliminaron por ser inconsistentes con la informaci\u00f3n registrada...</p> <p>En general, se distinguen dos tipos de situaciones cuando existen valores perdidos: datos perdidos completamente aleatorios y datos perdidos de forma no completamente aleatoria. En el segundo caso, puede ser interesante intentar analizar la raz\u00f3n de la p\u00e9rdida de los datos, la cual puede ser indicativa. En muchas ocasiones, los valores perdidos tienen relaci\u00f3n con un subconjunto de variables predictoras y no se encuentran aleatoriamente distribuidos por todas ellas. Por todo ello, las aproximaciones m\u00e1s comunes a la hora de gestionar datos perdidos son:</p> <ul> <li> <p>Eliminaci\u00f3n de instancias que contengan valores perdidos: consiste en establecer un porcentaje umbral de tal forma que, si una instancia contiene un n\u00famero de valores perdidos que supera este porcentaje, la instancia se descarta. Se trata de una t\u00e9cnica \u00fatil cuando el conjunto de datos es grande y el n\u00famero de instancias con valores perdidos no es muy elevado. No obstante, esta estrategia se debe utilizar con precauci\u00f3n, ya que puede suponer una excesiva p\u00e9rdida de informaci\u00f3n si el conjunto de datos no es muy grande, el n\u00famero de instancias con valores perdidos es alto o el porcentaje umbral es muy peque\u00f1o.</p> </li> <li> <p>Asignaci\u00f3n de valores fijos: consiste en asignar un valor fijo a todos los valores perdidos de todas las variables. Este valor puede ser el n\u00famero 0 o incluso un valor desconocido Unknown o no num\u00e9rico (NaN, Not a Number) en funci\u00f3n del lenguaje de programaci\u00f3n utilizado.</p> </li> <li> <p>Asignaci\u00f3n de valores de referencia: asigna un valor de referencia a los valores perdidos para cada variable. Estos valores de referencia suelen ser medidas de centralizaci\u00f3n como la media o la mediana de los valores de cada variable.</p> </li> <li> <p>Imputaci\u00f3n de valores perdidos: consiste en la aplicaci\u00f3n de t\u00e9cnicas m\u00e1s sofisticadas, como pueden ser t\u00e9cnicas estad\u00edsticas o de aprendizaje autom\u00e1tico para predecir o averiguar los valores que se han perdido.</p> </li> </ul> <p>Finalmente, la etapa de limpieza de datos tambi\u00e9n se encarga de la detecci\u00f3n de valores an\u00f3malos o outliers. Se trata de valores que se han introducido de forma err\u00f3nea o bien a una deformaci\u00f3n en la distribuci\u00f3n de valores. El proceso de detecci\u00f3n de anomal\u00edas consiste, fundamentalmente, en dos etapas: en primer lugar, asumir que existe un modelo generador de datos, como podr\u00eda ser una distribuci\u00f3n de probabilidad. En segundo lugar, considerar que las anomal\u00edas representan un modelo generador distinto, que no coincide con el original. Existen multitud de t\u00e9cnicas para detectar y descartar o imputar valores an\u00f3malos, como lo son t\u00e9cnicas estad\u00edsticas basadas en la desviaci\u00f3n y el rango intercuart\u00edlico o t\u00e9cnicas de aprendizaje autom\u00e1tico.</p> <p>ver presentaci\u00f3n BDA2.4</p> <p>Despu\u00e9s de la etapa de limpieza, comienza la etapa de transformaci\u00f3n/manipulaci\u00f3n. En ella, los datos se preparan para su carga en el almac\u00e9n de datos. Para ello, se transformar\u00e1n los datos provenientes de sistemas transaccionales OLTP en datos preparados para su an\u00e1lisis OLAP. A continuaci\u00f3n, se muestran algunos de los procesos de transformaci\u00f3n m\u00e1s comunes:</p> <ul> <li> <p>Estandarizaci\u00f3n de c\u00f3digos y formatos de representaci\u00f3n: incluye tareas como transformar la informaci\u00f3n EBCDIC a formato ASCII o Unicode, convertir n\u00fameros cardinales en ordinales o viceversa, homogeneizaci\u00f3n y tratamiento de fechas, expandir codificaciones a\u00f1adiendo textos descriptivos, unificaci\u00f3n de c\u00f3digos (sexo, estado civil, etc) y est\u00e1ndares (medidas, monedas, etc). </p> </li> <li> <p>Conversiones y combinaciones de campos: realizaci\u00f3n de agregaciones y c\u00e1lculos simples (importe neto, beneficio, realizaci\u00f3n de cuentas y promedios), c\u00e1lculos derivados con valores num\u00e9ricos y textuales... </p> </li> <li> <p>Correcciones: en caso de que no se pudiera hacer en el origen o en la etapa de limpieza, se deben corregir errores tipogr\u00e1ficos, datos sin sentido ni significado, resolver conflictos de dominio de variables, aclaraci\u00f3n de datos ambiguos y asignaci\u00f3n de valores a datos nulos.</p> </li> <li> <p>Integraci\u00f3n de varias fuentes: implica la resoluci\u00f3n de claves y utilizaci\u00f3n de diccionarios de datos y repositorios de metadatos para el dise\u00f1o del almac\u00e9n de datos.</p> </li> <li> <p>Eliminaci\u00f3n de datos y/o registros duplicados: frecuente cuando se trabaja con distintas fuentes de datos provenientes de diferentes dimensiones o departamentos de la organizaci\u00f3n. Para ello, es muy \u00fatil el uso de funciones de b\u00fasqueda y agrupamiento aproximado.</p> </li> <li> <p>Escalado y centrado: para reducir los efectos adversos de contar con datos dispersos, es muy \u00fatil utilizar t\u00e9cnicas para escalar y centrar los datos. Una posible transformaci\u00f3n consiste en restar a cada valor de cada instancia el valor medio de la variable correspondiente y despu\u00e9s dividir los valores por la desviaci\u00f3n est\u00e1ndar. De esta forma, los valores se escalan y se centran entorno al cero. Esta transformaci\u00f3n tambi\u00e9n puede hacerse entorno al valor de la media o la mediana. Tambi\u00e9n es posible escalar los valores en el intervalo [0\u22121] o aplicar el logaritmo, para reducir la dispersi\u00f3n de los datos y mejorar la visualizaci\u00f3n.</p> </li> <li> <p>Discretizaci\u00f3n: la aplicaci\u00f3n de muchas t\u00e9cnicas de aprendizaje autom\u00e1tico requiere que los valores de las variables sean discreto, en lugar de continuos. La discretizaci\u00f3n es el proceso por el cual se divide un intervalo de valores continuos en tantos fragmentos como etiquetas o valores discretos se quieran asignar, sustituyendo los valores originales por la etiqueta o valor discreto correspondiente.</p> </li> <li> <p>Selecci\u00f3n de caracter\u00edsticas: permite reducir el coste computacional de trabajar con todas las variables del conjunto de datos cuando, en muchas ocasiones, existen variables que no aportan informaci\u00f3n para el aprendizaje. Adem\u00e1s de reducir el coste computacional, se reduce el n\u00famero de dimensiones del conjunto de datos, mejorando la visualizaci\u00f3n y mejorando el rendimiento de los modelos no solo en t\u00e9rminos de tiempo, sino tambi\u00e9n de rendimiento, puesto que existen m\u00e9todos de aprendizaje cuyo rendimiento es peor cuando se ejecutan utilizando variables no significativas.</p> </li> </ul> <p>ver presentaci\u00f3n BDA2.5</p> <p>\ud83d\udd00 Carga</p> <p>Se trata de la \u00faltima fase de cara a incluir datos en el almac\u00e9n de datos. La carga inicial de los datos puede requerir bastante tiempo al cargar de forma progresiva todos los datos hist\u00f3ricos, por lo que es normal realizarla en horas de baja carga de los sistemas. Una vez que el almac\u00e9n de datos ha sido inicialmente cargado, las sucesivas cargas de datos se pueden realizar de dos formas:</p> <ul> <li> <p>Refresco: El almac\u00e9n de datos se reescribe completamente, de forma que se reemplazan los datos antiguos. El refresco es utilizado habitualmente junto con la extracci\u00f3n est\u00e1tica y suele ser una estrategia muy utilizada para la carga inicial del almac\u00e9n de datos, aunque puede tambi\u00e9n realizarse a posteriori.</p> </li> <li> <p>Actualizaci\u00f3n: Se a\u00f1aden al almac\u00e9n de datos solamente aquellos datos nuevos que se pretenden incluir, sin eliminar ni modificar los datos ya existentes. Esta t\u00e9cnica es utilizada frecuentemente junto con la extracci\u00f3n incremental para actualizar regularmente el almac\u00e9n de datos. Se trata de una estrategia de carga compleja de dise\u00f1ar, ya que requiere que sea eficiente computacionalmente. Para ello, se requieren mecanismos de detecci\u00f3n del cambio como aquellos basados en la marca de tiempo (timestamp) o la utilizaci\u00f3n de tablas de log, entre otros.</p> </li> </ul> <p>ver presentaci\u00f3n BDA2.6</p> <p>\u2b06\ufe0f\ud83d\udd04\ud83d\udd00 Frameworks ETL</p> <p>Recientemente, han aparecido m\u00faltiples frameworks que ofrecen herramientas tecnol\u00f3gicas para dar un soporte integrado al proceso ETL. A continuaci\u00f3n, se describen algunos de los m\u00e1s utilizados:</p> <ul> <li> <p>Bubbles http://bubbles.databrewery.org: Se trata de un framework ETL escrito en Python, aunque es posible utilizarlo desde otros lenguajes. Ofrece un marco de trabajo sencillo, donde el proceso ETL se describe como una secuencia de nodos conectados que representan los datos y las distintas operaciones que se pueden realizar con ellos. De esta forma, es posible construir un grafo que implemente el proceso ETL completo para un conjunto de datos.</p> </li> <li> <p>Apache Camel http://bubbles.databrewery.org: Este framework escrito en lenguaje Java y de acceso abierto se enfoca en facilitar la integraci\u00f3n entre distintas fuentes de datos, haciendo el proceso m\u00e1s accesible a los desarrolladores, ofreciendo disitntas herramientas para dar soporte al proceso ETL.</p> </li> <li> <p>Keetle https://help.hitachivantara.com/Documentation/Pentaho/9.3/Products/Pentaho_Data_Integration: Es el framework de Pentaho para dar soporte al proceso ETL. Pentaho es una suite o conjunto de programas para construir soluciones de inteligencia de negocio y Keetle es su entorno de trabajo ETL. Similar a Bubbles, ofrece un entorno de trabajo sencillo para implementar un proceso ETL a trav\u00e9s de nodos y conexiones entre ellos. Adem\u00e1s, Keetle es de acceso abierto.</p> </li> </ul>"},{"location":"UD2%20-%20Gesti%C3%B3n%20de%20soluciones%20II/index.html#242-diseno-en-estrella","title":"2.4.2 Dise\u00f1o en Estrella","text":"<p>A la hora de dise\u00f1ar un almac\u00e9n de datos, existen dos alternativas ampliamente utilizadas: el dise\u00f1o en estrella, que promueve el dise\u00f1o directo de estructuras l\u00f3gicas sobre el modelo relacional, y el dise\u00f1o en copo de nieve, como variante del dise\u00f1o en estrella.</p> <p>El proceso de desarrollo de un almac\u00e9n de datos siguiendo el dise\u00f1o en estrella puede estructurarse seg\u00fan las siguientes fases:</p> <ol> <li> <p>Elegir un proceso de negocio a modelar: cualquier proceso de negocio puede ser modelado como un almac\u00e9n de datos. No obstante, ser\u00e1n de especial inter\u00e9s aquellos procesos necesarios para la toma de decisiones y que involucran a diferentes unidades de la organizaci\u00f3n.</p> </li> <li> <p>Escoger la granularidad del proceso de negocio: Los datos almacenados en el almac\u00e9n de datos deben expresarse siempre al mismo nivel de detalle. Por este motivo es necesario escoger un nivel de granularidad al comienzo del dise\u00f1o del almac\u00e9n de datos.</p> </li> <li> <p>Selecci\u00f3n de medidas/hechos: indican qu\u00e9 se necesita medir o evaluar enel proceso de negocio con el fin de dar respuesta a las necesidades de informaci\u00f3n y toma de decisiones por las cuales se pretende dise\u00f1ar el almac\u00e9nde datos. Por ejemplo, en el \u00e1mbito log\u00edstico las medidas podr\u00edan ser las unidades aceptadas odevueltas, mientras que en el \u00e1mbito del control de calidad podr\u00edan ser lasunidades producidas, defectuosas, costo de la producci\u00f3n.</p> </li> <li> <p>Elegir las dimensiones que se aplicar\u00e1n a cada medida o hecho: las dimensiones especifican las distintas propiedades de las medidas o hechos que se pretenden almacenar e integrar dentro del almac\u00e9n de datos. Por ejemplo, si las medidas seleccionadas est\u00e1n relacionadas con las ventas de una determinada empresa, las dimensiones podr\u00edan ser: fecha de la venta, vendedor ,cliente, producto...</p> </li> </ol> <p>A la hora de dise\u00f1ar el almac\u00e9n de datos siguiendo el dise\u00f1o en estrella, se crea una tabla central, tambi\u00e9n llamada tabla de hechos, tabla factual o fact table. Esta tabla es la que posee los datos (medidas o hechos) sobre las diferentes combinaciones de las dimensiones. En algunas ocasiones, un dise\u00f1o en estrella presenta m\u00e1s de una tabla de hechos. Los hechos introducidos pueden ser de distintos tipos: completamente aditivos (total de ventas de un departamento), no aditivos (m\u00e1rgen de beneficios expresado como porcentaje) o semiaditivos (como datos intermedios que pueden agregarse con datos de otras dimensiones).</p> <p>Rodeando a la tabla de hechos, se encuentra una tabla por cada una de las dimensiones definidas. La clave primaria de la tabla de hechos se crea combinando las claves primarias de sus dimensiones relacionadas, de forma que est\u00e1 compuesta por las claves ajenas de las tablas de dimensiones que relaciona. De esta forma, la tabla de hechos presenta una relaci\u00f3n del tipo \u201cmuchos a muchos\u201d entre todas las tablas de dimensiones que relaciona, a la vez que una relaci\u00f3n \u201cmuchos a uno\u201d con cada tabla de dimensi\u00f3n por separado. Este esquema con forma de estrella da nombre a este tipo de dise\u00f1o.</p> <p>Ejemplo</p> <p>Sea un almac\u00e9n de datos en el que se pretende almacenar las ventas llevadas a cabo en una organizaci\u00f3n. La tabla central de hechos representa los datos de cada venta, mientras que las dimensiones que rodean a la tabla central recogen datos sobre los productos vendidos, la fecha de la venta, el canal de distribuci\u00f3n y el lugar de entrega. La figura2.5 muestra un ejemplo de este esquema de almac\u00e9n de datos.</p> <p></p> <p>Figura 2.5: Ejemplo de almac\u00e9n de datos dise\u00f1ado en estrella.</p> <p>Ventajas e incovenientes</p> <p>Entre las ventajas del dise\u00f1o en estrella, destaca ser un dise\u00f1o f\u00e1cil de modificar que proporciona tiempos r\u00e1pidos de respuesta, simplificando la navegaci\u00f3n de los medatadatos. Adem\u00e1s, este dise\u00f1o permite simular vistas de los datos que obtendr\u00e1n los usuarios finales y facilita la interacci\u00f3n con otras herramientas.</p> <p>Sin embargo, el dise\u00f1o en estrella presenta algunos inconvenientes que surgen, por ejemplo, cuando se combinan dimensiones con distintos niveles de detalle. Adem\u00e1s, cuando se pretende limitar los niveles de una dimensi\u00f3n se debe a\u00f1adir un campo de nivel o utilizar un modelo de tipo constelaci\u00f3n, donde se incluyen diferentes estrellas que almacenan tablas de hechos agregadas, lo cual a\u00f1ade complejidad al esquema. </p> <p>Resumen</p> <p>\ud83d\udcc4 El dise\u00f1o en estrella permite crear un almac\u00e9n de datos con una estructura centralizada. El dise\u00f1o se compone de una tabla de hechos central, que recoge los valores de las medidas del proceso de negocio que se pretende modelar. Rodeando a la tabla de hechos, se incluyen tantas tablas como dimensiones se hayan especificado en el modelo, las cuales se relacionan entre s\u00ed a trav\u00e9s de la tabla de hechos.</p> <p>ver presentaci\u00f3n BDA2.7</p>"},{"location":"UD2%20-%20Gesti%C3%B3n%20de%20soluciones%20II/index.html#243-diseno-en-copo-de-nieve","title":"2.4.3 Dise\u00f1o en Copo de Nieve","text":"<p>Otro de los dise\u00f1os m\u00e1s extendidos a la hora de implementar un almac\u00e9n de datos es el dise\u00f1o en copo de nieve. De hecho, el dise\u00f1o de copo de nieve es considerado una variante o derivado del dise\u00f1o en estrella y, por tanto, puede construirse a partir de este siguiendo las mismas etapas que se describieron en la secci\u00f3n anterior. </p> <p>Este dise\u00f1o es muy utilizado cuando se dispone de tablas de dimensi\u00f3n con una gran cantidad de atributos. En esta circunstancia, el dise\u00f1o de copo de nieve permite normalizar las tablas de dimensi\u00f3n, ofreciendo un mejor rendimiento cuando las consultas realizadas sobre el almac\u00e9n de datos requieren del uso de operadores de agregaci\u00f3n. De esta forma, la tabla de hechos deja de ser la \u00fanica tabla que presenta relaciones con las dem\u00e1s, apareciendo nuevas relaciones que se dan entre las tablas normalizadas que componen las tablas de dimensiones. Este esquema, que incluye ramificaciones en cada dimensi\u00f3n que se corresponden con las tablas necesarias para su normalizaci\u00f3n, guarda un gran parecido con la estructura de un copo de nieve, lo cual da nombre a este dise\u00f1o.</p> <p>La diferencia entre los dise\u00f1os en estrella y copo de nieve radica fundamentalmente en la modelizaci\u00f3n de las tablas de dimensiones. Para obtener un dise\u00f1o en copo de nieve, basta con partir de un dise\u00f1o en estrella en el que, tras un proceso de normalizaci\u00f3n, se obtienen varias tablas de dimensi\u00f3n a partir de una tabla desnormalizada. Dentro del dise\u00f1o de copo de nieve, es posible distinguir entre copo de nieve parcial, en el que solo algunas de las dimensiones son normalizadas y copo de nieve completo, en el que todas las dimensiones del esquema son normalizadas.</p> <p>Siguiendo con el ejemplo anterior, en el que se mostraba un almac\u00e9n de datos para representar las ventas realizadas por una organizaci\u00f3n, la figura2.6 muestra un dise\u00f1o para un almac\u00e9n de datos siguiendo un esquema de copo de nieve parcial, en el que las dimensiones geogr\u00e1fica y producto han sido normalizadas. </p> <p></p> <p>Figura 2.6: Ejemplo de almac\u00e9n de datos dise\u00f1ado en copo de nieve.</p> <p>Ventajas e Inconvenientes</p> <p>La normalizaci\u00f3n de las tablas de dimensiones proporciona al dise\u00f1o de copo de nieve la mejora de la eficiencia en la realizaci\u00f3n de consultas complejas que requieren el uso de operadores de agregaci\u00f3n. Sin embargo, este dise\u00f1o es conceptualmente m\u00e1s dif\u00edcil de implementar, ya que incluye un mayor n\u00famero de tablas y requiere de realizar muchos joins entre ellas, lo que incrementa los tiempos de consulta.</p> <p>En ocasiones, se requiere dise\u00f1ar un almac\u00e9n de datos que contenga m\u00e1s de una tabla de hechos. Para ello, el esquema o dise\u00f1o en constelaci\u00f3n permite crear un almac\u00e9n de datos de forma similar al dise\u00f1o en estrella, con la diferencia de que este modelo incluye m\u00e1s de una tabla de hechos. Adem\u00e1s, cada tabla de hechos puede vincularse con todas o solo algunas tablas de dimensiones. Este esquema contribuye a la reutilizaci\u00f3n de las tablas de dimensiones, las cuales se pueden relacionar con m\u00e1s de una tabla de hechos.</p> <p>Resumen</p> <p>\ud83d\udcc4 El dise\u00f1o en copo de nieve permite crear un almac\u00e9n de datos a partir de un dise\u00f1o en estrella. Para ello, las tablas de dimensiones se normalizan, generando m\u00e1s de una tabla para cada una de las dimensiones, mejorando la eficiencia de consultas complejas que requieren el uso de operadores avanzados.</p> <p>ver presentaci\u00f3n BDA2.8</p> <ol> <li> <p>https://www.ine.es \u21a9</p> </li> <li> <p>https://ec.europa.eu/eurostat \u21a9</p> </li> <li> <p>https://www.who.int/es/data/gho/publications/world-health-statistics \u21a9</p> </li> <li> <p>https://archive.ics.uci.edu/ml/index.php \u21a9</p> </li> <li> <p>https://www.kaggle.com \u21a9</p> </li> </ol>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html","title":"UD 3 - Gesti\u00f3n de Soluciones III","text":"<p>Este cap\u00edtulo profundiza en dos soluciones de almacenamiento y gesti\u00f3n de datos propias de las bases de datos NoSQL: las bases de datos XML y las bases de datos documentales.</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#31-bases-de-datos-nosql","title":"3.1 Bases de datos NoSQL","text":"<p>El t\u00e9rmino NoSQL proviene del ingl\u00e9s y est\u00e1 formado por las palabras \"No\" y \"SQL\". El significado literal del t\u00e9rmino no debe confundir al lector ni alejarlo de su correcto significado. NoSQL no engloba solamente un conjunto de bases de datos y sistemas de almacenamiento que no utilizan SQL para su gesti\u00f3n. El significado de NoSQL ampl\u00eda lo anterior, siendo el acr\u00f3nimo de \"Not only SQL\". Por tanto, NoSQL es una categor\u00eda general de sistemas de bases de datos que, adem\u00e1s de SQL, utilizan otra serie de tecnolog\u00edas para almacenar y gestionar los datos. El t\u00e9rmino NoSQL fue acu\u00f1ado en 1998 por Carlo Strozzy y retomado en el a\u00f1o 2009 por Eric Evans, quien aprovech\u00f3 este t\u00e9rmino para referirse a esta nueva generaci\u00f3n de familias de bases de datos como \"Big Data\".</p> <p>Las transacciones realizadas en los sistemas de bases de datos relacionales garantizan la consistencia y escalabilidad de las operaciones a trav\u00e9s de una serie de caracter\u00edsticas especificadas en el modelo ACID.</p> <ul> <li> <p>Atomicidad (A): Garantiza que una transacci\u00f3n se ha realizado completamente o no, evitando que algunas operaciones se ejecuten a medias.</p> </li> <li> <p>Consistencia (C): Tambi\u00e9n conocida como integridad, asegura que cualquier transacci\u00f3n realizada desde un estado seguro en la base de datos conducir\u00e1 a esta a otro estado tambi\u00e9n seguro o v\u00e1lido.</p> </li> <li> <p>Aislamiento (I: Isolation): Permite que el resultado en el estado del sistema sea el mismo, independientemente del modo de ejecuci\u00f3n de las transacciones (secuencial o concurrente).</p> </li> <li> <p>Durabilidad (D): O persistencia, garantiza que a pesar de que se produzca un fallo en el sistema, los cambios realizados por una transacci\u00f3n persistir\u00e1n.</p> </li> </ul> <p>Alternativamente, las bases de datos NoSQL siguen el modelo conocido como BASE, m\u00e1s flexible que ACID en aquellos sistemas de bases de datos que no se adhieren estrictamente al modelo relacional.</p> <ul> <li> <p>Disponibilidad b\u00e1sica (Basic Availability): El sistema siempre ofrece una respuesta a una petici\u00f3n de datos, aunque los datos sean inconsistentes, est\u00e9n en fase de cambio o incluso se produzca un fallo.</p> </li> <li> <p>Soft-state (S): La consistencia de este tipo de bases de datos es eventual, por lo que el estado del sistema cambia a lo largo del tiempo, aunque no haya habido entradas de datos.</p> </li> <li> <p>Eventual Consistency (E): Cuando se dejan de recibir datos, el sistema se vuelve consistente. As\u00ed pues, los datos se propagan a todo el sistema, pero este contin\u00faa recibiendo datos sin evaluar la consistencia de los mismos para cada transacci\u00f3n antes de avanzar a la siguiente.</p> </li> </ul> <p>En la actualidad, las aplicaciones web modernas, el auge de la computaci\u00f3n ubicua y el Big Data, presentan desaf\u00edos muy diferentes a los que presentan los sistemas de informaci\u00f3n tradicionales, implementados por medio de sistemas de bases de datos relacionales. Algunos desaf\u00edos son: el procesamiento masivo de datos, la alta frecuencia de lecturas y escrituras, los cambios din\u00e1micos y frecuentes en el esquema de datos, la escalabilidad a costes razonables, gesti\u00f3n de datos temporales...En este contexto, en los \u00faltimos a\u00f1os han aparecido multitud de enfoques, modelos y tecnolog\u00edas para aportar soluciones a los desaf\u00edos mencionados anteriormente. En este cap\u00edtulo, se profundiza en las bases de datos XML y las bases de datos documentales.</p> <p>Base de datos NoSQL</p> <p>\ud83d\udcc4 Base de datos NoSQL: Sistema de bases de datos no relacional que utiliza distintas tecnolog\u00edas para la gesti\u00f3n y almacenamiento distribuido de datos masivos sin un esquema estricto subyacente. Esta familia de base de datos permite incrementar la disponibilidad y escalabilidad del sistema, mejorando su rendimiento.</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#32-bases-de-datos-xml","title":"3.2 Bases de datos XML","text":"<p>El lenguaje XML (eXtensible Markup Language) es un lenguaje de marcas extensible derivado del lenguaje SGML (Standard Generalized Markup Language). Se trata de un lenguaje ideado para la definici\u00f3n y gesti\u00f3n de documentos que permite representar datos estructurados y semi-estructurados. En la actualidad, XML se ha convertido en un formato est\u00e1ndar para la comunicaci\u00f3n entre aplicaciones, facilitando su integraci\u00f3n. Entre las principales ventajas de XML destacan:</p> <ul> <li> <p>Datos autodocumentados: Gracias a la posibilidad de definir cualquier tipo de etiquetas, no es necesario conocer el esquema para entender el significado de los datos. </p> </li> <li> <p>Extensi\u00f3n: Permite adaptar el lenguaje f\u00e1cilmente a cualquier dominio. Anidamiento: La definici\u00f3n de estructuras anidadas dota a los documentos de gran flexibilidad para transferir y consultar informaci\u00f3n. </p> </li> <li> <p>Flexibilidad: Los documentos XML no tienen un formato estricto ni r\u00edgido, permitiendo a\u00f1adir o ignorar informaci\u00f3n en ellos. </p> </li> </ul> <p>Estructura de un documento XML</p> <p>La estructura de un documento XML est\u00e1 compuesta de elementos. Un elemento es una porci\u00f3n de documento delimitado por un par de etiquetas &lt; &gt; (apertura) y &lt; /&gt; (cierre) entre las cuales se incluye un texto llamado contexto del elemento. Un elemento sin contexto puede abreviarse mediante el uso de una \u00fanica etiquete . La estructura de un XML se forma anidando elementos para especificar la estructura del documento, formando una estructura de \u00e1rbol. El fragmento de c\u00f3digo 3.1 representa la estructura b\u00e1sica de un documento XML que representa un listado de asignaturas.</p> <p><pre><code>&lt;Asignaturas_Primero&gt;\n&lt;Asignatura&gt;\n&lt;titulo&gt; C\u00e1lculo &lt;/titulo&gt;\n&lt;tipo&gt; Anual &lt;/tipo&gt;\n&lt;profesor&gt;\n&lt;nombre&gt; Ana &lt;/nombre&gt;\n&lt;apellido&gt; Sanz &lt;/apellido&gt;\n&lt;/profesor&gt;\n&lt;profesor&gt;\n&lt;nombre&gt; Roberto &lt;/nombre&gt;\n&lt;apellido&gt; Hern\u00e1ndez &lt;/apellido&gt;\n&lt;/profesor&gt;\n&lt;/Asignatura&gt;\n&lt;Asignatura&gt;\n&lt;titulo&gt; F\u00edsica &lt;/titulo&gt;\n&lt;tipo&gt; Semestral &lt;/tipo&gt;\n&lt;profesor&gt;\n&lt;nombre&gt; Carmelo &lt;/nombre&gt;\n&lt;apellido&gt; Moya &lt;/apellido&gt;\n&lt;/profesor&gt;\n&lt;/Asignatura&gt;\n&lt;/Asignaturas_Primero&gt;\n</code></pre> Listado 3.1: Listado de Asignaturas</p> <p>Los elementos de un documento pueden contener atributos. Los atributos se incluyen en la definici\u00f3n del elemento, dentro de la etiqueta de apertura. Sint\u00e1cticamente, se representan mediante un par nombre = valor. A diferencia de un subelemento, un atributo solo puede aparecer una vez en una etiqueta. El uso de atributos y su diferencia con respecto a los elementos no siempre es evidente. A nivel de documento, los atributos se introducen para a\u00f1adir texto que no se visualizar\u00e1 en el documento, mientras que los subelementos formar\u00e1n parte del contenido del documento. A nivel de datos, la diferencia es insustancial, puesto que la misma informaci\u00f3n se puede representar utilizando atributos o subelementos. Por ejemplo, el subelemento &lt; tipo &gt; de una asignatura, podr\u00eda representarse como un atributo del elemento asignatura. El listado 3.2 representa la asignatura de C\u00e1lculo del ejemplo anterior, incluyendo el t\u00edtulo de la asignatura como atributo de la misma.</p> <p><pre><code>&lt;Asignaturas_Primero&gt;\n&lt;Asignatura&gt;\n&lt;titulo&gt; C\u00e1lculo tipo = 'anual' &lt;/titulo&gt;\n&lt;tipo&gt; Anual &lt;/tipo&gt;\n&lt;profesor&gt;\n&lt;nombre&gt; Ana &lt;/nombre&gt;\n&lt;apellido&gt; Sanz &lt;/apellido&gt;\n&lt;/profesor&gt;\n&lt;profesor&gt;\n&lt;nombre&gt; Roberto &lt;/nombre&gt;\n&lt;apellido&gt; Hern\u00e1ndez &lt;/apellido&gt;\n&lt;/profesor&gt;\n&lt;/Asignatura&gt;\n&lt;/Asignaturas_Primero&gt;\n</code></pre> Listado 3.2: Uso de atributos para la definici\u00f3n de asignaturas</p> <p>Dado que una de las principales aplicaciones de XML es el intercambio de informaci\u00f3n entre organizaciones, y ante la flexibilidad del lenguaje que permite definir cualquier nombre para una etiqueta, es posible que existan conflictos entre los nombres de \u00e9stas. Para ello, se recurre a los espacios de nombres, los cuales permiten a las organizaciones especificar nombres de etiquetas \u00fanicos. De esta forma, se a\u00f1ade al nombre de la etiqueta o atributo un identificador de recursos universal. El listado muestra de forma resumida un ejemplo de utilizaci\u00f3n del espacio de nombres para la definici\u00f3n de asignaturas.</p> <p><pre><code>&lt;Asignatura xmlns:AS=\"http://www.listado-asignaturas.com\"&gt;\n&lt;AS:titulo&gt; C\u00e1lculo tipo = 'anual' &lt;/AS:titulo&gt;\n&lt;AS:tipo&gt; Anual &lt;/AS:tipo&gt;\n&lt;AS:profesor&gt;\n&lt;AS:nombre&gt; Ana &lt;/AS:nombre&gt;\n&lt;AS:apellido&gt; Sanz &lt;/AS:apellido&gt;\n&lt;/AS:profesor&gt;\n&lt;AS:profesor&gt;\n&lt;AS:nombre&gt; Roberto &lt;/AS:nombre&gt;\n&lt;AS:apellido&gt; Hern\u00e1ndez &lt;/AS:apellido&gt;\n&lt;/AS:profesor&gt;\n&lt;/Asignatura&gt;\n</code></pre> Listado 3.3: Uso de un espacio de nombres</p> <p>Al igual que ocurre en las bases de datos relacionales, las bases de datos XML presentan una serie de lenguajes de definici\u00f3n de tipos de documentos XML. Concretamente, a continuaci\u00f3n se profundizaran en DTD y XML Schema. Seguidamente, se estudiar\u00e1n los lenguajes XPath, XSLT y XQuery, como lenguajes de manipulaci\u00f3n de datos XML para la definici\u00f3n y ejecuci\u00f3n de consultas.</p> <p>ver presentaci\u00f3n BDA3.1</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#321-definition-type-document-dtd","title":"3.2.1 Definition Type Document (DTD)","text":"<p>En XML, se dice que un documento est\u00e1 bien formado si cumple con la sintaxis del lenguaje: correcta definici\u00f3n de elementos y atributos, correcto anidamiento...Como se puede apreciar, este concepto alude a la dimensi\u00f3n sint\u00e1ctica de los documentos.</p> <p>Por otra parte, se dice que un documento XML es v\u00e1lido cuando est\u00e1 bien formado y cumple con la estructura de documento que ha sido dada a trav\u00e9s de un lenguaje de definici\u00f3n de datos. Esta definici\u00f3n de la estructura de los documentos XML es opcional, si bien es cierto que es recomendable, especialmente cuando se trabaja con documentos complejos y con un volumen grande de ellos. La definici\u00f3n de tipos de documento o DTD permite definir la estructura de un documento XML. Para ello, la DTD especifica la estructura de los elementos y atributos de un documento: qu\u00e9 elementos pueden aparecer, qu\u00e9 atributos, qu\u00e9 subelementos, multiplicidad de ellos... Existen multitud de validadores on-line de documentos XML y sus correspondientes DTDs y/o XML Schemas1.</p> <p>La declaraci\u00f3n de una DTD dentro de un documento XML puede implementarse de dos formas: en primer lugar, de manera interna. En este caso, se a\u00f1ade la DTD despu\u00e9s del pre\u00e1mbulo XML y justo antes del documento propiamente dicho. El listado 3.4 muestra un ejemplo de declaraci\u00f3n interna de una DTD.</p> <p><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\" ?&gt;\n&lt;!DOCTYPE Elemento_Raiz [Aqu\u00ed la DTD]&gt;\n&lt;!-------Aqu\u00ed va el documento XML-----&gt;\n</code></pre> Listado 3.4: Declaraci\u00f3n Interna DTD</p> <p>En segundo lugar, la DTD puede declararse de forma externa, definiendo la misma en un fichero .DTD aparte que despu\u00e9s es enlazado en el documento. El listado 3.5 muestra un ejemplo de declaraci\u00f3n externa de una DTD.</p> <p><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\" ?&gt;\n&lt;!DOCTYPE Elemento_Raiz SYSTEM \"Mi_DTD.dtd\"&gt;\n&lt;!-------Aqu\u00ed va el documento XML-----\n</code></pre> Listado 3.5: Declaraci\u00f3n Externa DTD</p> <p>Siguiendo con el ejemplo del listado 3.1, el listado 3.6 muestra el archivo .dtd que validar\u00eda la estructura del documento de asignaturas.</p> <p><pre><code>&lt;\u00a1 DOCTYPE Listado_Asignaturas[\n&lt;! ELEMENT Asignaturas_Primero (Asignatura+)&gt;\n&lt;! ELEMENT Asignatura (titulo, tipo, profesor+)&gt;\n&lt;! ELEMENT titulo (#PCDATA)&gt;\n&lt;! ELEMENT tipo (#PCDATA)&gt;\n&lt;! ELEMENT profesor (nombre, apellido)&gt;\n&lt;! ELEMENT nombre (#PCDATA)&gt;\n&lt;! ELEMENT apellido (#PCDATA)&gt;\n</code></pre> Listado 3.6: DTD Listado de Asignaturas</p> <p>En el listado anterior, se define en primer lugar el tipo de documento \"Listado_Asignaturas\". A continuaci\u00f3n, y especificados entre corchetes, se describen los elementos que contendr\u00e1 el documento, comenzando por el elemento ra\u00edz \"Asignaturas_Primero\". Este elemento estar\u00e1 formado por uno o m\u00e1s elementos \"Asignatura\" (especificado mediante el operador +). Seguidamente, el elemento \"Asignatura\" viene dado por un conjunto de tres subelementos: t\u00edtulo, tipo y profesor. Adem\u00e1s, cada asignatura puede tener uno o m\u00e1s profesores. Los elementos t\u00edtulo y tipo vienen dados por cadenas de caracteres (denotadas con la instrucci\u00f3n #PCDATA) mientras que el elemento profesor estar\u00e1 formado por dos subelementos llamados \"nombre\" y \"apellido\" ambos definidos mediante cadenas de caracteres.</p> <p>La definici\u00f3n de atributos para un elemento se hace a continuaci\u00f3n de la definici\u00f3n del mismo utilizando &lt;!ATTLIST &gt;. El listado 3.7 muestra la DTD anterior en caso de que el tipo de asignatura sea considerado un atributo. </p> <p><pre><code>&lt;\u00a1 DOCTYPE Listado_Asignaturas[\n&lt;! ELEMENT Asignaturas_Primero (Asignatura+)&gt;\n&lt;! ELEMENT Asignatura (titulo, profesor+)&gt;\n&lt;! ATTLIST tipo id #CDATA REQUIRED&gt;\n&lt;! ELEMENT titulo (#PCDATA)&gt;\n&lt;! ELEMENT tipo (#PCDATA)&gt;\n&lt;! ELEMENT profesor (nombre, apellido)&gt;\n&lt;! ELEMENT nombre (#PCDATA)&gt;\n</code></pre> Listado 3.7: DTD Listado de Asignaturas con atributos</p> <p>Finalmente, la tabla 3.1a muestra un resumen de definici\u00f3n de elementos y atributos en una DTD.</p> Tipos ELEMENTO #PCDATA Cadena de caracteres EMPTY Elemento sin contexto ANY Cualquier tipo Operadores ELEMENTO | Disyunci\u00f3n + Uno o m\u00e1s * Cero o m\u00e1s ? Cero o uno <p>Tabla 3.1a: Descripci\u00f3n DTD de elementos y atributos XML</p> <p>Y la tabla 3.1b muestra un resumen de definici\u00f3n de atributos en una DTD.</p> Tipos ATRIBUTOS CDATA Cadena de caracteres ID Identificador IDREF Referencia a un ID Operadores ELEMENTO #REQUIRED Obligatorio #IMPLIED Opcional #FIXED Valor fijo #DEFAULT Valor por defecto <p>Tabla 3.1a: Descripci\u00f3n DTD de atributos XML</p> <p>ver presentaci\u00f3n BDA3.2</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#322-xml-schema","title":"3.2.2 XML Schema","text":"<p>Se trata de un lenguaje de definici\u00f3n de la estructura de documentos XML m\u00e1s sofisticado que DTD. Aunque es m\u00e1s complejo, soluciona muchos de los inconvenientes de las DTD. De hecho, XML Schema es un superconjunto del lenguaje DTD especificado mediante la sintaxis de XML. Entre otros aspectos, XML Schema soporta no solo el tipo cadena de caracteres (soportado por DTD), sino tambi\u00e9n tipos num\u00e9ricos, tipos complejos como listas y adem\u00e1s permite al usuario definir sus propios tipos as\u00ed como extender tipos de datos complejos mediante un mecanismo similar a la herencia. Adem\u00e1s, XML Schema soporta el concepto de espacios de nombre, permitiendo reutilizar elementos de un esquema en otros.</p> <p>Un XML Schema se especifica en un documento aparte del documento XML con extensi\u00f3n .xsd. Dentro de este archivo, que sigue las reglas sint\u00e1cticas de cualquier documento XML, se especifican las definiciones de cada elemento del esquema, qu\u00e9 atributos incluye as\u00ed como sus tipos de datos v\u00e1lidos asociados tanto a los elementos como a los atributos. De esta forma, XML Schema permite definir un esquema para validar documentos XML, de forma m\u00e1s sofisticada a como lo hace DTD. El listado 3.8 muestra un XML Schema para el ejemplo del listado de asignaturas.</p> <p><pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;xs:schema attributeFormDefault=\"unqualified\" elementFormDefault=\"qualified\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"&gt;\n&lt;xs:element name=\"Asignaturas_Primero\"&gt;\n&lt;xs:complexType&gt;\n&lt;xs:sequence&gt;\n&lt;xs:element maxOccurs=\"unbounded\" name=\"Asignatura\"&gt;\n&lt;xs:complexType&gt;\n&lt;xs:sequence&gt;\n&lt;xs:element name=\"titulo\" type=\"xs:string\" /&gt;\n&lt;xs:element name=\"tipo\" type=\"xs:string\" /&gt;\n&lt;xs:element maxOccurs=\"unbounded\" name=\"profesor\"&gt;\n&lt;xs:complexType&gt;\n&lt;xs:sequence&gt;\n&lt;xs:element name=\"nombre\" type=\"xs:string\" /&gt;\n&lt;xs:element name=\"apellido\" type=\"xs:string\" /&gt;\n&lt;/xs:sequence&gt;\n&lt;/xs:complexType&gt;\n&lt;/xs:element&gt;\n&lt;/xs:sequence&gt;\n&lt;/xs:complexType&gt;\n&lt;/xs:element&gt;\n&lt;/xs:sequence&gt;\n&lt;/xs:complexType&gt;\n&lt;/xs:element&gt;\n&lt;/xs:schema&gt;\n</code></pre> Listado 3.8: XML Schema: Listado de asignaturas</p> <p>El listado anterior comienza definiendo un XML Schema en la segunda l\u00ednea de c\u00f3digo. Cualquier XML Schema deber\u00e1 incluir esta l\u00ednea de c\u00f3digo. El atributo attributeFormDefault = \"unqualified\" indica que no es necesario utilizar el prefijo del espacio de nombres para definir los atributos del esquema, mientras que el atributo elementFormDefault = \"qualified\" indica que no es necesario utilizar el prefijo del espacio de nombre para definir los elementos del esquema.</p> <p>A continuaci\u00f3n, se define el elemento ra\u00edz \"Asignaturas_Primero\" el cual se trata de un tipo complejo, puesto que no se define con un tipo de datos b\u00e1sico como entero, car\u00e1cter, etc. Este elemento est\u00e1 formado por una secuencia de elementos del tipo \"Asignatura\" (el n\u00famero m\u00e1ximo de elementos Asignatura no est\u00e1 limitado ya que maxOccurs = \"unbounded\". Los elementos Asignatura, a su vez, est\u00e1n formados de una secuencia de elementos \"nombre\", \"tipo\" (ambos de tipo cadena de caracteres) y un conjunto de elementos de tipo \"profesor\" no acotados, el cual se compone a su vez de los elementos \"nombre\" y \"apellido\", ambos de tipo cadena de caracteres.</p> <p>XML Schema permite representar la cardinalidad de un elemento, definiendo el n\u00famero m\u00ednimo y m\u00e1ximo de ocurrencias mediante los atributos minOccurs y maxOccurs. Adem\u00e1s, tambi\u00e9n es posible a\u00f1adir restricciones de forma que se limiten los valores de un elemento o atributo, se permita elegir entre una lista de posibles...Finalmente, XML Schema dispone de los contenedores sequence para definir una secuencia ordenada de subelementos, chocie para definir la elecci\u00f3n entre posibles grupos de elementos o elementos y all para definir un conjunto no ordenado de elementos. Al igual que suced\u00eda con DTD, existen distintas herramientas on-line que no solo permiten validar un documento XML a partir de su esquema XSD, sino que tambi\u00e9n permiten generar el esquema XSD a partir del archivo XML2.</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#322-xpath","title":"3.2.2 XPath","text":"<p>En cualquier sistema de bases de datos, adem\u00e1s de los lenguajes de definici\u00f3n de datos, los lenguajes de manipulaci\u00f3n de datos son esenciales para poder realizar consultas y recuperar datos, realizar actualizaciones, etc.</p> <p>XPath es un lenguaje de manipulaci\u00f3n de datos XML. La principal caracter\u00edstica de este lenguaje es que interpreta un documento XML como una estructura de datos de tipo \u00e1rbol, donde los nodos y sus respectivas hojas se corresponden con los elementos y subelementos del documento XML. De esta forma, y utilizando comandos que permiten recorrer el documento de forma an\u00e1loga a los comandos que se utilizan en una terminal de comandos para recorrer el sistema de archivos de un computador, es posible realizar consultas. La tabla 3.2 muestra un listado de los principales operadores utilizados en XPath.</p> Operador Significado / Navegaci\u00f3n entre nodos del documento. Selecciona los nodos del nivel inferior. // Navegaci\u00f3n entre nodos del documento. Selecciona los nodos del nivel inferior de cualquier nodo especificado a continuaci\u00f3n . Selecciona el nodo actual .. Selecciona el nodo padre o nodo de nivel superior | Expresa alternativas @ Selecciona un atributo de un nodo * Selecciona todos los nodos [] Agrupa otros operadores <p>Tabla 3.2: XPath: Operadores b\u00e1sicos</p> <p>Con estos operadores, veamos c\u00f3mo realizar algunas consultas de ejemplo sobre el fragmento XML que especifica un listado de asignaturas.</p> <p>??? question \"Consulta 1. Recuperar los nombres de todas las asignaturas.\"</p> <pre><code>/Asignaturas_Primero/Asignatura/titulo/text()\n</code></pre> <p>??? question \"Consulta 2. \u00bfCu\u00e1ntas asignaturas son impartidas por m\u00e1s de un profesor?\"</p> <pre><code>/Asignaturas_Primero/Asignatura[count(profesor)&gt;1]\n</code></pre> <p>??? question \"Consulta 3. \u00bfQui\u00e9nes son los profesores que imparten la asignatura de f\u00edsica?\"</p> <pre><code>/Asignaturas_Primero/Asignatura[titulo=\"Fisica\"]/profesor/nombre\n</code></pre> <p>Es posible utilizar XPath de forma on-line a trav\u00e9s de visualizadores3 o, por ejemplo, instalando el plug-in XPatherizer de Notepad++, en caso de utilizar este \u00faltimo editor de c\u00f3digo XML.</p> <p>ver presentaci\u00f3n BDA3.3</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#324-xslt","title":"3.2.4 XSLT","text":"<p>Otro lenguaje de manipulaci\u00f3n de datos XML es el lenguaje XSLT. El lenguaje XSLT (XML StylesSheet Transformations) proviene del lenguaje XSL (XML StylesSheet). Este \u00faltimo permite especificar las opciones de formato de un documento XML en un archivo separado, aislando as\u00ed la especificaci\u00f3n del contenido y la presentaci\u00f3n de un archivo XML. Podr\u00eda decirse que XSL es a XML lo que CSS es a HTML. El lenguaje XSLT permite especificar las opciones e formato de un documento XML, pudiendo transformar este documento en otros formatos como HTML, PDF, etc. Dada la potencia de este lenguaje, que es capaz de transformar un archivo XML en otro que tambi\u00e9n pudiera ser XML, se utiliza frecuentemente como lenguaje de consulta.</p> <p>Las transformaciones XSLT se definen por medio de plantillas, que permiten a su vez recuperar contenido del documento XML a trav\u00e9s de la utilizaci\u00f3n de expresiones XPath. La principal caracter\u00edstica de XSLT es que la aplicaci\u00f3n de plantillas se realiza de forma recursiva, lo cual recibe el nombre de recursividad estructural. El listado 3.9 presenta un ejemplo de aplicaci\u00f3n de una transformaci\u00f3n XSLT por medio de una plantilla.</p> <p><pre><code>&lt;xsl: template match = /Asignaturas_Primero/Asignatura &gt;\n&lt;titulo_asignatura&gt;\n&lt;xsl:value-of select = titulo/&gt;\n&lt;/titulo_asignatura&gt;\n&lt;/xsl:template&gt;\n</code></pre> Listado 3.9: XSLT: Ejemplo de plantilla</p> <p>Tal y como se puede observar en el fragmento anterior, una transformaci\u00f3n o consulta XSLT se define mediante dos \u00f3rdenes b\u00e1sicas: match que especifica una expresi\u00f3n XPath para seleccionar uno o m\u00e1s nodos y value \u2212 of select que devuelve los valores especificados de los nodos que se han obtenido como resultado de la expresi\u00f3n XPath. Merece la pena destacar que cualquier texto o etiqueta del archivo XSLT que no est\u00e9 en el espacio de nombre se copia a la salida sin cambios. Esto quiere decir, que el resultado del fragmento anterior no son \u00fanicamente los t\u00edtulos de las asignaturas, sino estos mismos delimitados por una etiqueta de apertura y cierre llamada &lt; titulo_asignatura &gt;. De esta forma, a partir de un documento XML se ha generado otro con el resultado de la consulta. A modo de resumen, la tabla 3.3 define los principales constructores XSLT.</p> Constructor XSLT Significado &lt; xsl:template match = \"expresion XPath\" &gt; Selecciona nodos a devolver Devuelve todos los nodos que no coinciden con alguna otra plantilla Selecciona el nodo actual A\u00f1ade un atributo al elemento precedente Aplica la recursividad estructural Define una clave Devuelve los nodos que coincidan con el valor especificado. Especifica operaciones de combinaci\u00f3n (JOINS) Devuelve los nodos resultados, ordenados por un elemento o atributo <p>Tabla 3.3: Principales Constructores XSLT</p> <p>Para trabajar con XSLT, se aconseja la descarga del software XML Copy Editor https://xml-copy-editor.sourceforge.io.</p> <p>ver presentaci\u00f3n BDA3.4</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#325-xquery","title":"3.2.5 XQuery","text":"<p>Es un lenguaje de consulta de prop\u00f3sito general sobre datos XML estandarizado por el consorcio W3C. XQuery procede del lenguaje de programaci\u00f3n Quilt y, por este motivo, toma prestadas muchas caracter\u00edsticas de otros lenguajes como XPath o SQL. De hecho, la sintaxis de XQuery es mucho m\u00e1s similar a la de SQL, al contrario que ocurr\u00eda con lenguajes como XPath o XSLT. </p> <p>Para definir una consulta en XQuery se construye una expresi\u00f3n, denominada \"FLWR\"(flower, flor en ingl\u00e9s). A continuaci\u00f3n, se describen cada una de las sentencias de las que se compone la expresi\u00f3n:</p> <ul> <li> <p>FOR: Obtiene una serie de variables cuyos valores son el resultado de una expresi\u00f3n XPath. Es el equivalente a la cl\u00e1usula FROM del lenguaje SQL.</p> </li> <li> <p>LET: Define y renombra expresiones complejas para ser utilizadas en el resto de la consulta, reduciendo la complejidad y aliviando la sintaxis. Es una cl\u00e1usula opcional.</p> </li> <li> <p>WHERE: En esta cl\u00e1usula, llamada igualmente en SQL, se especifican condiciones sobre los resultados obtenidos en la cl\u00e1usula FOR.</p> </li> <li> <p>RETURN: Especifica y define el resultado que se va a obtener de la consulta. Se trata del equivalente a la cl\u00e1usula SELECT en SQL.</p> </li> </ul> <p>Siguiendo la sintaxis anteriormente mencionada, el listado 3.10 muestra un ejemplo para obtener un listado con las asignaturas anuales.</p> <p><pre><code>for $x in /Asignaturas_Primero/Asignatura\nlet $nombre_asignatura:= $x/titulo\nwhere $x/tipo='anual'\nreturn &lt;asignatura_anual&gt; $nombre_asignatura &lt;/asignatura_anual&gt;\n</code></pre> Listado 3.10: XQuery: Ejemplo de consulta</p> <p>En el listado anterior,la cl\u00e1usula for permitir\u00e1 recorrer todos los elementos asignatura. Por su parte, let define una expresi\u00f3n en la que se asigna a nombre_asignatura el t\u00edtulo de la asignatura que se est\u00e1 recorriendo en cada momento. La cl\u00e1usula where impone el criterio de que el tipo de asignatura sea anual y, finalmente, result define como resultado un fragmento de c\u00f3digo XML donde el nombre de la asignatura anual (seg\u00fan la expresi\u00f3n indicada en let) aparecer\u00e1 como resultado encerrado entre dos etiquetas XML llamadas asignatura_anual. Tal y como se puede apreciar, XQuery tambi\u00e9n permite generar nuevos documetnos XML a partir de consultas.</p> <p>Al igual que SQL, XQuery dispone de multitud de funciones para la consulta de datos. As\u00ed, es posible utilizar la funci\u00f3n distinct() para eliminar duplicados en los resultados as\u00ed como funciones de agregaci\u00f3n como sum() o count() adem\u00e1s de poder especificar funciones definidas por el usuario. XQuery tambi\u00e9n permite utilizar \"joins\" incluyendo en la cl\u00e1usula where el operador de combinaci\u00f3n al igual que en SQL. Por otra parte, si bien es cierto que XQuery no posee el operador group by de SQL, este puede implementarse mediante la creaci\u00f3n de consultas anidadas, las cuales s\u00ed que son soportadas por el lenguaje.</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#33-bases-de-datos-documentales-mongodb","title":"3.3 Bases de datos documentales: MongoDB","text":"<p>Los sistemas de bases de datos documentales u orientados a documentos son sistemas de bases de datos NoSQL que almacenan datos, los cuales se estructuran en forma de documentos. En los \u00faltimos a\u00f1os, han proliferado una gran variedad de sistemas de este tipo como pueden ser Cassandra4, CouchDB5, Riak6 o MongoDB7, sobre el cual trata esta secci\u00f3n.</p> <p>El nombre MongoDB proviene de la palabra inglesa \"homongous\" que significa enorme. MongoDB es, por tanto, un sistema de base de datos NoSQL, open source, orientado a documentos y escrito en lenguaje C++. Este sistema de bases de datos est\u00e1 disponible no solo para m\u00faltiples plataformas y sistemas operativos (Windows, Linux, OS X) sino tambi\u00e9n como servicio empresarial en la nube y se puede integrar con otros servicios como, por ejemplo, Amazon Web Services (AWS).</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#331-mongodb-caracteristicas-y-aplicaciones","title":"3.3.1 MongoDB: Caracter\u00edsticas y aplicaciones","text":"<p>Tal y como se ha presentado anteriormente, MongoDB es un sistema de bases de datos documental u orientado a documentos. Esta orientaci\u00f3n hace que los datos se almacenen de forma estructurada en forma de documentos, los cuales se acoplan sin problema en los tipos de datos utilizados por los lenguajes de programaci\u00f3n. Adem\u00e1s, esta concepci\u00f3n hace que una base de datos MongoDB disponga de un esquema din\u00e1mico y f\u00e1cilmente modificable.</p> <p>En este apartado se pretende poner de relieve las principales caracter\u00edsticas de MongoDB que hacen que sea en la actualidad uno de los sistemas de bases de datos NoSQL m\u00e1s utilizados tanto en el \u00e1mbito acad\u00e9mico como en el profesional. </p> <ul> <li> <p>Alto rendimiento: Gracias a la definici\u00f3n de los documentos y la creaci\u00f3n de \u00edndices que hace que las lecturas y escrituras se realicen de forma m\u00e1s r\u00e1pida.</p> </li> <li> <p>Alta disponibilidad: MongoDB dispone de servidores replicados con restablecimiento autom\u00e1tico maestro.</p> </li> <li> <p>F\u00e1cil escalabilidad: Permitiendo distribuir colecciones de documentos entre diferentes m\u00e1quinas de forma muy sencilla.</p> </li> <li> <p>Indexaci\u00f3n: Similar al concepto de \u00edndice en una base de datos relacional, permite crear \u00edndices e \u00edndices secundarios para mejorar el rendimiento de las consultas.</p> </li> <li> <p>Consultas ad-hoc: Al igual que en una base de datos relacional, MongoDB da soporte a la b\u00fasqueda por campos, consultas de rangos, uso de expresiones regulares... A todo lo anterior, se a\u00f1ade la posibilidad de ejecutar y devolver una funci\u00f3n JavaScript definida por el programador.</p> </li> <li> <p>Replicaci\u00f3n: Siguiendo el modelo maestro-esclavo. El maestro puede ejecutar comandos de lectura y escritura mientras que el esclavo solo tiene acceso de lectura y la posibilidad de realizar copias de seguridad. En caso de que el maestro caiga, el esclavo puede elegir un nuevo maestro para mantener el servicio de replicaci\u00f3n.</p> </li> <li> <p>Balanceo de carga: MongoDB es capaz de ejecutarse en m\u00faltiples servidores, pudiendo balancear la carga y/o duplicar los datos para mantener el correcto funcionamiento del sistema aunque se produzca un fallo hardware.</p> </li> <li> <p>Almacenamiento de archivos: Todo lo anterior, facilita que este sistema de base de datos pueda ser usado con un sistema de archivos GridFS con balanceo de carga y replicaci\u00f3n.</p> </li> <li> <p>Agregaci\u00f3n: Proporciona operadores de agregaci\u00f3n y la posibilidad de utilizar funciones MapReduce para el procesamiento de datos por lotes.</p> </li> <li> <p>Ejecuci\u00f3n de Javscript del lado del servidor: Es posible realizar consultas utilizando JavaScript, de forma que \u00e9stas son enviadas directamente a la base de datos para ser ejecutadas.</p> </li> </ul> <p>Todas estas caracter\u00edsticas hacen que, en la actualidad, MongoDB sea uno de los principales sistemas de bases de datos NoSQL elegidos en multitud de aplicaciones como: almacenamiento y registro de eventos, comercio electr\u00f3nico, juegos, aplicaciones m\u00f3viles, almacenes de datos, gesti\u00f3n de estad\u00edsticas en tiempo real y cualquier aplicaci\u00f3n que requiera llevar a cabo anal\u00edticas sobre grandes vol\u00famenes de datos.</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#332-mongodb-conceptos-basicos-utilidades-y-herramientas","title":"3.3.2 MongoDB: Conceptos b\u00e1sicos, utilidades y herramientas","text":"<p>Getting Started MongoDB</p> <p>Aunque MongoDB es un sistema de bases de datos NoSQL con todo lo que ello implica, tambi\u00e9n ofrece toda la funcionalidad de la que disponen las bases de datos relacionales. Sin embargo, la estructura de una base de datos MongoDB difiere de la de una base de datos relacional.</p> <p>En un servidor MongoDB es posible crear tantas bases de datos como se desee. El concepto de base de datos es en este caso equivalente al de base de datos en los sistemas relacionales. Una vez creada, la base de datos estar\u00e1 compuesta por una o m\u00e1s colecciones. El t\u00e9rmino colecci\u00f3n en el \u00e1mbito NoSQL es el equivalente al concepto de tabla en los sistemas relacionales. Cada colecci\u00f3n, por tanto, estar\u00e1 formada por un conjunto de documentos (o incluso ninguno, en cuyo caso la colecci\u00f3n estar\u00eda vac\u00eda). El concepto de documento en NoSQL se corresponde con el concepto de registro en una base de datos relacional. Un documento estar\u00e1 compuesto por una serie de campos, al igual que lo est\u00e1n los registros de una base de datos relacional. En el \u00e1mbito NoSQL, y m\u00e1s concretamente en MongoDB, cada documento viene dado por un archivo JSON (http://www.json.org/) en el cual, siguiendo una estructura clave-valor se especifican las caracter\u00edsticas de cada documento. El listado 3.11 muestra un ejemplo de documento que define un barco.</p> <p><pre><code>{\nname: 'USS Enterprise-D',\noperator: 'Starfleet',\ntype: 'Explorer',\nclass: 'Galaxy',\ncrew: 750,\ncodes: [10,11,12]\n}\n</code></pre> Listado 3.11: Definici\u00f3n json de un documento barco</p> <p>Para la administraci\u00f3n del sistema de bases de datos, MongoDB pone a disposici\u00f3n de los usuarios las siguientes utilidades:</p> <ul> <li> <p>mongo: Se trata del shell interactivo de MongoDB que permite insertar, eliminar, actualizar datos y realizar consultas, adem\u00e1s de replicar la informaci\u00f3n, apagar servidores y ejecutar c\u00f3digo JavaScript.</p> </li> <li> <p>mongostat: Herramienta de l\u00ednea de comandos que muestra las estad\u00edsticas de una instancia en ejecuci\u00f3n de MongoDB</p> </li> <li> <p>mongotop: Herramienta de l\u00ednea de comandos que muestra la cantidad de tiempo empleado en la lectura y escritura de datos por parte de la instancia en ejecuci\u00f3n</p> </li> <li> <p>mongosniff: Herramienta de l\u00ednea de comandos que permite hacer un rastreo del tr\u00e1fico de la red que va desde y hacia MongoDB.</p> </li> <li> <p>mongoimport/mongoexport: Herramienta de l\u00ednea de comandos para importar y exportar contenido en o desde .json, .csv o .tsv entre otros.</p> </li> <li> <p>mongodump/mongorestore: Herramienta de l\u00ednea de comandos para la creaci\u00f3n de una exportaci\u00f3n binaria del contenido de la base de datos.</p> </li> </ul> <p>Aunque MongoDB tiene una interfaz administrativa accesible desde http://localhost:28017/ (siempre que se haya iniciado con la opci\u00f3n mongod \u2212 \u2212rest, existen algunas herramientas gr\u00e1ficas para la administraci\u00f3n y uso de este sistema de base de datos como UMongo (https://github.com/agirbal/umongo) , el cual es una aplicaci\u00f3n open source de sobremesa para navegar y administrar un cl\u00faster MongoDB o Robomongo (https://robomongo.org), que tambi\u00e9n es una herramienta gr\u00e1fica open source que incluye una terminal de comandos completamente compatible con el shell de mongo.</p> <p>ver presentaci\u00f3n BDA3.5</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#333-mongodb-operaciones-crud","title":"3.3.3 MongoDB: Operaciones CRUD","text":"<p>MongoDB CRUD Operations</p> <p>Cualquier sistema de datos permite definir operaciones b\u00e1sicas como son la creaci\u00f3n de tablas e inserci\u00f3n de registros, lectura de datos, actualizaci\u00f3n y eliminaci\u00f3n de registros. Estas operaciones b\u00e1sicas se conocen con el nombre de CRUD (Create, Read, Update, Delete). A continuaci\u00f3n, se muestran distintos ejemplos para ilustrar la implementaci\u00f3n de estas operaciones en MongoDB.</p> <p>Una vez iniciada una sesi\u00f3n del Shell de Mongo, es posible crear una base de datos utilizando el siguiente comando.</p> <pre><code>test&gt;use DB_Ejemplo\nswitched to db DB_Ejemplo\n</code></pre> <p>Para crear una colecci\u00f3n de documentos \"Barco\" dentro de la base de datos DB_Ejemplo se puede escribir. <pre><code>DB_Ejemplo&gt;db.createCollection(\"ships\")\n{ ok: 1 }\n</code></pre></p> <p>Para visualizar las bases de datos almacenadas en el servidor se puede utilizar el comando show dbs mientras que para obtener un listado de las colecciones de la base de datos sobre la que se est\u00e1 trabajando, es posible utilizar el comando show collections. Por otra parte, para insertar un documento dentro de la colecci\u00f3n, como el mostrado en el listado 3.11, se ejecuta el siguiente comando.</p> <p><pre><code>db.ships.insertOne({name:'USS-Enterprise-D',operator:'Starfleet',type:'Explorer',class:'Galaxy',crew:750,codes:[10,11,12]})\n</code></pre> ver presentaci\u00f3n BDA3.6</p> <p>Para actualizar el nombre del barco \"USS-Enterprise-D\" por \"USS Something\" es posible escribir el comando.</p> <p>MongoDB Update Operations</p> <pre><code>db.ships.updateOne({name : {$eq: 'USS-Enterprise-D'}}, {$set : {name: 'USS Something'}})\n</code></pre> <p>Mientras que si se pretenden establecer o cambiar varios atributos de un mismo documento, como por ejemplo el operador y la clase, se puede utilizar el comando update de la siguiente forma.</p> <pre><code>db.ships.updateOne({name : {$eq: 'USS Something'}}, {$set : {name: 'USS Prometheus', class: 'Prometheus'}})\n</code></pre> <p>Finalmente, la eliminaci\u00f3n de alg\u00fan atributo de un documento tambi\u00e9n es una operaci\u00f3n de actualizaci\u00f3n que puede realizarse de la siguiente forma.</p> <pre><code>db.ships.updateOne({name : {$eq: 'USS Prometheus'}}, {$unset : {operator: 1}})\n</code></pre> <p>MongoDB Delete Operations</p> <p>La eliminaci\u00f3n de documentos de una colecci\u00f3n puede realizarse de forma directa o bien utilizando expresiones regulares. A continuaci\u00f3n se muestran dos comandos para ilustrar ambas formas de eliminaci\u00f3n. El segundo de ellos elimina aquellos documentos que comienzan por \"a\".</p> <pre><code>db.ships.deleteOne({name : 'USS Prometheus'})\ndb.ships.deleteOne({name:{$regex:'^A*'}})\n</code></pre> <p>Para leer y mostrar documentos, se utiliza el comando find(). A continuaci\u00f3n se muestra un ejemplo en el que, el primer comando muestra un documento al azar de los existentes, el segundo muestra todos los documentos y lo hace de forma indexada en lugar de como texto seguido, el tercero muestra solo los nombres de los barcos y el \u00faltimo, encuentra un documento cuyo nombre sea \"USS Defiant\".</p> <p>MongoDB Read Operations</p> <p><pre><code>db.ships.findOne()\ndb.ships.find().pretty()\ndb.ships.find({}, {name:true})\ndb.ships.findOne({'name':'USS Defiant'})\n</code></pre> ver presentaci\u00f3n BDA3.7</p>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/index.html#334-mongodb-consultas-y-agregacion","title":"3.3.4 MongoDB Consultas y Agregaci\u00f3n","text":"<p>Al igual que cualquier lenguaje de manipulaci\u00f3n de datos, MongoDB dispone de las herramientas y operadores necesarios para realizar consultas sobre los documentos almacenados en las colecciones. A continuaci\u00f3n, se muestran ejemplos de uso de los principales operadores.</p> <p>Los operadores relaciones mayor que, menor que, mayor o igual que y menor o igual que se corresponden, respectivamente, con los operadores $gt, $lt, $gte, $lte. Adem\u00e1s, tambi\u00e9n es posible utilizar el operador $regex para recuperar elementos que cumplan una expresi\u00f3n regular. A continuaci\u00f3n, se muestran dos consultas que recuperan aquellos barcos que permitan subir a bordo a m\u00e1s de cien pasajeros y otra consulta para recuperar aquellos que dejen subir tan solo a 1\" pasajeros o menos.</p> <pre><code>db.ships.find({crew:{$gt:100}})\ndb.ships.find({crew:{$lte:100}})\n</code></pre> <p>El operador \\$exists permite devolver aquellos documentos para los que existe o no un determinado atributo. El siguiente comando permite encontrar aquellos barcos para los cuales existe el campo \"class\".</p> <pre><code>db.ships.find({class:{$exists:true}})\n</code></pre> <p>MongoDB Aggregation Operations</p> <p>Las funciones de agregaci\u00f3n son especialmente \u00fatiles en los lenguajes de consulta y manipulaci\u00f3n de datos. De esta forma, $sum permite agregar mediante la operaci\u00f3n suma una serie de valores, $avg permite obtener la media, $mnin y $max encontrar el valor m\u00e1ximo y m\u00ednimo de un atributo para el conjunto de elementos, $push introduce en un array los resultados de la consulta que se ha realizado, $addToSet es similar al anterior solo que sin incluir valores duplicados y, por \u00faltimo, $first y $last permiten obtener el primer y \u00faltimo documento en una consulta. A continuaci\u00f3n, se muestra un ejemplo del uso de cada uno de estos operadores de agregaci\u00f3n.</p> <pre><code>db.ships.aggregate([{$group:{_id:\"$operator\",num_ships:{$sum:\"$crew\"}}}])\ndb.ships.aggregate([{$group : {_id : \"$operator\", num_ships : {$avg : \"$crew\"}}}])\ndb.ships.aggregate([{$group : {_id : \"$operator\", num_ships : {$min : \"$crew\"}}}])\ndb.ships.aggregate([{$group : {_id : \"$operator\", classes : {$push: \"$class\"}}}])\ndb.ships.aggregate([{$group : {_id : \"$operator\", classes : {$addToSet : \"$class\"}}}])\ndb.ships.aggregate([{$group:{_id:\"$operator\",last_class:{$last: \"$class\"}}}])\n</code></pre> <p>Finalmente, MongoDB pone a disposici\u00f3n de los usuarios multitud de funciones \u00fatiles en la realizaci\u00f3n de consultas. La tabla 3.4 muestra algunas de las m\u00e1s utilizadas.</p> Funci\u00f3n Significado $project Cambia el conjunto de documentos modificando sus claves y valores $match Operaci\u00f3n de filtrado para reducir el n\u00famero de elementos recuperados $group Operador de agregaci\u00f3n para agrupar resultados $sort Ordena los documentos $skip Recupera los documentos a partir de un numero especificado por el usuario $limit Limita los resultados de la consulta al valor pasado como par\u00e1metro a la funci\u00f3n $unwind Utilizado como equivalente al join de SQL <p>Tabla 3.4: Funciones \u00fatiles en MongoDB</p> <p>ver presentaci\u00f3n BDA3.8</p> <ol> <li> <p>https://www.liquid-technologies.com/online-xsd-validator y http://xmlvalidator.new-studio.org \u21a9</p> </li> <li> <p>https://www.freeformatter.com, https://www.liquid-technologies.com/online-xsd-validator y https://extendsclass.com/xml-schema-validator.html \u21a9</p> </li> <li> <p>https://download.cnet.com/XPath-Visualizer/3000-7241_4-75804649.html, https://www.freeformatter.com/xpath-tester.html \u21a9</p> </li> <li> <p>https://cassandra.apache.org/ \u21a9</p> </li> <li> <p>http://couchdb.apache.org \u21a9</p> </li> <li> <p>https://riak.com \u21a9</p> </li> <li> <p>https://www.mongodb.com/es \u21a9</p> </li> </ol>"},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/Ejercicios_clase/Ejemplo1.html","title":"Big Data APlicado","text":""},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/Ejercicios_clase/Ejemplo1.html#ud-3-gestion-de-soluciones-iii","title":"UD 3 - Gesti\u00f3n de Soluciones III","text":""},{"location":"UD3%20-%20Gesti%C3%B3n%20de%20soluciones%20III/Ejercicios_clase/Ejemplo1.html#ejemplo-1-mongodb","title":"Ejemplo 1 MongoDB","text":"<ol> <li> <p>Instalar MongoDB. Para ello puedes optar por diferentes opciones</p> <ul> <li>Instalarla en tu m\u00e1quina local</li> <li>Usar Atlas MongoDB: https://www.mongodb.com/docs/atlas/getting-started/</li> <li>Crear un contendor docker. Yo voy a usar este caso</li> </ul> </li> <li> <p>Instalar MondoDB en Docker</p> </li> <li>Vamos a la imagen oficial de mondodb en docker</li> <li>Seguimos los pasos, que tienen este formato</li> </ol> <p><code>docker run -d --network some-network --name some-mongo \\     -e MONGO_INITDB_ROOT_USERNAME=mongoadmin \\     -e MONGO_INITDB_ROOT_PASSWORD=secret \\     mongo</code></p> <pre><code>En mi caso voy a a\u00f1adr un bind mount para poder luego importar un fichero para el ejemplo. Por tanto ser\u00eda:\n\n```\ndocker run -d --name mongo-bda1 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=admin --mount type=bind,src=/home/jaime/BigData/BDA/,dst=/home/bda mongo\n```\n</code></pre> <ol> <li> <p>Entrar a la terminal y autorizarnos</p> <pre><code>mongosh\nuse admin\ndb.auth(\"admin\",\"admin\")\n</code></pre> </li> <li> <p>Ver las bases de datos que hay</p> <pre><code>show collections\n</code></pre> </li> <li> <p>Ver las colecciones de una base de datos</p> <pre><code>show dbs\n</code></pre> </li> <li> <p>Crear y/o usar una Base de datos (use nombre_bd)</p> <p><pre><code>use db_ejemplo1\n</code></pre> 7. Probamos los ejemplos que aparecen en el temario</p> </li> <li> <p>Probamos otro ejemplo. Vamos a importar un archivo fuente. Para ello nos abrimos otra terminal en la m\u00e1quina y ejecutamos el siguiente comando</p> <pre><code>mongoimport --db=db_ej1_restaurantes --collection=restaurantes --file=home/bda/UD3/Ejemplo1/restaurantes1.csv --authenticationDatabase=admin --username=admin --password=admin\n</code></pre> </li> <li> <p>Comprobamos la carga</p> <pre><code>show dbs\ndb.restaurantes.find()\n</code></pre> </li> <li> <p>Crear una consultar para encontrar qu\u00e9 restaurantes no tienen direcci\u00f3n (Todas tienen direcci\u00f3n)</p> <pre><code>db.restaurantes.find({address:{$exists:false}})\n</code></pre> </li> <li> <p>Crear una consulta para encontrar aquellos restaurantes de cocina italiana que se encuentren en la zona geogr\u00e1fica con c\u00f3digo postal 10075</p> <p><pre><code>db.restaurantes.find({$and:[{\"cuisine\":\"Italian\"},{\"address.zipcode\":\"10075\"}]})\n</code></pre> 12. Encontrar aquellos restaurantes que tengan grado A, puntuaci\u00f3n 11 y fecha 2014-10-01T00:00:00Z</p> <pre><code>db.restaurantes.find({grades:{\"date\":ISODate(\"2014-10-01T00:00:00Z\"),\"grade\":\"A\",\"score\":11}})\n</code></pre> </li> <li> <p>Contabiliza cu\u00e1ntos restaurantes tienen una puntuaci\u00f3n menor o igual a 5</p> <pre><code>db.restaurantes.find({\"grades.score\":{$lt:5}}).count()\n</code></pre> </li> <li> <p>Obtener los nombres del segundo y el tercer restaurante de cocina italiana ordenados por nombre</p> <pre><code>db.restaurantes.find({\"cuisine\":\"Italian\"}).sort({name:1}).limit(2).skip(1)\n</code></pre> </li> <li> <p>A\u00f1adir una valoraci\u00f3n al restaurante 41156888</p> <pre><code>db.restaurantes.updateOne({\"restaurant_id\":\"41156888\"},{$push:{grades: {\"date\":ISODate(\"2016-01-02T00:00:00.000Z\"),\"grade\":\"A\",\"score\":14}}})\n</code></pre> </li> </ol>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html","title":"UD 4 - Gesti\u00f3n de Soluciones IV","text":"<p>En este cap\u00edtulo se profundizar\u00e1 en las bases de datos orientadas a grafos como soluci\u00f3n a problemas de Big Data. En la actualidad, cualquier problema puede ser modelizado por medio de grafos. Esta estructura matem\u00e1tica vers\u00e1til y escalable proporciona un mayor rendimiento a los sistemas que utilizan este tipo de bases de datos. Para ilustrar todos los conceptos de este cap\u00edtulo, se utilizar\u00e1 Neo4j como plataforma de bases de datos orientada a grafos.</p>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#41-bases-de-datos-orientadas-a-grafos","title":"4.1 Bases de datos orientadas a grafos","text":"<p>Una base de datos orientada a grafos es un tipo de base de datos NoSQL como lo son tambi\u00e9n, por ejemplo, las bases de datos documentales. Un sistema de gesti\u00f3n de bases de datos orientadas a grafos es aquel define m\u00e9todos y operaciones CRUD sobre un modelo de datos representado mediante un grafo. Por norma general, este tipo de bases de datos se implementan para su uso junto con sistemas OLTP, por lo que est\u00e1n dise\u00f1adas en t\u00e9rminos de relaciones de integridad y optimizadas para ofrecer un rendimiento operacional.</p> <p>Aunque existen diferentes tipos de modelos de datos basados en grafos, cada sistema de base de datos elegir\u00e1 uno de ellos para representar el contenido de la base de datos. Cualquiera de estos modelos est\u00e1 inspirado en la estructura matem\u00e1tica de un grafo para modelizar un conjunto de datos. Matem\u00e1ticamente, un grafo es un conjunto de nodos o v\u00e9rtices, que representan entidades de un dominio, y un conjunto de aristas o enlaces, que representan las relaciones o interacciones que se producen entre las entidades. La figura4.1 muestra un ejemplo de grafo que representa las relaciones de seguimiento entre distintos canales en YouTube.</p> <p></p> <p>Figura 4.1: Ejemplo de grafo: relaciones de seguimiento entre canales YouTube</p> <p>Uno de los modelos de datos basado en grafos m\u00e1s popular es el modelo de grafo de propiedades etiquetadas. Un grafo representado mediante este modelo, debe cumplir las siguientes caracter\u00edsticas principales:</p> <p>1) Debe contener un conjunto de nodos y aristas o relaciones entre dichos nodos, 2) Los nodos contienen propiedades, definidas a trav\u00e9s de pares \u201cclave-valor\u201d, 3) Los nodos pueden estar etiquetados con una o m\u00e1s etiquetas, 4) Las relaciones entre los nodos est\u00e1n nombradas y son dirigidas, teniendo siempre un nodo de inicio y otro nodo de fin, 5) Las relaciones del grafo tambi\u00e9n pueden contener propiedades. Este modelo, a pesar de su simplicidad, es sencillo de entender y permite modelar cualquier problema y/o conjunto de datos.</p>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#411-caracteristicas-principales","title":"4.1.1 Caracter\u00edsticas principales","text":"<p>El aumento del uso de los sistemas de bases de datos orientados a grafos y el auge que est\u00e1n alcanzando en los \u00faltimos a\u00f1os se debe, principalmente, a una serie de caracter\u00edstica que confieren a estos sistemas una serie de ventajas importantes sobre los sistemas de bases de datos tradicionales y otros sistemas NoSQL.</p> <ul> <li>Almacenamiento: Las bases de datos orientadas a grafo pueden ser de almacenamiento nativo, en cuyo caso los datos se almacenan internamente con estructura de grafo. No obstante, existen bases de datos de grafos que mapean los datos para adptarlos a una estructura relacional.</li> <li>Procesamiento: El almacenamiento nativo permite obtener una estructura de datos que no requiere \u00edndices de adyacencia para referenciar los nodos del grafo, mejorando el rendimiento en la ejecuci\u00f3n de consultas.</li> <li>Rendimiento: La ejecuci\u00f3n de consultas sobre datos estructurados en forma de grafo proporciona escalabilidad al sistema de bases de datos, ya que aunque el n\u00famero de datos aumente, las consultas se ejecutan sobre la porci\u00f3n del grafo de inter\u00e9s.</li> <li>Flexibilidad: Los modelos de datos basados en grafos permiten r\u00e1pidamente a\u00f1adir nuevos nodos, enlaces y subgrafos a un modelo existente con facilidad y sin p\u00e9rdida de funcionalidad ni rendimiento.</li> <li>Rapidez: La naturaleza libre de esquema de las bases de datos orientadas a grafos, juntos con las APIs desarrolladas para su manipulaci\u00f3n y los lenguajes de consulta, hacen que estos sistemas sean \u00e1giles y r\u00e1pidos, integr\u00e1ndolos con las metodolog\u00edas de desarrollo de software iterativas e incrementales.</li> </ul>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#412-areas-de-aplicacion","title":"4.1.2 \u00c1reas de Aplicaci\u00f3n","text":"<p>Las caracter\u00edsticas de las bases de datos orientadas a grafos desarrolladas en el apartado anterior confieren a esta tecnolog\u00eda un sinf\u00edn de aplicaciones reales en las que su uso mejora significativamente a las tecnolog\u00edas tradicionales. A continuaci\u00f3n, se muestran algunos ejemplos de aplicaci\u00f3n donde, actualmente, este tipo de bases de datos es el m\u00e1s ampliamente utilizado.</p> <ul> <li> <p>Redes sociales: El an\u00e1lisis de redes sociales es, a d\u00eda de hoy, una de las principales fuentes de informaci\u00f3n de cualquier dominio. El an\u00e1lisis de redes sociales mediante bases de datos orientadas a grafos permite identificar relaciones expl\u00edcitas e impl\u00edcitas entre usuarios y grupos de usuarios, as\u00ed como identificar la forma en la que ellos interact\u00faan pudiendo inferir el comportamiento de un usuario en base a sus conexiones. En una red social, dos usuarios presentan una conexi\u00f3n expl\u00edcita si est\u00e1n directamente conectados. Esto ocurre, por ejemplo, entre dos usuarios de Facebook que son amigos o dos compa\u00f1eros de trabajo de la misma empresa en LinkedIn. Por otra parte, una relaci\u00f3n impl\u00edcita es aquella que se produce entre dos usuarios a trav\u00e9s de un intermediario, como puede ser otro usuario, un post donde han hecho un comentario, un \u201clike\u201d o un art\u00edculo que ambos han comprado.</p> </li> <li> <p>Sistemas de recomendaci\u00f3n: Estos sistemas permiten modelar en forma de grafo las relaciones que se establecen entre personas o usuarios y cosas, como pueden ser productos, servicios, contenido multimedia o cualquier otro concepto relevante en funci\u00f3n del dominio de aplicaci\u00f3n. Las relaciones se establecen en funci\u00f3n del comportamiento de los usuarios al comprar, consumir contenido, puntuarlo o evaluarlo etc. De esta forma, los sistemas de recomendaci\u00f3n identifican recursos de inter\u00e9s para un usuario espec\u00edfico y pueden predecir su comportamiento al comprar un producto o contratar un servicio.</p> </li> <li> <p>Informaci\u00f3n geogr\u00e1fica: Se trata del caso de aplicaci\u00f3n m\u00e1s popular de la teor\u00eda de grafos. Las aplicaciones de las bases de datos orientadas a grafos en informaci\u00f3n geogr\u00e1fica van desde calcular rutas \u00f3ptimas entre dos puntos en cualquier tipo de red (red de carreteras, de ferrocarril, a\u00e9rea, log\u00edstica...) hasta encontrar todos los puntos de inter\u00e9s en un \u00e1rea concreta, encontrar el centro de una regi\u00f3n u obtener la intersecci\u00f3n entre dos o m\u00e1s regiones, entre otras muchas. As\u00ed, las bases de datos orientadas a grafos permiten modelar estos casos de aplicaci\u00f3n como grafos dirigidos sobre los cuales operar para obtener los resultados deseados.</p> </li> </ul> <p>Aunque en este apartado se hayan analizado las principales \u00e1reas de aplicaci\u00f3n, son muchos los dominios de aplicaci\u00f3n donde las bases de datos orientadas a grafos se est\u00e1n convirtiendo en una soluci\u00f3n predominante, como lo son la gesti\u00f3n de datos maestros, gesti\u00f3n de redes y centros de datos o control de acceso entre otros muchos.</p> <p>ver presentaci\u00f3n BDA4.1</p>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#42-neo4j","title":"4.2 Neo4j","text":"<p>En esta secci\u00f3n se profundiza en una tecnolog\u00eda concreta de bases de datos orientadas a grafos. Aunque existe un amplio abanico de tecnolog\u00edas que dan soporte a este tipo de bases de datos, como lo son OrientDB11 , FlockDB2 o InfiniteGraph3 , en esta secci\u00f3n se profundiza en el trabajo con bases de datos orientadas a grafos utilizando Neo4j4 .</p>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#421-introduccion","title":"4.2.1 Introducci\u00f3n","text":"<p>Neo4j es una plataforma de bases de datos orientada a grafos lanzada en 2007 e implementada en Java. Entre sus caracter\u00edsticas principales destacan el almacenamiento y procesamiento de grafos nativo, lo que la convierte en una alternativa potente para el trabajo en entornos reales, donde se requiere de estas caracter\u00edsticas para mejorar el rendimiento y la escalabilidad de los sistemas. Adem\u00e1s, a pesar de lo anterior, Neo4j tambi\u00e9n permite trabajar con modelos de datos en forma de grafo de manera transaccional, por lo que tambi\u00e9n es \u00fatil en sistemas transaccionales.</p> <p>Junto a todo lo anterior, Neo4j dispone de amplias librer\u00edas que permiten crear tambi\u00e9n modelos de aprendizaje autom\u00e1tico sobre grafos. Esta plataforma puede integrarse con m\u00faltiples lenguajes de programaci\u00f3n como Python o C++, entre otros, y ofrece una serie de herramientas muy \u00fatiles para usuarios y desarrolladores. A continuaci\u00f3n, se enumeran y detallan algunas de ellas:</p> <ul> <li> <p>Graph Data Science Library: Se trata de la librer\u00eda principal que contiene una gran cantidad de algoritmos de b\u00fasqueda sobre grafos adem\u00e1s de modelos de aprendizaje autom\u00e1tico. Se trata de algoritmos y modelos eficientemente programados y escalables para el trabajo con grafos.</p> </li> <li> <p>Neo4j Bloom: Es una aplicaci\u00f3n de visualizaci\u00f3n y exploraci\u00f3n de grafos que permite la visualizaci\u00f3n de estos desde distintas perspectivas y puntos de vista, ofreciendo as\u00ed una herramienta visual muy \u00fatil para visualizar los resultados de los algoritmos as\u00ed como mostrar resultados a clientes.</p> </li> <li> <p>Cypher: Es el lenguaje de creaci\u00f3n de consultas utilizado en Neo4j. Se trata de un lenguaje inspirado en SQL, solo que m\u00e1s sencillo y optimizado para el trabajo con grafos.</p> </li> <li> <p>Integradores: Con el objetivo de integrar y conectar Neo4j con otras plataformas y lenguajes, se han desarrollado distintos conectores para integrar esta plataforma con tecnolog\u00edas como Apache Spark, Apache Kafka, BI... </p> </li> <li> <p>Herramientas para desarrolladores: Neo4j dispone de versiones para escritorio y web as\u00ed como un sandbox en el que se proporciona un entorno controlado e integrado de los servicios Neo4j para desarrolladores.</p> </li> <li> <p>Neo4j Aura: Neo4j ofrece un servicio de computaci\u00f3n en la nube para la creaci\u00f3n de grafos y ejecuci\u00f3n de algoritmos y modelos sobre ellos. Se trata de Neo4j Aura, el cual se encuentra disponible de forma gratuita y est\u00e1 integrado con Google Cloud Platform.</p> </li> </ul> <p>Las caracter\u00edsticas anteriormente mencionadas y la cantidad de herramientas disponibles que Neo4j pone a disposici\u00f3n de los desarrolladores han sido los motivos principales para optar por esta plataforma para ilustrar el trabajo con bases de datos orientadas a grafos.</p> <p>ver presentaci\u00f3n BDA4.2</p>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#422-importacion-de-datos-creacion-de-grafos","title":"4.2.2 Importaci\u00f3n de datos. Creaci\u00f3n de grafos","text":"<p>ver presentaci\u00f3n BDA4.3</p> <p>Para comenzar a trabajar con Neo4j, es necesario crear un grafo o bien partir de un grafo ya existente que se importa dentro del \u00e1rea de trabajo de Neo4j. A continuaci\u00f3n, se muestra c\u00f3mo implementar ambos procedimientos.</p> <p>Importaci\u00f3n de un grafo</p> <p>Sea el caso en el que se pretende importar un grafo a partir de un fichero .csv. En el ejemplo que se muestra a continuaci\u00f3n, se dispone de dos ficheros .csv que contienen, respectivamente, la definici\u00f3n de los nodos y de las aristas del grafo que se pretende importar. As\u00ed pues, el listado 4.1 muestra los nodos del grafo. Para cada nodo se almacena su identificador, latitud, longitud y poblaci\u00f3n</p> <p><pre><code>id,latitude,longitude,population\n\"Amsterdam\",52.379189,4.899431,821752\n\"Utrecht\",52.092876,5.104480,334176\n\"Den Haag\",52.078663,4.288788,514861\n\"Immingham\",53.61239,-0.22219,9642\n\"Doncaster\",53.52285,-1.13116,302400\n\"Hoek van Holland\",51.9775,4.13333,9382\n\"Felixstowe\",51.96375,1.3511,23689\n\"Ipswich\",52.05917,1.15545,133384\n\"Colchester\",51.88921,0.90421,104390\n\"London\",51.509865,-0.118092,8787892\n\"Rotterdam\",51.9225,4.47917,623652\n\"Gouda\",52.01667,4.70833,70939\n</code></pre> Listado 4.1: Definici\u00f3n de nodos de un grafo</p> <p>Por su parte, el listado 4.2 especifica cada uno de los enlaces entre los nodos del grafo. Para cada uno de los enlaces se identifica el origen y destino del enlace, el tipo de relaci\u00f3n que se define entre ellos y el coste. </p> <p><pre><code>src,dst,relationship,cost\n\"Amsterdam\",\"Utrecht\",\"EROAD\",46\n\"Amsterdam\",\"Den Haag\",\"EROAD\",59\n\"Den Haag\",\"Rotterdam\",\"EROAD\",26\n\"Amsterdam\",\"Immingham\",\"EROAD\",369\n\"Immingham\",\"Doncaster\",\"EROAD\",74\n\"Doncaster\",\"London\",\"EROAD\",277\n\"Hoek van Holland\",\"Den Haag\",\"EROAD\",27\n\"Felixstowe\",\"Hoek van Holland\",\"EROAD\",207\n\"Ipswich\",\"Felixstowe\",\"EROAD\",22\n\"Colchester\",\"Ipswich\",\"EROAD\",32\n\"London\",\"Colchester\",\"EROAD\",106\n\"Gouda\",\"Rotterdam\",\"EROAD\",25\n\"Gouda\",\"Utrecht\",\"EROAD\",35\n\"Den Haag\",\"Gouda\",\"EROAD\",32\n\"Hoek van Holland\",\"Rotterdam\",\"EROAD\",33\n</code></pre> Listado 4.2: Definici\u00f3n de aristas de un grafo</p> <p>Dados estos dos ficheros, es posible importar los nodos del grafo seg\u00fan se indica en el listado 4.3.</p> <p><pre><code> // Importamos los nodos\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/transport-nodes.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMERGE (place:Place {id:row.id})\nSET place.latitude = toFloat(row.latitude),\nplace.longitude = toFloat(row.latitude),\nplace.population = toInteger(row.population)\n</code></pre> Listado 4.3: Importaci\u00f3n de nodos</p> <p>As\u00ed, en el listado anterior se define como identificador de recurso la direcci\u00f3n donde se encuentra el archivo que se pretende cargar, el cual es cargado a trav\u00e9s de la instrucci\u00f3n LOAD CSV. La instrucci\u00f3n MERGE permite definir cada fila como nodo de un grafo de tipo lugar (place) y a\u00f1adir a cada nodo la propiedad id cuyo valor ser\u00e1 el identificador de la fila en la que se encuentra cada nodo dentro del fichero csv. Finalmente, los valores de latitud y longitud se convierten en valores de tipo real y la poblaci\u00f3n en un valor de tipo entero.</p> <p>An\u00e1logamente a la importaci\u00f3n de nodos, el listado 4.4 muestra el c\u00f3digo necesario para la importaci\u00f3n del fichero que contiene los enlaces entre los nodos del grafo.</p> <p><pre><code>//Importamos las relaciones\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/transport-relationships.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMATCH (origin:Place {id: row.src})\nMATCH (destination:Place {id: row.dst})\nMERGE (origin)-[:EROAD {distance: toInteger(row.cost)}]-&gt;(destination)\n</code></pre> Listado 4.4: Importaci\u00f3n de aristas</p> <p>La diferencia principal de este listado con respecto al anterior, es que en este se necesita identificar mediante la instrucci\u00f3n MATCH los nodos de origen y destino de cada uno de los enlaces, lo cual es posible mediante los atributos \u201csrc\u201d y \u201cdst\u201d incluidos en el archivo .csv. Finalmente, se utiliza la sentencia MERGE para crear una relaci\u00f3n entre \u201corigin\u201d y \u201cdestination\u201d de tipo EROAD que contiene la propiedad \u201cdistance\u201d cuyo valor es el coste que aparec\u00eda en el fichero .csv.</p> <p>Creaci\u00f3n de un grafo</p> <p>Otra alternativa es la creaci\u00f3n de un grafo desde cero, especificando para ello sus nodos y enlaces. La figura4.2 muestra un ejemplo de un grafo de coocurrencia de hashtags en el que cada uno de los nodos es un hashtag y los enlaces entre ellos se producen cuando dos hashtags aparecieron en un mismo tweet. Sobre cada enlace, se puede apreciar un valor que indica en cu\u00e1ntos tweets concurrieron los hashtags conectados por medio de dicha arista.</p> <p></p> <p>Figura 4.2: Grafo de co-ocurrencia de hashtags</p> <p>Para representar este grafo en Neo4j, se puede escribir el siguiente fragmento de c\u00f3digo que muestra el listado 4.5.</p> <p><pre><code>CREATE\n(JS:Hashtag {name: 'JoaquinSabina'}),\n(RS:Hashtag {name: 'Rusia2018'}),\n(AG:Hashtag {name: 'Argentina'}),\n(FD:Hashtag {name: 'Feliz Domingo'}),\n(MS:Hashtag {name: 'Messi'}),\n\n(JS)-[:COOC {ntweet: 52}]-&gt;(FD),\n(FD)-[:COOC {ntweet: 52}]-&gt;(JS),\n(RS)-[:COOC {ntweet: 183}]-&gt;(FD),\n(FD)-[:COOC {ntweet: 183}]-&gt;(RS),\n(RS)-[:COOC {ntweet: 73}]-&gt;(AG),\n(AG)-[:COOC {ntweet: 73}]-&gt;(RS),\n(AG)-[:COOC {ntweet: 112}]-&gt;(MS),\n(MS)-[:COOC {ntweet: 112}]-&gt;(AG),\n(FD)-[:COOC {ntweet: 81}]-&gt;(MS),\n(MS)-[:COOC {ntweet: 81}]-&gt;(FD)\n</code></pre> Listado 4.5: Creaci\u00f3n grafo de co-ocurrencia de hashtags</p> <p>Tal y como se puede ver, en primer lugar se definen los nodos y sus propiedades y a continuaci\u00f3n los enlaces y sus propiedades. Todo grafo en Neo4j tiene aristas dirigidas, si bien es cierto que cuando se ejecutan las consultas puede no tenerse en cuenta la direcci\u00f3n de las aristas. Por este motivo, se han especificado las aristas en ambas direcciones. En caso de querer crear una arista que sea interpretada como no dirigida, se debe crear sin el operador flecha que determina el sentido de la arista. Esto es posible hacerlo solo fuera de la instrucci\u00f3n CREATE. El listado 4.6 muestra c\u00f3mo crear la primera relaci\u00f3n del grafo solo que de forma no dirigida.</p> <p><pre><code>MATCH(JS:Hashtag{name:'JoaquinSabina'})\nMATCH(FD:Hashtag{name:'Feliz Domingo'})\nMERGE (JS)-[:COOC{ntweet:52}]-(FD)\n</code></pre> Listado 4.6: Creaci\u00f3n de una arista no dirigida</p> <p>Para mostrar el grafo, es posible utilizar el siguiente comando. La figura 4.3 muestra c\u00f3mo quedar\u00eda el grafo creado en el listado 4.5.</p> <pre><code>MATCH(n) return (n)\n</code></pre> <p>Para eliminar un nodo creado, es posible utilizar el comando:</p> <pre><code>MATCH(JS:Hashtag{name:'JoaquinSabina'}) delete (JS)\n</code></pre> <p>Sin embargo, un nodo solo puede ser eliminado si se eliminan las relaciones que contiene. Para ello, es posible utilizar el comando:</p> <p></p> <p>Figura 4.3: Visualizaci\u00f3n del grafo de co-ocurrencia de hashtags</p> <pre><code>MATCH(JS:Hashtag{name:'JoaquinSabina'}) detach delete (JS)\n</code></pre> <p>Mientras que, para eliminar todos los nodos y aristas, es posible escribir:</p> <pre><code>MATCH(n) detach delete(n)\n</code></pre>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#423-recorridos-sobre-grafos","title":"4.2.3 Recorridos sobre grafos","text":"<p>Los recorridos sobre grafos son una utilidad imprescindible en el trabajo con esta estructura de datos. Del mismo modo, los recorridos tambi\u00e9n son una herramienta fundamental cuando se trabaja con bases de datos orientadas a grafos.</p> <p>Un recorrido sobre grafos es un procedimiento sistem\u00e1tico que permite explorar un grafo examinando todos sus v\u00e9rtices y aristas, comenzando desde un v\u00e9rtice inicial. A la hora de recorrer un grafo, existen dos tipos de recorridos: el recorrido en anchura (denotado BFS, por sus siglas en ingl\u00e9s) y el recorrido en profundidad (denotado DFS, tambi\u00e9n por sus siglas en ingl\u00e9s).</p> <p>ver presentaci\u00f3n BDA4.4</p> <p>Recorrido en anchura (BFS)</p> <p>El recorrido en anchura, tambi\u00e9n conocido por sus siglas en ingl\u00e9s Breadth First Search (BFS) es un procedimiento que permite recorrer un grafo, explorando todos sus nodos y aristas.</p> <p>El recorrido en anchura parte de un nodo inicial. A continuaci\u00f3n, el m\u00e9todo comienza a explorar todos los nodos hijo del nodo inicial seleccionado que se encuentran a un salto. Una vez visitados, comienzan a explorarse los nodos hijo del siguiente nivel y as\u00ed sucesivamente hasta haber recorrido el grafo completo. Este recorrido es muy utilizado cuando se busca un camino con el n\u00famero m\u00ednimo de aristas entre dos v\u00e9rtices dados o cuando se busca un ciclo simple, es decir, una secuencia de nodos y aristas que se pueden describir circularmente en el que todos los nodos y aristas son distintos.</p> <p>Por ejemplo, sea el grafo de la figura4.4 que se corresponde con el grafo que se import\u00f3 anteriormente. Es posible recorrerlo utilizando BFS escribiendo el siguiente c\u00f3digo Neo4j que aparece en el listado 4.7. </p> <p></p> <p>Figura 4.4: Visualizaci\u00f3n del grafo de ciudades</p> <pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_BFR',\n    'Place',\n    'EROAD'\n)\n</code></pre> <p><pre><code>//Realizamos la b\u00fasqueda en anchura\nMATCH (a:Place{id:'Doncaster'})\nWITH id(a) AS startNode\nCALL gds.bfs.stream('myGraph_BFR', {sourceNode: startNode})\nYIELD path\nUNWIND [ n in nodes(path) | n.id ] AS tags\nRETURN tags\n</code></pre> Listado 4.7: Recorrido en anchura</p> <p>En el fragmento anterior, se define un grafo llamado myGraph_BFR el cual est\u00e1 formado por nodos de tipo Place y cuyas relaciones son del tipo EROAD y tienen una propiedad, denominada distance. A continuaci\u00f3n, se define el nodo inicial con las sentencias MATCH y WITH. Despu\u00e9s, se llama al m\u00e9todo que realiza el recorrido BFS sobre el grafo anterior partiendo desde el nodo inicial para producir un camino (path). Finalmente, se devuelven todos los nodos que forman parte del camino, mostrando los identificadores de cada uno de ellos. El resultado del recorrido es: Doncaster, London, Colchester, Ipswich, Felixstowe, Hoek van Holand, Den Haag, Rotterdam, Gouda, Utrecht.</p> <p>Recorrido en profundidad (DFS)</p> <p>El recorrido en profundidad, tambi\u00e9n conocido por sus siglas en ingl\u00e9s Depth First Search (DFS) es un procedimiento alternativo al recorrido en anchura para explorar un grafo, recorriendo todos sus nodos y aristas.</p> <p>El procedimiento para recorrer un grafo mediante el m\u00e9todo DFS es el siguiente: en primer lugar, se parte de un nodo inicial. A partir de \u00e9l, se empiezan a recorrer todos sus hijos hasta el \u00faltimo nivel. Una vez llegados al \u00faltimo nivel del grafo, se vuelve hacia atr\u00e1s recorriendo todos los hijos de los nodos de niveles anteriores. </p> <p>En Neo4j, el procedimiento para recorrer un grafo utilizando el recorrido BFS se muestra en el listado de c\u00f3digo 4.8. El procedimiento, como se puede comprobar, es pr\u00e1cticamente id\u00e9ntico al del listado 4.7 solo que cambiando el m\u00e9todo de recorrido. El resultado obtenido es: Doncaster, London, Colchester, Ipswich, Felixstowe, Hoek van Holland, Rotterdam, Den Haag, Gouda, Utrecht.</p> <p><pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_DFS',\n    'Place',\n    'EROAD'\n)\n\n//Realizamos la b\u00fasqueda en Profundidad\nMATCH (a:Place{id:'Doncaster'})\nWITH id(a) AS startNode\nCALL gds.dfs.stream('myGraph_BFR', {sourceNode: startNode})\nYIELD path\nUNWIND [ n in nodes(path) | n.id ] AS tags\nRETURN tags\n</code></pre> Listado 4.8: Recorrido en profundidad</p>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#434-caminos-minimos","title":"4.3.4 Caminos m\u00ednimos","text":"<p>El c\u00e1lculo y la obtenci\u00f3n de caminos m\u00ednimos dentro de un grafo es uno de los problemas cl\u00e1sicos en teor\u00eda de grafos con multitud de aplicaciones en el mundo real. Un camino dentro de un grafo es un conjunto de nodos y aristas que conectan un par de nodos del grafo. En el caso de los grafos no pesados, es decir, aquellos cuyas aristas no tienen coste, el c\u00e1lculo del camino m\u00ednimo se obtiene minimizando el n\u00famero de saltos entre el nodo origen y el nodo destino. En el caso de los grafos pesados, aquellos cuyas aristas tienen coste, el c\u00e1lculo del camino m\u00ednimo se obtiene a trav\u00e9s del camino en el que la suma de los costes de sus aristas es m\u00ednima.</p> <p>ver presentaci\u00f3n BDA4.5</p> <p>Junto con los recorridos de grafos, los m\u00e9todos de obtenci\u00f3n de caminos m\u00ednimos son muy \u00fatiles cuando se trabaja en entornos din\u00e1micos, donde aparecen y desaparecen continuamente aristas del grafo o sus costes cambian din\u00e1micamente. El c\u00e1lculo de caminos m\u00ednimos tambi\u00e9n es muy \u00fatil en aplicaciones que deben dar respuestas en tiempo real. Algunos ejemplos claros pueden ser encontrar rutas entre dos localidades, como ocurre en Google Maps u obtener los grados de separaci\u00f3n entre una empresa y un empleado potencial al que se quiere contratar, como puede ocurrir en LinkedIn.</p> <ul> <li>Algoritmo de Dijkstra</li> </ul> <p>El algoritmo de Dijkstra es uno de los algoritmos m\u00e1s utilizados en teor\u00eda de grafos para la obtenci\u00f3n de caminos m\u00ednimos. Dado el grafo de ciudades con el que se est\u00e1 trabajando a lo largo de este cap\u00edtulo, el listado 4.9 permite obtener los caminos m\u00ednimos entre un nodo del grafo y todos los dem\u00e1s nodos alcanzables(Single-Source Shortest Path).</p> <p><pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_Dijkstra',\n    'Place',\n    'EROAD',\n    {\n        relationshipProperties: 'distance'\n    }\n)\n\nMATCH (source:Place {id: 'Amsterdam'})\nCALL gds.allShortestPaths.dijkstra.stream('myGraph_Dijkstra', {\n    sourceNode: source,\n    relationshipWeightProperty: 'distance'\n})\nYIELD index, sourceNode, targetNode, totalCost, nodeIds, costs, path\nRETURN\n    index,\n    gds.util.asNode(sourceNode).id AS sourceNodeName,\n    gds.util.asNode(targetNode).id AS targetNodeName,\n    totalCost,\n    [nodeId IN nodeIds | gds.util.asNode(nodeId).id] AS nodeNames,\n    costs,\n    nodes(path) as path\nORDER BY index\n</code></pre> Listado 4.9: Ejecuci\u00f3n del algoritmo de Dijkstra sobre el nodo Amsterdam</p> <p>En el listado anterior, se utiliza la sentencia MATCH para definir el nodo de origen y, posteriormente, se guarda el grafo incluyendo el nodo de origen y la propiedad de distancia que es la utilizada para obtener el camino m\u00ednimo. A continuaci\u00f3n, se producir\u00e1 un conjunto de caminos en el que se mostrar\u00e1 informaci\u00f3n completa del nodo de origen, destino, coste total y camino recorrido as\u00ed como los distintos costes parciales. En caso de querer obtener el camino m\u00ednimo entre un par de nodos concretos(Single Source-Target Shortest Path), es posible utilizar el c\u00f3digo mostrado en el listado 4.10 an\u00e1logo al listado 4.9.</p> <p><pre><code>MATCH (source:Place {id: 'Amsterdam'}), (target:Place {id: 'London'})\nCALL gds.shortestPath.dijkstra.stream('myGraph_Dijkstra', {\n    sourceNode: source,\n    targetNode: target,\n    relationshipWeightProperty: 'distance'\n})\nYIELD index, sourceNode, targetNode, totalCost, nodeIds, costs, path\nRETURN\n    index,\n    gds.util.asNode(sourceNode).id AS sourceNodeName,\n    gds.util.asNode(targetNode).id AS targetNodeName,\n    totalCost,\n    [nodeId IN nodeIds | gds.util.asNode(nodeId).id] AS nodeNames,\n    costs,\n    nodes(path) as path\nORDER BY index\n</code></pre> Listado 4.10: Ejecuci\u00f3n del algoritmo de Dijkstra sobre el par de nodos Amsterdam-London</p> <p>El camino m\u00ednimo entre Amsterdam y Londres es el formado por: Amsterdam Immingham, Doncaster, London. Este camino tiene un coste total de 720.</p> <ul> <li>Algoritmo A*</li> </ul> <p>El algoritmo A* es una t\u00e9cnica de b\u00fasqueda informada que permite, al igual que el algoritmo de Dijkstra, calcular el camino m\u00ednimo entre dos nodos dados cualesquiera. Al contrario que el algoritmo de Dijkstra, cuando se va a seleccionar el siguiente nodo del camino, la t\u00e9cnica A no solo se tiene en cuanta la distancia ya calculada, sino que este algoritmo combina la distancia calculada con el resultado de una funci\u00f3n heur\u00edstica. Esta funci\u00f3n toma un nodo como entrada y devuelve un valor que corresponde con el coste de llegar al nodo objetivo desde ese nodo. Esta t\u00e9cnica, que se conoce tambi\u00e9n como de b\u00fasqueda informada, contin\u00faa el recorrido del grafo en cada iteraci\u00f3n desde el nodo con el menor coste combinado entre la distancia ya calculada y la funci\u00f3n heur\u00edstica. El listado 4.11 muestra c\u00f3mo obtener el camino m\u00ednimo mediante el m\u00e9todo A entre Amsterdam y Londres.</p> <p><pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_A',\n    'Place',\n    'EROAD',\n    {\n        nodeProperties: ['latitude', 'longitude', 'population'],\n        relationshipProperties: 'distance'\n    }\n)\n\nMATCH (source:Place {id: 'Amsterdam'}), (target:Place {id: 'London'})\nCALL gds.shortestPath.astar.stream('myGraph_A', {\n    sourceNode: source,\n    targetNode: target,\n    latitudeProperty: 'latitude',\n    longitudeProperty: 'longitude',\n    relationshipWeightProperty: 'distance'\n})\nYIELD index, sourceNode, targetNode, totalCost, nodeIds, costs, path\nRETURN\n    index,\n    gds.util.asNode(sourceNode).id AS sourceNodeName,\n    gds.util.asNode(targetNode).id AS targetNodeName,\n    totalCost,\n    [nodeId IN nodeIds | gds.util.asNode(nodeId).id] AS nodeNames,\n    costs,\n    nodes(path) as path\nORDER BY index\n</code></pre> Listado 4.11: Ejecuci\u00f3n del algoritmo A* sobre el par de nodos Amsterdam-London</p> <p>Como se puede ver, este fragmento de c\u00f3digo es similar a los vistos anteriormente, solo que a la hora de guardar el grafo, se almacenan los propiedades de latitud y longitud, necesarias en Neo4j para ejecutar el algoritmo A*.</p>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#425-medidas-de-centralidad","title":"4.2.5 Medidas de centralidad","text":"<p>La centralidad se define como la relevancia de un nodo dentro de un grafo. Por tanto, el estudio de medidas de centralidad permite identificar nodos relevantes dentro de un grafo. Esta identificaci\u00f3n permitir\u00e1 entender el comportamiento de la red, qu\u00e9 nodos permiten viralizar con mayor rapidez el contenido de la red, desde qu\u00e9 nodos la informaci\u00f3n est\u00e1 m\u00e1s accesible, etc. El estudio de medidas de centralidad tiene multitud de aplicaciones, aunque son especialmente notables las relacionadas con los \u00e1mbitos de la publicidad y el marketing, donde el estudio de estas m\u00e9tricas permite identificar actores relevantes dentro de una red a los que se puede contratar para anunciar un producto o identificar sobre qu\u00e9 actores enviar informaci\u00f3n para alcanzar a m\u00e1s usuarios.</p> <p>Respecto a las medidas de centralidad, no existe una \u00fanica medida sino que, en funci\u00f3n de los datos y del prop\u00f3sito que se pretende alcanzar, se utilizan unas m\u00e9tricas u otras. a continuaci\u00f3n, se van a definir tres medidas de centralidad con las que se realizar\u00e1n ejemplos en Neo4j: centralidad de grado, cercan\u00eda e intermediaci\u00f3n. </p> <p>ver presentaci\u00f3n BDA4.6</p> <p>Para trabajar con medidas de centralidad, se va a importar un grafo social que servir\u00e1 de ejemplo para el c\u00e1lculo de estas m\u00e9tricas. El listado 4.12 muestra el c\u00f3digo necesario para su creaci\u00f3n. Por otra parte, la figura4.5 muestra una visualizaci\u00f3n del grafo importado.</p> <p><pre><code>// Importamos los nodos\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/\" AS base\nWITH base + \"social-nodes.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMERGE (:User {id: row.id})\n\n//Importamos las relaciones\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/\" AS base\nWITH base + \"social-relationships.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMATCH (source:User {id: row.src})\nMATCH (destination:User {id: row.dst})\nMERGE (source)-[:FOLLOWS]-&gt;(destination)\n</code></pre> Listado 4.12: Importaci\u00f3n de un grafo social</p> <p></p> <p>Figura 4.5: Visualizaci\u00f3n de un grafo social</p> <p>Centralidad de grado (Degree Centrality)</p> <p>El grado de un nodo en un grafo se define como el n\u00famero de aristas o conexiones que entran o salen de un nodo. As\u00ed pues, en un nodo de un grafo se distinguen dos grados: el grado de entrada, correspondiente al conjunto de conexiones que entran a un nodo y el grado de salida, que se corresponde con el conjunto de aristas que salen de un nodo. El primero indica la prominencia de un nodo dentro de la red, mientras que el segundo la influencia del nodo dentro del grafo. Esta medida de centralidad es muy utilizada para identificar actores prominentes o influyentes o para identificar conductas fraudulentas, caracterizadas en ocasiones por una actividad anormal. Para calcular la centralidad de grado en este grafo, se almacenar\u00e1 en primer lugar el grafo creado y, a continuaci\u00f3n, se ejecutar\u00e1 el m\u00e9todo que calcula la centralidad de grado seg\u00fan el c\u00f3digo mostrado en el listado 4.13. El nodo con mayor grado es Doug, que tiene un grado de 5, seguido de Alice que tiene un grado de 3.</p> <pre><code>CALL gds.graph.project(\n'myGraph_centralidad_grado',\n'User',\n{\n    FOLLOWS: {\n    orientation: 'REVERSE'\n    }\n}\n)\n</code></pre> <p>Como no tenemos ninguna propiedad/coste/puntuci\u00f3n en la relaci\u00f3n, no hay que indicarla. Si la quisieramos tener en cuenta habr\u00eda que a\u00f1adirla dentro de follow de la siguiente forma:</p> <pre><code>CALL gds.graph.project(\n'myGraph_centralidad_grado',\n'User',\n{\n    FOLLOWS: {\n    orientation: 'REVERSE',\n    properties: ['score']\n    }\n}\n)\n</code></pre> <p>La centralidad de grado de este grafo ser\u00eda</p> <p><pre><code>CALL gds.degree.stream('myGraph_centralidad_grado')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).id AS id, score AS followers\nORDER BY followers DESC, id DESC\n</code></pre> Listado 4.13: C\u00e1lculo de la centralidad de grado</p> <p>Cercan\u00eda (Closeness Centrality) Beta a 9-11-2022</p> <p>La cercan\u00eda es una medida de centralidad que determina qu\u00e9 nodos del grafo expanden r\u00e1pida y eficientemente la informaci\u00f3n a trav\u00e9s del grafo. Para calcular esta medida, se obtiene la suma del inverso de las distancias de un nodo al resto. De esta forma, dado un nodo u la cercan\u00eda del mismo se calcula por medio de la ecuaci\u00f3n4.1.</p> <p></p> <p>Donde n es el n\u00famero de nodos del grafo y d(u, v) es la distancia del camino m\u00ednimo entre u y v. La cercan\u00eda es una m\u00e9trica muy utilizada para estimar tiempos de llegada en redes log\u00edsticas, para descubrir actores en posiciones privilegiadas en redes sociales o para estudiar la prominencia de palabras en un documento en el campo de la miner\u00eda de textos. Para obtener la cercan\u00eda en el grafo social de ejemplo, se puede utilizar el fragmento de c\u00f3digo mostrado en el listado 4.14. En el grafo de la imagen, Doug y David tienen una cercan\u00eda de 1.</p> <p><pre><code>CALL gds.graph.project(\n    'myGraph_cercania',\n    'User',\n    'FOLLOWS'\n)\n\nCALL gds.beta.closeness.stream('myGraph_cercania')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).id AS id, score\nORDER BY score DESC\n</code></pre> Listado 4.14: C\u00e1lculo de la cercan\u00eda</p> <p>Indeterminaci\u00f3n (Betweenness Centrality)</p> <p>La intermediaci\u00f3n es una medida de centralidad que permite detectar la influencia que tiene un nodo o actor del grafo en el flujo de informaci\u00f3n o de recursos de la red. El c\u00e1lculo de la intermediaci\u00f3n permite identificar a nodos que hacen de puentes entre distintas porciones del grafo. Esta medida de centralidad es muy utilizada para la identificaci\u00f3n de influencers y el estudio de la viralizaci\u00f3n de mensajes en redes sociales. Intuitivamente, la intermediaci\u00f3n de un nodo ser\u00e1 mayor en tanto en cuanto dicho nodo aparezca en los caminos m\u00ednimos de cualquier otro par de nodos. M\u00e1s formalmente, la intermediaci\u00f3n puede calcularse seg\u00fan la ecuaci\u00f3n4.2.</p> <p></p> <p>Donde u es el nodo del cual se calcula la intermediaci\u00f3n (B),s y t son nodos del grafo, p(u) es el n\u00famero de caminos m\u00ednimos entre s y t que pasan por u y p es el n\u00famero de caminos m\u00ednimos entre s y t. Para el c\u00e1lculo de la intermediaci\u00f3n, se puede utilizar el fragmento de c\u00f3digo mostrado en el listado 4.15. Al ejecutar este listado, se obtiene que Alice tiene la mayor intermediaci\u00f3n, teniendo un valor de 10.</p> <pre><code>CALL gds.graph.project(\n    'myGraph_intermediacion',\n    'User',\n    'FOLLOWS'\n)\n</code></pre> <p>Como no tenemos ninguna propiedad/coste/puntuci\u00f3n en la relaci\u00f3n, no hay que indicarla. Si la quisieramos tener en cuenta habr\u00eda que a\u00f1adirla dentro de follow de la siguiente forma:</p> <pre><code>CALL gds.graph.project(\n    'myGraph_intermediacion',\n    'User',\n    {\n        FOLLOWS:\n        {\n            properties: 'weight'\n        }\n    }\n)\n</code></pre> <p>El c\u00e1lculo de la indeterminaci\u00f3n ser\u00eda</p> <p><pre><code>CALL gds.betweenness.stream('myGraph_intermediacion')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).id AS id, score\nORDER BY score DESC\n</code></pre> Listado 4.15: C\u00e1lculo de la intermediaci\u00f3n</p>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#426-deteccion-de-comunidades","title":"4.2.6 Detecci\u00f3n de comunidades","text":"<p>A la hora de trabajar con grafos reales, que presentan una gran cantidad de nodos y enlaces, muchas veces se pretende identificar comunidades dentro del grafo para aplicar algoritmos sobre ellas. Una comunidad es, por tanto, un conjunto de nodos que presentan m\u00e1s relaciones entre s\u00ed que con el resto de nodos fuera de la comunidad. La detecci\u00f3n e identificaci\u00f3n de comunidades permite identificar comportamientos emergentes y de reba\u00f1o dentro de una red. De esta forma, es posible detectar y predecir h\u00e1bitos de los usuarios. A continuaci\u00f3n, se van a aplicar tres m\u00e9todos de detecci\u00f3n de comunidades sobre el grafo social de ejemplo que se ha utilizado en secciones anteriores.</p> <p>ver presentaci\u00f3n BDA4.7</p> <p>Conteo de tri\u00e1ngulos(Triangle Count)</p> <p>Un tri\u00e1ngulo es un conjunto de tres nodos que tienen relaciones entre s\u00ed. El conteo de tri\u00e1ngulos para un nodo dado, permite estudiar o inspeccionar de forma global un grafo y, aplicado sobre componentes conexas, permite inspeccionar regiones de un grafo. Para aplicar este m\u00e9todo, es necesario almacenar el grafo teniendo en cuenta que las aristas deben almacenarse como UNDIRECTED. El listado 4.16 muestra el fragmento de c\u00f3digo necesario para aplicar este m\u00e9todo sobre el grafo social de ejemplo. La ejecuci\u00f3n de este m\u00e9todo da como resultado que Alice y Doug son aquellos que pertenecen a m\u00e1s tri\u00e1ngulos, un total de 5.</p> <p><pre><code>CALL gds.graph.project(\n    'myGraph_conteo_triangulo',\n    'User',\n    {\n        FOLLOWS: {\n        orientation: 'UNDIRECTED'\n        }\n    }\n)\n\nCALL gds.triangleCount.stream('myGraph_conteo_triangulo')\nYIELD nodeId, triangleCount\nRETURN gds.util.asNode(nodeId).id AS id, triangleCount\nORDER BY triangleCount DESC\n</code></pre> Listado 4.16: Ejecuci\u00f3n del conteo de tri\u00e1ngulos</p> <p>Coeficiente local de clustering (Local Clustering Coefficient)</p> <p>Este coeficiente proporciona una medida cuantitativa del grado de agrupaci\u00f3n de un nodo. Para el c\u00e1lculo del coeficiente local de clustering se utiliza el conteo de tri\u00e1ngulos, de forma que se compara el grado de agrupaci\u00f3n de un nodo con el grado m\u00e1ximo de agrupaci\u00f3n que podr\u00eda tener. As\u00ed pues, el coeficiente local de clustering se puede calcular mediante la ecuaci\u00f3n4.3.</p> <p></p> <p>Donde u es un nodo, R(u) es el n\u00famero de relaciones que a trav\u00e9s de los vecinos de u (lo cual puede medirse con el n\u00famero de tri\u00e1ngulos que pasan por u y k(u) es el grado de u. El c\u00e1lculo del coeficiente local de clustering puede realizarse por medio del listado 4.17. Los actores Bridget, Charles, Mark y Michael tienen el mayor coeficiente local de clustering, que es 1. </p> <p><pre><code>CALL gds.graph.project(\n    'myGraph_LCC',\n    'User',\n    {\n        FOLLOWS: {\n        orientation: 'UNDIRECTED'\n        }\n    }\n)\n\nCALL gds.localClusteringCoefficient.stream('myGraph_LCC')\nYIELD nodeId, localClusteringCoefficient\nRETURN gds.util.asNode(nodeId).id AS id, localClusteringCoefficient\nORDER BY localClusteringCoefficient DESC\n</code></pre> Listado 4.17: C\u00e1lculo del coeficiente local de clustering</p> <p>Por \u00faltimo, el coeficiente global de clustering se calcula como la suma normalizada de los coeficientes de clustering locales. As\u00ed, estos coeficientes nos permiten encontrar medidas cuantitativas para detectar comunidades, pudiendo especificar incluso un umbral para establecer la comunidad (por ejemplo, especificar que los nodos han de estar conectados en un 40 %).</p> <p>Componentes fuertemente conexas (Strongly Connected Components) alpha a 09-11-2022</p> <p>En un grafo dirigido, una componente fuertemente conexa es aquel grupo de nodos en el que cualquier nodo puede ser alcanzado por cualquier otro en ambas direcciones. El estudio de las componentes fuertemente conexas en un grafo permite estudiar la conectividad de la red. Para realizar el c\u00e1lculo de las componentes conexas, el cual se realiza en un tiempo de procesamiento proporcional al n\u00famero de nodos, es posible utilizar el listado 4.18. El resultado permite comprobar que hay un total de cuatro componentes fuertemente conexas en el grafo social.</p> <p><pre><code>CALL gds.graph.project(\n    'myGraph_SCC',\n    'User',\n    'FOLLOWS'\n)\n\nCALL gds.alpha.scc.stream('myGraph_SCC', {})\nYIELD nodeId, componentId\nRETURN gds.util.asNode(nodeId).id AS Name, componentId AS Component\nORDER BY Component DESC\n</code></pre> Listado 4.18: Obtenci\u00f3n de componentes fuertemente conexas</p>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#427-prediccion-de-enlaces","title":"4.2.7 Predicci\u00f3n de enlaces","text":"<p>Los grafos son estructuras de datos que representan sistemas din\u00e1micos, que evolucionan a lo largo del tiempo. Por este motivo, es muy com\u00fan que en un grafo cualquiera aparezcan y desaparezcan nuevos nodos y conexiones entre dichos nodos. Los m\u00e9todos de predicci\u00f3n de enlaces permiten predecir qu\u00e9 enlaces se formar\u00e1n pr\u00f3ximamente entre los nodos del grafo, permitiendo adelantarse a los acontecimientos y prevenir eventualidades. Como norma general, los m\u00e9todos de predicci\u00f3n de enlaces se basan en medidas de cercan\u00eda y de centralidad asumiendo que los nuevos enlaces se producir\u00e1n, mayoritariamente, sobre/desde los nodos m\u00e1s relevantes. A continuaci\u00f3n, se detalla el funcionamiento de tres m\u00e9todos com\u00fanmente utilizados en predicci\u00f3n de enlaces. </p> <p>ver presentaci\u00f3n BDA4.8</p> <p>Vecinos comunes (Common Neighbors) alpha a 09-11-2022</p> <p>El m\u00e9todo de vecinos comunes se basa en la idea gen\u00e9rica de que dos actores de la red que tienen una relaci\u00f3n con un usuario com\u00fan tendr\u00e1n m\u00e1s posibilidad de conectarse entre s\u00ed que quienes no. Formalmente, dados dos nodos, la posibilidad de que se produzca un enlace entre los nodos x e y viene dada por la ecuaci\u00f3n4.4.</p> <p></p> <p>Donde N(x) es el conjunto de nodos adyacentes a x y N(y) es el conjunto de nodos adyacentes a y. Cuanto mayor es el valor de CN calculado, existe mayor posibilidad de que se produzca un nuevo enlace entre x e y. En el grafo social de ejemplo, el c\u00e1lculo de vecinos comunes para Charles y Bridget se puede realizar a trav\u00e9s del listado 4.19. El resultado de este c\u00e1lculo es 2.</p> <p><pre><code>//No funciona ni en la documentaci\u00f3n oficial (09-11-2022)\nMATCH (x:User{id:'Charles'})\nMATCH (y:User{id:'Bridget'})\nRETURN gds.alpha.linkprediction.commonNeighbors(x,y) AS score\n</code></pre> Listado 4.19: Obtenci\u00f3n del valor de vecinos comunes para Charles y Bridget</p> <p>Adhesi\u00f3n preferencial (Preferential Attachment) alpha a 09-11-2022</p> <p>Este m\u00e9todo se basa en la idea general de que cuanto m\u00e1s conectado est\u00e1 un nodo, es m\u00e1s probable que reciba nuevos enlaces. Formalmente, esta idea se puede expresar por medio de la ecuaci\u00f3n4.5</p> <p></p> <p>En el grafo social de ejemplo, el c\u00e1lculo de adhesi\u00f3n preferencial para Charles y Bridget se puede realizar a trav\u00e9s del listado 4.20. El resultado de este c\u00e1lculo es 10.</p> <p><pre><code>MATCH (x:User{id:'Charles'})\nMATCH (y:User{id:'Bridget'})\nRETURN gds.alpha.linkprediction.preferentialAttachment(x, y) AS score\n</code></pre> Listado 4.20: Obtenci\u00f3n del valor de adhesi\u00f3n preferencial para Charles y Bridget</p> <p>Asignaci\u00f3n de recursos (Resource Allocation) alpha a 09-11-2022</p> <p>Se trata de una m\u00e9trica compleja que eval\u00faa la cercan\u00eda de un par de nodos para determinar la posibilidad de que, entre ellos, se produzca un nuevo enlace. Para ello, se utiliza la expresi\u00f3n dada en la ecuaci\u00f3n4.6.</p> <p></p> <p>En el grafo social de ejemplo, el c\u00e1lculo de la asignaci\u00f3n de recursos para Charles y Bridget se puede realizar a trav\u00e9s del listado 4.21. El resultado de este c\u00e1lculo es 0.309.</p> <p><pre><code>//No funciona ni en la documentaci\u00f3n oficial (09-11-2022)\nMATCH (x:User{id:'Charles'})\nMATCH (y:User{id:'Bridget'})\nRETURN gds.alpha.linkprediction.resourceAllocation(x, y) AS score\n</code></pre> Listado 4.21: Obtenci\u00f3n del valor de asignaci\u00f3n de recursos para Charles y Bridget</p>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/index.html#428-machine-learning","title":"4.2.8 Machine Learning","text":"<p>Existe tambi\u00e9n la posibilidad de trabajar con Machine learning en los sistema de Big Data orientados a grafos como Neo4j</p> <p>Toda la documentaci\u00f3n de los mismos se encuentra tambi\u00e9n en la documentaci\u00f3n oficial. Esta funcionalidad no forma parte de los Resultados de Aprendizaje de este m\u00f3dulo, ya que existe un m\u00f3dulo especif\u00edco para este tema. A\u00fan as\u00ed, podr\u00eda ser interesante investigarlo e intentar integrarlo como una pr\u00e1ctica opcional \ud83d\ude09</p> <ol> <li> <p>https://orientdb.org \u21a9</p> </li> <li> <p>https://github.com/twitter-archive/flockdb \u21a9</p> </li> <li> <p>https://objectivity.com/infinitegraph/ \u21a9</p> </li> <li> <p>https://neo4j.com \u21a9</p> </li> </ol>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/Ejercicios_clase/Ejemplo1.html","title":"Big Data APlicado","text":""},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/Ejercicios_clase/Ejemplo1.html#ud-4-gestion-de-soluciones-iv","title":"UD 4 - Gesti\u00f3n de Soluciones IV","text":""},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/Ejercicios_clase/Ejemplo1.html#ejemplo-1-neo4j","title":"Ejemplo 1 Neo4j","text":"<ol> <li> <p>Instalar Neo4j. Para ello puedes optar por diferentes opciones</p> <ul> <li>Instalarla en tu m\u00e1quina local (junto con Graph Data Science). Neo4j Community =&gt; https://neo4j.com/download-center/?ref=subscription#community</li> <li>Usar SandBox Neo4j.com: https://sandbox.neo4j.com/</li> <li>Necesitamos usar Graph Data Science =&gt; https://neo4j.com/docs/graph-data-science/current/</li> <li>Lo necesitamos para aplicar los algoritmos a nuestros grafos</li> <li>Si usas esta opci\u00f3n, tendr\u00e1s que instalar Neo4j Desktop (sigue las instrucciones una vez creado en sandbox)</li> <li>Crear un contendor docker. Yo voy a usar este caso</li> </ul> </li> <li> <p>Neo4j en Docker</p> </li> <li> <p>Vamos a la imagen oficial de mondodb en docker</p> <pre><code>docker pull neo4j\n</code></pre> </li> <li> <p>Debemos generar un contenedor que incluya Graph Data Science (https://neo4j.com/docs/graph-data-science/current/installation/installation-docker/)</p> </li> <li>Vamos a configurarlo en modo desarrollo, para no necesitar usuario y contrase\u00f1a</li> </ol> <p><code>docker run -it --rm -d \\     --name neo4j_1 \\     -p=7474:7474 -p=7687:7687 \\     --user=\"$(id -u):$(id -g)\" \\     -e NEO4J_AUTH=none \\     -e NEO4J_PLUGINS='[\"graph-data-science\"]' \\     neo4j</code></p> <ol> <li> <p>Vamos a realizar cada una de los c\u00f3digo de ejemplo que aparecen en la teor\u00eda</p> </li> <li> <p>Toda la documnetaci\u00f3n sobre Graph Databases la encontramos en la documnetaci\u00f3n oficial para desarrolladores de neo4j =&gt; https://neo4j.com/developer/graph-database/</p> </li> <li> <p>Entramos en neo4j =&gt; http://localhost:7474/</p> </li> <li> <p>Importamos el grafo de ejemplo</p> </li> </ol> <pre><code>// Importamos los nodos\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/transport-nodes.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMERGE (place:Place {id:row.id})\nSET place.latitude = toFloat(row.latitude),\nplace.longitude = toFloat(row.latitude),\nplace.population = toInteger(row.population)\n\n//Importamos las relaciones\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/transport-relationships.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMATCH (origin:Place {id: row.src})\nMATCH (destination:Place {id: row.dst})\nMERGE (origin)-[:EROAD {distance: toInteger(row.cost)}]-&gt;(destination)\n</code></pre> <ol> <li>Comprobamos</li> </ol> <pre><code>match(n) return n\n</code></pre> <ol> <li> <p>Algoritmo BFS (Breadth First Search)</p> <p>Creamos el grafo. S\u00f3lo una vez <pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_BFR',\n    'Place',\n    'EROAD'\n)\n</code></pre></p> </li> </ol> <p>B\u00fasqueda en anchura teniendo como inicio el nodo \"Doncaster\"</p> <pre><code>//Realizamos la b\u00fasqueda en anchura\nMATCH (a:Place{id:'Doncaster'})\nWITH id(a) AS startNode\nCALL gds.bfs.stream('myGraph_BFR', {sourceNode: startNode})\nYIELD path\nUNWIND [ n in nodes(path) | n.id ] AS tags\nRETURN tags\n</code></pre> <p>Podemos probar cambiando el nodo de inicio</p> <pre><code>//Realizamos la b\u00fasqueda en anchura\nMATCH (a:Place{id:'Amsterdam'})\nWITH id(a) AS startNode\nCALL gds.bfs.stream('myGraph_BFR', {sourceNode: startNode})\nYIELD path\nUNWIND [ n in nodes(path) | n.id ] AS tags\nRETURN tags\n</code></pre> <ol> <li> <p>Algoritmo DFS (Depth First Search)</p> <p>Creamos el grafo. S\u00f3lo una vez <pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_DFS',\n    'Place',\n    'EROAD'\n)\n</code></pre></p> </li> </ol> <p>B\u00fasqueda en anchura teniendo como inicio el nodo \"Doncaster\"</p> <pre><code>//Realizamos la b\u00fasqueda en Profundidad\nMATCH (a:Place{id:'Doncaster'})\nWITH id(a) AS startNode\nCALL gds.dfs.stream('myGraph_BFR', {sourceNode: startNode})\nYIELD path\nUNWIND [ n in nodes(path) | n.id ] AS tags\nRETURN tags\n</code></pre> <ol> <li>Caminos M\u00ednimos. Algoritmo de Dijkstra: Single-Source Shortest Path (de un nodo a tolos los dem\u00e1s ()</li> </ol> <pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_Dijkstra',\n    'Place',\n    'EROAD',\n    {\n        relationshipProperties: 'distance'\n    }\n)\n</code></pre> <pre><code>MATCH (source:Place {id: 'Amsterdam'})\nCALL gds.allShortestPaths.dijkstra.stream('myGraph_Dijkstra', {\n    sourceNode: source,\n    relationshipWeightProperty: 'distance'\n})\nYIELD index, sourceNode, targetNode, totalCost, nodeIds, costs, path\nRETURN\n    index,\n    gds.util.asNode(sourceNode).id AS sourceNodeName,\n    gds.util.asNode(targetNode).id AS targetNodeName,\n    totalCost,\n    [nodeId IN nodeIds | gds.util.asNode(nodeId).id] AS nodeNames,\n    costs,\n    nodes(path) as path\nORDER BY index\n</code></pre> <ol> <li>Caminos M\u00ednimos. Algoritmo de Dijkstra: Single Source-Target Shortest Path (de un nodo origen a otro concreto)</li> </ol> <pre><code>MATCH (source:Place {id: 'Amsterdam'}), (target:Place {id: 'London'})\nCALL gds.shortestPath.dijkstra.stream('myGraph_Dijkstra', {\n    sourceNode: source,\n    targetNode: target,\n    relationshipWeightProperty: 'distance'\n})\nYIELD index, sourceNode, targetNode, totalCost, nodeIds, costs, path\nRETURN\n    index,\n    gds.util.asNode(sourceNode).id AS sourceNodeName,\n    gds.util.asNode(targetNode).id AS targetNodeName,\n    totalCost,\n    [nodeId IN nodeIds | gds.util.asNode(nodeId).id] AS nodeNames,\n    costs,\n    nodes(path) as path\nORDER BY index\n</code></pre> <ol> <li>Caminos M\u00ednimos. Algoritmo A*</li> </ol> <pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_A',\n    'Place',\n    'EROAD',\n    {\n        nodeProperties: ['latitude', 'longitude', 'population'],\n        relationshipProperties: 'distance'\n    }\n)\n</code></pre> <pre><code>MATCH (source:Place {id: 'Amsterdam'}), (target:Place {id: 'London'})\nCALL gds.shortestPath.astar.stream('myGraph_A', {\n    sourceNode: source,\n    targetNode: target,\n    latitudeProperty: 'latitude',\n    longitudeProperty: 'longitude',\n    relationshipWeightProperty: 'distance'\n})\nYIELD index, sourceNode, targetNode, totalCost, nodeIds, costs, path\nRETURN\n    index,\n    gds.util.asNode(sourceNode).id AS sourceNodeName,\n    gds.util.asNode(targetNode).id AS targetNodeName,\n    totalCost,\n    [nodeId IN nodeIds | gds.util.asNode(nodeId).id] AS nodeNames,\n    costs,\n    nodes(path) as path\nORDER BY index\n</code></pre> <ol> <li>Medidas de centralidad. Importamos el grafo de ejemplo</li> </ol> <pre><code>// Importamos los nodos\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/\" AS base\nWITH base + \"social-nodes.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMERGE (:User {id: row.id})\n\n//Importamos las relaciones\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/\" AS base\nWITH base + \"social-relationships.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMATCH (source:User {id: row.src})\nMATCH (destination:User {id: row.dst})\nMERGE (source)-[:FOLLOWS]-&gt;(destination)\n</code></pre> <ol> <li>Medidas de centralidad. Centralidad de grado (Degree Centrality)</li> </ol> <pre><code>CALL gds.graph.project(\n'myGraph_centralidad_grado',\n'User',\n    {\n        FOLLOWS: {\n        orientation: 'REVERSE'\n        }\n    }\n)\n</code></pre> <p>Como no tenemos ninguna propiedad/coste/puntuci\u00f3n en la relaci\u00f3n, no hay que indicarla. Si la quisieramos tener en cuenta habr\u00eda que a\u00f1adirla dentro de follow de la siguiente forma:</p> <pre><code>CALL gds.graph.project(\n'myGraph_centralidad_grado',\n'User',\n    {\n        FOLLOWS: {\n        orientation: 'REVERSE',\n        properties: ['score']\n        }\n    }\n)\n</code></pre> <p>La centralidad de grado de este grafo ser\u00eda</p> <pre><code>CALL gds.degree.stream('myGraph_centralidad_grado')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).id AS id, score AS followers\nORDER BY followers DESC, id DESC\n</code></pre> <ol> <li>Medidas de centralidad. Cercan\u00eda (Closeness Centrality)</li> </ol> <pre><code>CALL gds.graph.project(\n    'myGraph_cercania',\n    'User',\n    'FOLLOWS'\n)\n</code></pre> <p>El c\u00e1lculo de la cercan\u00eda ser\u00eda</p> <pre><code>CALL gds.beta.closeness.stream('myGraph_cercania')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).id AS id, score\nORDER BY score DESC\n</code></pre> <ol> <li>Medidas de centralidad. Intermediaci\u00f3n (Betweenness Centrality)</li> </ol> <pre><code>CALL gds.graph.project(\n    'myGraph_intermediacion',\n    'User',\n    'FOLLOWS'\n)\n</code></pre> <p>Como no tenemos ninguna propiedad/coste/puntuci\u00f3n en la relaci\u00f3n, no hay que indicarla. Si la quisieramos tener en cuenta habr\u00eda que a\u00f1adirla dentro de follow de la siguiente forma:</p> <pre><code>CALL gds.graph.project(\n    'myGraph_intermediacion',\n    'User',\n    {\n        FOLLOWS:\n        {\n            properties: 'weight'\n        }\n    }\n)\n</code></pre> <p>El c\u00e1lculo de la Intermediaci\u00f3n ser\u00eda</p> <pre><code>CALL gds.betweenness.stream('myGraph_intermediacion')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).id AS id, score\nORDER BY score DESC\n</code></pre> <ol> <li>Detecci\u00f3n de comunidades. Conteo de tri\u00e1ngulos(Triangle Count)</li> </ol> <pre><code>CALL gds.graph.project(\n    'myGraph_conteo_triangulo',\n    'User',\n    {\n        FOLLOWS: {\n        orientation: 'UNDIRECTED'\n        }\n    }\n)\n\nCALL gds.triangleCount.stream('myGraph_conteo_triangulo')\nYIELD nodeId, triangleCount\nRETURN gds.util.asNode(nodeId).id AS id, triangleCount\nORDER BY triangleCount DESC\n</code></pre> <ol> <li>Detecci\u00f3n de comunidades. Coeficiente local de clustering (Local Clustering Coefficient)</li> </ol> <p><pre><code>CALL gds.graph.project(\n    'myGraph_LCC',\n    'User',\n    {\n        FOLLOWS: {\n        orientation: 'UNDIRECTED'\n        }\n    }\n)\n</code></pre> <pre><code>CALL gds.localClusteringCoefficient.stream('myGraph_LCC')\nYIELD nodeId, localClusteringCoefficient\nRETURN gds.util.asNode(nodeId).id AS id, localClusteringCoefficient\nORDER BY localClusteringCoefficient DESC\n</code></pre></p> <ol> <li>Detecci\u00f3n de comunidades. Componentes fuertemente conexas (Strongly Connected Components) alpha a 09-11-2022</li> </ol> <p><pre><code>CALL gds.graph.project(\n    'myGraph_SCC',\n    'User',\n    'FOLLOWS'\n)\n</code></pre> <pre><code>CALL gds.alpha.scc.stream('myGraph_SCC', {})\nYIELD nodeId, componentId\nRETURN gds.util.asNode(nodeId).id AS Name, componentId AS Component\nORDER BY Component DESC\n</code></pre></p> <ol> <li>Predicci\u00f3n de enlaces. Vecinos comunes (Common Neighbors) alpha a 09-11-2022</li> </ol> <pre><code>//No funciona ni en la documentaci\u00f3n oficial (09-11-2022)\nMATCH (x:User{id:'Charles'})\nMATCH (y:User{id:'Bridget'})\nRETURN gds.alpha.linkprediction.commonNeighbors(x,y) AS score\n</code></pre> <ol> <li>Predicci\u00f3n de enlaces. Adhesi\u00f3n preferencial (Preferential Attachment) alpha a 09-11-2022</li> </ol> <pre><code>MATCH (x:User{id:'Charles'})\nMATCH (y:User{id:'Bridget'})\nRETURN gds.alpha.linkprediction.preferentialAttachment(x, y) AS score\n</code></pre> <ol> <li>Predicci\u00f3n de enlaces. Asignaci\u00f3n de recursos (Resource Allocation) alpha a 09-11-2022</li> </ol> <pre><code>//No funciona ni en la documentaci\u00f3n oficial (09-11-2022)\nMATCH (x:User{id:'Charles'})\nMATCH (y:User{id:'Bridget'})\nRETURN gds.alpha.linkprediction.resourceAllocation(x, y) AS score\n</code></pre>"},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/Ejercicios_clase/Ejemplo2.html","title":"Big Data APlicado","text":""},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/Ejercicios_clase/Ejemplo2.html#ud-4-gestion-de-soluciones-iv","title":"UD 4 - Gesti\u00f3n de Soluciones IV","text":""},{"location":"UD4%20-%20Gesti%C3%B3n%20de%20soluciones%20IV/Ejercicios_clase/Ejemplo2.html#ejemplo-2-neo4j","title":"Ejemplo 2 Neo4j","text":"<ol> <li> <p>Para esta pr\u00e1ctica vamos a usar la herramienta SandBox Neo4j: https://sandbox.neo4j.com/</p> <ul> <li>Crearemos un nuevo proyecto y elegiremos el proyecto de twitter.</li> <li>Vamos obtener conocimiento a trav\u00e9s de nuestras relaciones en nuestra cuenta oficial de Twitter</li> <li>Seguimos los pases para dar acceso a Neo4j a nuestra cuenta. No olvides derogar el permiso al terminar el ejemplo</li> </ul> </li> <li> <p>El esquema del grafo que nos va a crear de nuestra cuenta de Twitter es la siguiente</p> </li> </ol> <p></p> <ul> <li>Qui\u00e9n te est\u00e1 mencionando en Twitter</li> <li>\u00bfQui\u00e9nes son tus seguidores m\u00e1s influyentes?</li> <li>Qu\u00e9 etiquetas usas con frecuencia</li> <li>\u00bfCu\u00e1ntas personas a las que sigues tambi\u00e9n te siguen?</li> <li>La gente tuitea sobre ti, pero no los sigues</li> <li>Enlaces de retweets interesantes</li> <li>Otras personas tuiteando con algunos de tus mejores hashtags</li> </ul> <ol> <li>Tus menciones</li> </ol> <p>La siguiente consulta de Cypher determinar qui\u00e9n te est\u00e1 mencionando en Twitter.</p> <p><pre><code>// Graph of some of your mentions\nMATCH (u:Me:User)-[p:POSTS]-&gt;(t:Tweet)-[:MENTIONS]-&gt;(m:User)\nWITH u,p,t,m, COUNT(m.screen_name) AS count\nORDER BY count DESC\nRETURN u,p,t,m\nLIMIT 10\n</code></pre> Detalles de algunas de tus menciones</p> <pre><code>// Detailed table of some of your mentions\nMATCH (u:User:Me)-[:POSTS]-&gt;(t:Tweet)-[:MENTIONS]-&gt;(m:User)\nRETURN m.screen_name AS screen_name, COUNT(m.screen_name) AS count \nORDER BY count DESC\nLIMIT 10\n</code></pre> <ol> <li>Seguidores m\u00e1s influyentes</li> </ol> <pre><code>// Most influential followers\nMATCH (follower:User)-[:FOLLOWS]-&gt;(u:User:Me)\nRETURN follower.screen_name AS user, follower.followers AS followers\nORDER BY followers DESC\nLIMIT 10\n</code></pre> <ol> <li>Hashtags mas usados</li> </ol> <pre><code>// The hashtags you have used most often\nMATCH (h:Hashtag)&lt;-[:TAGS]-(t:Tweet)&lt;-[:POSTS]-(u:User:Me)\nWITH h, COUNT(h) AS Hashtags\nORDER BY Hashtags DESC\nRETURN h.name, Hashtags\nLIMIT 10\n</code></pre> <ol> <li>Tasa de seguimiento. \u00bfA qu\u00e9 ritmo las personas a las que sigues tambi\u00e9n te siguen a ti?</li> </ol> <pre><code>// Followback rate\nMATCH (me:User:Me)-[:FOLLOWS]-&gt;(f)\nWITH me, f, size((f)-[:FOLLOWS]-&gt;(me)) as doesFollowBack\nRETURN SUM(doesFollowBack) / toFloat(COUNT(f))  AS followBackRate\n</code></pre> <ol> <li>Recomendaciones de seguidores. \u00bfQui\u00e9n tuitea sobre ti, pero no lo sigues?</li> </ol> <pre><code>// Follower Recommendations - tweeting about you, but you don't follow\nMATCH (ou:User)-[:POSTS]-&gt;(t:Tweet)-[mt:MENTIONS]-&gt;(me:User:Me)\nWITH DISTINCT ou, me\nWHERE (ou)-[:FOLLOWS]-&gt;(me) AND NOT (me)-[:FOLLOWS]-&gt;(ou)\nRETURN ou.screen_name\n</code></pre> <ol> <li>Enlaces de retweets interesantes. \u00bfQu\u00e9 enlaces retuiteas y con qu\u00e9 frecuencia se marcan como favoritos?</li> </ol> <pre><code>// Links from interesting retweets\nMATCH\n  (:User:Me)-[:POSTS]-&gt;\n  (t:Tweet)-[:RETWEETS]-&gt;(rt)-[:CONTAINS]-&gt;(link:Link)\nRETURN\n  t.id_str AS tweet, link.url AS url, rt.favorites AS favorites\nORDER BY\n  favorites DESC\nLIMIT 10\n</code></pre> <p>Mismo que el anterior, pero mostrando el nombre del usuario del tweet original</p> <pre><code>// Links from interesting retweets\nMATCH\n  (u:User:Me)-[:POSTS]-&gt;(t:Tweet)-[:RETWEETS]-&gt;(rt)-[:CONTAINS]-&gt;(link:Link)\nMATCH\n  (u:User:Me)-[:POSTS]-&gt;(t_link:Tweet)-[:RETWEETS]-&gt;(rt)&lt;-[:POSTS]-(m:User)\nRETURN\n  t.id_str AS tweet, link.url AS url, rt.favorites AS favorites, m.screen_name\nORDER BY\n  favorites DESC\nLIMIT 10\n</code></pre> <ol> <li>Hashtags comunes. \u00bfQu\u00e9 usuarios tuitean con algunos de tus mejores hashtags?</li> </ol> <pre><code>// Users tweeting with your top hashtags\nMATCH\n  (me:User:Me)-[:POSTS]-&gt;(tweet:Tweet)-[:TAGS]-&gt;(ht)\nMATCH\n  (ht)&lt;-[:TAGS]-(tweet2:Tweet)&lt;-[:POSTS]-(sugg:User)\nWHERE\n  sugg &lt;&gt; me\n  AND NOT\n  (tweet2)-[:RETWEETS]-&gt;(tweet)\nWITH\n  sugg, collect(distinct(ht)) as tags\nRETURN\n  sugg.screen_name as friend, size(tags) as common\nORDER BY\n  common DESC\nLIMIT 20\n</code></pre>"},{"location":"UD5%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html","title":"UD 5 - Introducci\u00f3n a la Computaci\u00f3n Paralela y Distribuida","text":""},{"location":"UD5%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html#51-computacion-paralela","title":"5.1 Computaci\u00f3n Paralela","text":"<p>Evoluci\u00f3n de computaci\u00f3n desde Von Neuman a los sistemas actuales de computaci\u00f3n</p> <p>Tanto el volumen de datos como la velocidad de procesamiento han seguido una trayectoria de aumento exponencial desde el inicio de la era digital, pero el primero ha aumentado a un ritmo mucho mayor que el segundo. De esta forma se hace imprescindible contar con nuevas herramientas como la computaci\u00f3n o el procesamiento paralelo para conseguir salvar la brecha generada. Adem\u00e1s de proporcionar una mayor capacidad de procesamiento para hacer frente a los requisitos de los grandes conjuntos de datos, el procesamiento paralelo tiene el potencial de aliviar el cuello de botella de von Neumann cuando los datos necesarios no pueden ser suministrados al procesador a la velocidad requerida.</p> <p>Los algoritmos y arquitecturas de procesamiento paralelo se estudian desde desde la d\u00e9cada de 1950 como una forma de mejorar el rendimiento de los sistemas de informaci\u00f3n y, m\u00e1s recientemente, como una forma de mejorar su rendimiento manteniendo el consumo de energ\u00eda bajo control.</p> <p>Los primeros supercomputadores segu\u00edan una arquitectura de flujo de instrucciones \u00fanico, flujo de datos m\u00faltiple (SIMD) que utiliza una \u00fanica unidad de ejecuci\u00f3n de instrucciones, con cada instrucci\u00f3n aplicada a m\u00faltiples elementos de datos simult\u00e1neamente. La otra clase principal de arquitecturas paralelas se conoce como MIMD, en la que hay m\u00faltiples flujos de instrucciones adem\u00e1s de m\u00faltiples flujos de datos. Los superordenadores modernos, incluida la mayor\u00eda de las entradas de la lista Top500 Supercomputers, tienden a ser MIMD. Los superordenadores de gama alta suelen utilizarse para realizar c\u00e1lculos num\u00e9ricos intensivos con n\u00fameros en coma flotante. Por ello, su rendimiento se mide en operaciones de coma flotante por segundo, o FLOPS.</p> <p>Por otro lado, la ley de Moore muestra el crecimiento exponencial en el n\u00famero de transistores, los cu\u00e1les eran doblados aproximadamente cada 2 a\u00f1os. Hasta principios del siglo XXI, el aumento de la densidad de los chips iba acompa\u00f1ado de una mejora exponencial del rendimiento, debido a las correspondientes frecuencias de reloj m\u00e1s altas. Despu\u00e9s, las l\u00edneas de tendencia de la frecuencia de reloj y el rendimiento comenzaron a aplanarse, sobre todo por el efecto de la disipaci\u00f3n del calor, lo que dificult\u00f3 la refrigeraci\u00f3n de los circuitos superdensos. En ese momento, la atenci\u00f3n a la mejora del rendimiento se traslad\u00f3 al uso de m\u00faltiples procesadores independientes en el mismo chip, dando lugar a los procesadores multin\u00facleo. Esto provoca que el rendimiento sea m\u00e1s eficiente desde el punto de vista energ\u00e9tico, dado que el consumo de energ\u00eda es una funci\u00f3n superlineal de el rendimiento de un solo n\u00facleo.</p> <p>Sin embargo, la aparici\u00f3n del Big Data requiere una reevaluaci\u00f3n de la forma en que medir el rendimiento de los superordenadores. Mientras que la clasificaci\u00f3n de FLOPS la entrada/salida y el ancho de banda de almacenamiento asumen un papel m\u00e1s importante en la determinaci\u00f3n del rendimiento (un superordenador que realiza c\u00e1lculos cient\u00edficos puede recibir escasos par\u00e1metros de entrada y un conjunto de ecuaciones que definen un un modelo y luego realizar c\u00e1lculos durante d\u00edas o semanas, antes de arrojar las respuestas), muchas aplicaciones Big Data requieren un flujo constante de nuevos datos que se introducen, se almacenan, se procesan y se ofrecen como resultados, con lo que posiblemente se ponga a prueba la memoria y el ancho de banda de E/S, capacidades que suelen ser m\u00e1s limitadas que la velocidad de c\u00e1lculo.</p> <p>Por tanto, en los \u00faltimos a\u00f1os, se han introducido muchos modelos abstractos de procesamiento paralelo en un esfuerzo por para representar con precisi\u00f3n los recursos de procesamiento, almacenamiento y comunicaci\u00f3n, junto con sus interacciones, permitiendo a los desarrolladores de aplicaciones paralelas visualizar y aprovechar las ventajas disponibles, sin tener que ocuparse en detalles espec\u00edficos de la m\u00e1quina. Con un alto nivel de abstracci\u00f3n podemos distinguir los enfoques de procesamiento paralelo de datos y de control.</p> <p>El paralelismo de datos implica la partici\u00f3n de un gran conjunto de datos entre m\u00faltiples nodos de procesamiento, cada uno de ellos operando sobre una parte asignada de los datos, antes de que los resultados parciales se terminen combinando. A menudo, en las aplicaciones Big Data el mismo conjunto de operaciones debe ejecutarse en cada subconjunto de datos, por lo que el procesamiento SIMD es la alternativa m\u00e1s eficiente. En la pr\u00e1ctica, sin embargo, la sincronizaci\u00f3n de un gran n\u00famero de nodos de procesamiento introduce sobrecargas e ineficiencias que reducen las ganancias de velocidad. Por eso puede ser beneficioso dirigir los nodos de procesamiento para que ejecuten un \u00fanico programa en m\u00faltiples de datos de forma as\u00edncrona, con una coordinaci\u00f3n dispersa.</p> <p>En los esquemas basados en el paralelismo de control varios nodos operan independientemente resolviendo subproblemas, sincroniz\u00e1ndose entre s\u00ed mediante el env\u00edo de mensajes informativos y de sincronizaci\u00f3n. Dicha independencia permite emplear recursos heterog\u00e9neos y recursos espec\u00edficos de la aplicaci\u00f3n sin interferencias cruzadas ni recursos m\u00e1s lentos obstaculicen el progreso de los m\u00e1s r\u00e1pidos. La Ley de Amdahl (Gene Amdahl, 1867) dice que a no ser que un programa secuencial pudiera ser completamente paralelizado, el speedup o aceleraci\u00f3n que se obtendr\u00e1 ser\u00e1 muy limitado independientemente del n\u00famero de n\u00facleos disponibles.</p> <p>ver presentaci\u00f3n BDA5.1</p>"},{"location":"UD5%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html#52-computacion-distribuida","title":"5.2 Computaci\u00f3n Distribuida","text":"<p>Un sistema distribuido es una colecci\u00f3n de ordenadores aut\u00f3nomos enlazados por una red de ordenadores y soportados por un software que hace que la colecci\u00f3n act\u00fae como un servicio integrado.</p> <p>Algunos de los aspectos m\u00e1s importantes a tener en cuenta dentro de un sistema distribuido actual son los siguientes:</p> <ul> <li> <p>Apertura: El sistema debe ser ampliable, es decir, debe existir la posibilidad de a\u00f1adir nuevos recursos y servicios compartidos y que \u00e9stos se puedan poner a disposici\u00f3n de los diferentes componentes que forman el sistema.</p> </li> <li> <p>Concurrencia: Los distintos componentes de un sistema distribuido pueden demandar acceder a un recurso simult\u00e1neamente. Es necesario que el sistema est\u00e9 dise\u00f1ado para permitirlo.</p> </li> <li> <p>Escalabilidad: Un sistema es escalable si mantiene su eficiencia cuando hay un incremento significativo en el n\u00famero de recursos y el n\u00famero de usuarios.</p> </li> <li> <p>Heterogeneidad: El sistema distribuido puede estar formado por una variedad de diferentes redes, sistemas operativos, hardware, etc. A pesar de estas diferencias, los componentes deben poder interactuar entre s\u00ed.</p> </li> <li> <p>Tolerancia a fallos: Cualquier sistema puede fallar. En el caso de un sistema distribuido a pesar del fallo de un componente tiene que ser posible que el sistema siga funcionando.</p> </li> <li> <p>Transparencia: La transparencia permite que ciertos aspectos del sistema sean invisibles a las aplicaciones, por ejemplo, la ubicaci\u00f3n, el acceso, la concurrencia, la gesti\u00f3n de los fallos, la replica de la informaci\u00f3n, etc.</p> </li> </ul> <p>Los sistemas computacionales distribuidos son sistemas de computaci\u00f3n de alto rendimiento que est\u00e1n formados por conjuntos de computadores interconectados mediante una red que ofrecen funcionalidades diversas algunas de ellas propias de la supercomputaci\u00f3n, tales como la computaci\u00f3n paralela. Entre estos sistemas cabe destacar los basados en Grid o en arquitecturas Cloud (en la nube) y de forma m\u00e1s b\u00e1sica, los denominados Clusters.</p> <p>Los Clusters est\u00e1n formados por colecciones de computadores de similares caracter\u00edsticas interconectados mediante una red de \u00e1rea local(ver figura5.1). Los computadores hacen uso de un mismo sistema operativo y un middleware que se encarga de abstraer y virtualizar los diferentes computadores del sistema dando la visi\u00f3n al usuario de un sistema operativo \u00fanico. Los clusters son sistemas dedicados a la supercomputaci\u00f3n. El sistema operativo de un cluster es est\u00e1ndar y, por lo tanto, es el middleware quien provee de librer\u00edas que permiten la computaci\u00f3n paralela.</p> <p></p> <p>Figura 5.1: Esquema de organizaci\u00f3n de un servicio de supercomputaci\u00f3n en Cluster . Fuente: Universidad de Castilla La Mancha</p> <p>ver presentaci\u00f3n BDA5.2</p>"},{"location":"UD5%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html#53-grid-computing","title":"5.3 Grid Computing","text":"<p>Se define Grid Computing como una infraestructura persistente que soporta la computaci\u00f3n y actividades de datos intensivas a trav\u00e9s de m\u00faltiples organizaciones virtuales. Evidentemente, todas las implementaciones de Grid Computing se basan en la comunicaci\u00f3n a trav\u00e9s de Internet mediante distintos protocolos. Actualmente existen dos aproximaciones para la implementaci\u00f3n de arquitecturas en Grid: Arquitecturas SOA y Arquitecturas Peer-to-Peer. </p> <p>Las arquitecturas SOA (Service Otiented Architecture) - (figura5.2) est\u00e1n basadas en la agregaci\u00f3n de servicios a los que se accede remotamente de forma independiente de las plataformas en las que se ejecutan y del lenguaje en los que est\u00e1n implementados. Un servicio es un programa autocontenido con una funci\u00f3n y un interfaz bien definido. Este tipo de servicios se adapta muy bien a los sistemas Grid ya que ofrece, entre otras ventajas, una alta portabilidad al permitir interactuar servicios de sistemas heterog\u00e9neos a trav\u00e9s de interfaces que son independientes de las plataformas donde est\u00e1n alojados; interoperabilidad a trav\u00e9s de protocolos est\u00e1ndar de redes, y a su vez, permiten el uso de clientes ligeros que a\u00edslan a los consumidores de la complejidad de los servicios.</p> <p></p> <p>Figura 5.2: Arquitecturas SOA</p> <p>Las arquitecturas Peer-To-Peer son agregaciones de programas equivalentes que se ejecutan sobre plataformas heterog\u00e9neas y que comparten parte de su memoria y capacidad de c\u00f3mputo. Estas plataformas presentan una alta tolerancia a errores, dado que cada nodo tiene la misma funci\u00f3n que el resto. Este tipo de arquitecturas son m\u00e1s habituales en un tipo de computaci\u00f3n conocido como Volunteer Computing.</p> <p>Escalabilidad</p> <p>La tecnolog\u00eda Grid permite de una forma muy f\u00e1cil la escalabilidad, puesto que los nodos que act\u00faan en ella son independientes a los dem\u00e1s. Esto permite que, si alguno de los nodos falla, se pueden delegar las tareas que estuviera realizando al resto de nodos, lo que permite seguir con la ejecuci\u00f3n del programa y evitar paradas que puedan afectar a la efectividad de la obtenci\u00f3n de los resultados deseados.</p> <p>Adem\u00e1s, este tipo de estructuraci\u00f3n permite que se puedan escalar los recursos para cada computador o nodo de forma independiente en caso de que fuera necesario, sin tener que afectar al resto de computadoras. En el caso de actualizaciones que deban sufrir los equipos para obtener nuevos fragmentos de programas o mejoras acerca de ciertos aspectos, esta tecnolog\u00eda permite realizarlas de forma sencilla. Esto se consigue mediante la obtenci\u00f3n de los recursos que vayan a verse afectados por estas actualizaciones y son puestas offline y retiradas de la red. Mientras tanto, las tareas que suelan llevar son delegadas al resto de computadoras que se encuentren en las otras localizaciones. Esto permite que las actualizaciones se procedan en forma de cascada, haciendo que el trabajo en los proyectos que se est\u00e9n llevando a cabo no se vea afectado.</p> <p>Ventajas e Inconvenientes</p> <p>Algunas de las ventajas que ofrece el Grid Computing son las siguientes:</p> <ul> <li> <p>Es robusta en cuanto a sucesos que pudieran afectar a parte de su infraestructura, por ejemplo, cat\u00e1strofes naturales. Los entornos Grid son bastante modulares e independientes, lo que reduce sus puntos vulnerables a fallo.</p> </li> <li> <p>Es una manera muy eficaz de utilizar los recursos de una manera \u00f3ptima y eficiente en una organizaci\u00f3n.</p> </li> <li> <p>Puede utilizarse para balanceos de carga y conexiones de red que sean redundantes ofreciendo muchas facilidades en cuanto a la escalabilidad y actualizaci\u00f3n y siendo muy conveniente para ejecutar programas o tareas de forma paralela.</p> </li> </ul> <p>Algunas de las desventajas que tiene esta tecnolog\u00eda son: los riesgos de seguridad que ofrece la participaci\u00f3n de diferentes entidades heterog\u00e9neas y la necesidad de una interconexi\u00f3n de altas prestaciones.</p>"},{"location":"UD5%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html#54-cloud-computing","title":"5.4 Cloud Computing","text":"<p>La computaci\u00f3n en nube o Cloud Computing es una de las tecnolog\u00edas actuales de mayor implantaci\u00f3n. El concepto de computaci\u00f3n en la nube supone el siguiente paso evolutivo de la computaci\u00f3n distribuida superando los sistemas legacy y evolucionando hacia los sistemas distribuidos a gran escala. El objetivo de este modelo inform\u00e1tico es hacer un mejor uso de los recursos distribuidos, ponerlos en com\u00fan para lograr un mayor rendimiento y poder abordar problemas de computaci\u00f3n a gran escala.</p> <p>En la computaci\u00f3n en la nube se deben abordar diferentes retos como: la virtualizaci\u00f3n, la escalabilidad, la interoperabilidad, la calidad del servicio, la tolerancia a errores y los modelos de nube.</p> <p>B\u00e1sicamente, la nube puede definirse mediante tres modelos distintos:</p> <ul> <li> <p>Nube privada: Los datos y procesos se gestionan dentro de la organizaci\u00f3n sin las restricciones de ancho de banda de la red, los requerimientos de seguridad y los requisitos legales que supone el uso de los servicios de la nube p\u00fablica a trav\u00e9s de redes p\u00fablicas y abiertas. Algunos ejemplos son Amazon VPC.</p> </li> <li> <p>Nube p\u00fablica: Describe la computaci\u00f3n en nube en el sentido tradicional, los recursos son facilitados de forma din\u00e1mica sobre una base de auto-servicio a trav\u00e9s de aplicaciones web/servicios web, desde un proveedor externo que comparte recursos. Algunos ejemplos son Azure o Amazon EC2.</p> </li> <li> <p>Nube h\u00edbrida: El entorno est\u00e1 formado por m\u00faltiples proveedores internos y/o externos. Algunos ejemplos son RightScale, Asigra Hybrid Cloud Backup, Carpathia, Skytap y Elastra.</p> </li> </ul> <p>La arquitectura en la nube o arquitectura cloud subyacen en una infraestructura que se se utiliza s\u00f3lo cuando se necesita para extraer los recursos necesarios bajo demanda y realizar un trabajo espec\u00edfico, para luego ceder los recursos innecesarios. Los servicios son accesibles en cualquier parte del mundo, y la nube aparece como un \u00fanico punto de acceso para todas las necesidades inform\u00e1ticas de los usuarios. Las arquitecturas en la nube abordan las principales dificultades que rodean el procesamiento de datos a gran escala. Existen diferentes categor\u00edas de servicios en la nube, como infraestructura, plataforma y aplicaciones. Estos servicios se prestan y consumen en tiempo real a trav\u00e9s de Internet.</p> <p>Software como servicio - Software as a Service (SaaS)</p> <p>El software como servicio es una plataforma multiusuario. Utiliza recursos comunes y una instancia \u00fanica tanto del c\u00f3digo objeto de una aplicaci\u00f3n como de la base de datos subyacente para dar soporte a varios clientes simult\u00e1neamente. Este servicio es el nuevo m\u00e9todo en la distribuci\u00f3n de software de aplicaciones. Algunos ejemplos de los principales proveedores son SalesForce.com, NetSuite, Oracle, IBM y Microsoft, etc.</p> <p>Plataforma como servicio PaaS - Platform as a Service (PaaS)</p> <p>La plataforma como servicio proporciona a los desarrolladores una plataforma que incluye todos los sistemas y entornos, que comprende el ciclo de vida completo de desarrollo, prueba despliegue y alojamiento de aplicaciones web como un servicio prestado con una base en la nube. Proporciona una forma m\u00e1s f\u00e1cil de desarrollar aplicaciones empresariales y diversos servicios a trav\u00e9s de Internet. El PaaS se plantea para facilitar el mantenimiento de la infraestructura de trabajo de los diferentes sistemas. Su aplicaci\u00f3n debe reducir el tiempo de desarrollo al ofrecer un amplio abanico de herramientas y servicios f\u00e1cilmente disponibles, y con una r\u00e1pida capacidad de escalado.</p> <p>Infraestructura como servicio - Infrastructure as a Service (IaaS)</p> <p>Infraestructura como servicio ofrece acceso basado en la web a almacenamiento y potencia de c\u00e1lculo. Por ello, no se necesita gestionar o controlar la infraestructura subyacente de la nube, pero si que se tiene control sobre los sistemas operativos, el almacenamiento y las aplicaciones desplegadas. Adem\u00e1s de una mayor flexibilidad, una de las principales ventajas de IaaS es el esquema de pago basado en el uso. Esto permite a los clientes pagar a medida que crecen. Otra ventaja importante es la de utilizar siempre la \u00faltima tecnolog\u00eda.</p> <p>El camino hacia la computaci\u00f3n en nube est\u00e1 impulsado por muchos factores, como la ubicuidad del acceso (todo lo que se necesita es un navegador), la facilidad de gesti\u00f3n (no hay necesidad de mejorar la experiencia del usuario, ya que no se necesita configuraci\u00f3n o copia de seguridad), y una menor inversi\u00f3n (soluci\u00f3n empresarial asequible desplegada sobre la base del pago por uso del hardware, con un software de sistemas proporcionado por los proveedores de la nube). Adem\u00e1s, la computaci\u00f3n en nube ofrece muchas ventajas a los proveedores, como una infraestructura f\u00e1cil de gestionar dado que el centro de datos tiene un hardware y un software de sistema homog\u00e9neos.</p>"},{"location":"UD5%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html#55-sistemas-de-archivos-distribuidos","title":"5.5 Sistemas de Archivos Distribuidos","text":"<p>El sistema de archivos es un subsistema de un sistema operativo cuyo objetivo es organizar, recuperar y almacenar datos archivos. Un sistema de archivos distribuido (DFS) es un sistema de archivos con archivos compartidos en recursos de almacenamiento dispersos en una red. El DFS hace f\u00e1cil la compartici\u00f3n de archivos entre aplicaciones cliente de forma controlada y autorizada. As\u00edmismo, los usuarios pueden beneficiarse de los servicios de un DFS ya que pueden localizar todos los archivos compartidos dentro de un \u00fanico servidor o nombre de dominio.</p> <p>Una gran variedad de aplicaciones de an\u00e1lisis Big Data dependen de entornos distribuidos para analizar grandes cantidades de datos. A medida que aumenta la cantidad de datos, la necesidad de proporcionar soluciones de almacenamiento fiables y eficientes se ha convertido en una de las principales preocupaciones de los administradores de infraestructuras de Big Data. Los sistemas y m\u00e9todos tradicionales de almacenamiento no son \u00f3ptimos debido a sus restricciones de precio o rendimiento, mientras que el DFS se ha desarrollado con el fin de facilitar  compartir archivos tanto en redes locales (LAN) como en redes de \u00e1rea amplia (WAN).</p> <p>Una de las principales caracter\u00edsticas de los DFS es la transparencia que hace que los archivos se lean, se almacenen y se gestionen en las m\u00e1quinas cliente, mientras que el procesamiento real se produce en los servidores, es decir, el DFS implementa sus pol\u00edticas de control de acceso y almacenamiento a sus clientes de forma centralizada proporcionando su servicios a trav\u00e9s de la red.</p> <p>El objetivo principal de la transparencia en el DFS es ocultar a los clientes el hecho de que los procesos y los recursos est\u00e1n distribuidos f\u00edsicamente en la red y proporcionar una visi\u00f3n com\u00fan de un sistema de archivos centralizado. Es decir, el DFS pretende ser invisible para los clientes, que consideran que el DFS es similar a un sistema de archivos local.</p> <p>Adem\u00e1s de la transparencia, otro de los factores fundamentales en los DFS es la fiabilidad. Dado que el lugar de uso de los archivos puede ser diferente de su lugar de almacenamiento, los modos de fallo son sustancialmente m\u00e1s complejos en los DFS en comparaci\u00f3n con los sistemas de archivos locales. La replica y la codificaci\u00f3n de borrado son dos t\u00e9cnicas t\u00edpicas para lograr una alta fiabilidad.</p> <p>Los DFS utilizan la replica haciendo varias copias de un archivo de datos en diferentes servidores. Cuando un cliente solicita el archivo, accede de forma transparente a una de las copias. Dentro de la estrategia de replica la ubicaci\u00f3n de las r\u00e9plicas juega un papel cr\u00edtico en tanto en el rendimiento como en la fiabilidad de los DFS. Las r\u00e9plicas se almacenan en diferentes servidores seg\u00fan de acuerdo con un esquema de colocaci\u00f3n. Muchos DFS (por ejemplo HDFS) utiliza por defecto la replica aleatoria en la que las r\u00e9plicas se almacenan aleatoriamente en diferentes nodos, en diferentes racks, en diferentes ubicaci\u00f3n geogr\u00e1fica, de modo que si se produce un fallo en cualquier parte del sistema, los datos siguen estando disponibles.</p> <p>Sin embargo, utilizando esta estrategia, no se puede separar eficazmente el cl\u00faster en niveles mientras se mantiene la consistencia del mismo y, por lo tanto, es bastante susceptible a la p\u00e9rdida frecuente de datos debido a fallos. La implementaci\u00f3n de HDFS  limita la colocaci\u00f3n de r\u00e9plicas de bloques a grupos m\u00e1s peque\u00f1os de nodos, lo que reduce la probabilidad de p\u00e9rdida de bloques con m\u00faltiples fallos de nodos. </p> <p>El procedimiento de replica dispersa m\u00faltiples copias de un archivo, y los cambios tienen que propagarse a todas las r\u00e9plicas. Esto hace que la consistencia de los datos sea un aspecto clave en el funcionamiento de los DFS.</p> <p>HDFS (Hadoop File System) es uno de los sistemas de archivos distribuidos m\u00e1s populares en la actualidad. Caracter\u00edsticas</p> <p>Arquitectura: Un cl\u00faster HDFS consiste en un \u00fanico nodo de nombres (namenode), que es el nodo maestro que gestiona el espacio de nombres del sistema de archivos y regula el acceso a los archivos por parte de los clientes. El flujo de datos se dividido en bloques distribuidos entre los datanodes, que gestionan el almacenamiento en los nodos en los que se ejecutan.</p> <p>Acceso: HDFS utiliza una librer\u00eda de c\u00f3digo que permite a los clientes leer, escribir y eliminar archivos y crear y eliminar directorios.</p> <p>Replica: HDFS divide los datos en bloques que se replican a trav\u00e9s de los datanodes de acuerdo con una pol\u00edtica de colocaci\u00f3n, por la cual cada datanode tiene como m\u00e1ximo una copia de un bloque y cada rack tiene como m\u00e1ximo dos copias del bloque.</p> <p>Tolerancia a fallos: Cada datanode compara la versi\u00f3n del software yel ID del espacio de nombres con los del namenode. Si no coinciden, el datanode se apaga para preservar la integridad del sistema.</p> <p>ver presentaci\u00f3n BDA5.3</p>"},{"location":"UD5%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html#56-tolerancia-a-fallos-en-big-data","title":"5.6 Tolerancia a fallos en Big Data","text":"<p>Todos los sistemas de Big Data necesitan manejar los fallos de software y hardware que se producen en el sistema despu\u00e9s de su desarrollo, lo que beneficiar\u00e1 a los sistemas de diferentes maneras, incluyendo: recuperaci\u00f3n de fallos, menor coste, mayor rendimiento, etc. La tolerancia a fallos es una configuraci\u00f3n que evita que un ordenador o dispositivo de red falle en caso de un problema o error inesperado. Para hacer que un computador o una red tolerante a fallos requiere que los usuarios o las empresas piensen en c\u00f3mo puede fallar un ordenador o dispositivo de red y tomen medidas que ayuden a prevenir ese tipo de fallos.</p> <p>El comportamiento de los sistemas Big Data se puede dividir en:</p> <ul> <li> <p>Batch Processing: Procesamiento por lotes de datos en reposo. En este escenario, los datos de origen se cargan en los dispositivos de almacenamiento de datos, ya sea por la propia aplicaci\u00f3n de origen o por un flujo de trabajo de orquestaci\u00f3n (orchestation workflow). A continuaci\u00f3n, los datos se procesan en el lugar por un trabajo paralelizado, que tambi\u00e9n puede ser iniciado por el flujo de trabajo de orquestaci\u00f3n.</p> </li> <li> <p>Stream Processing: Procesamieno continuo de datos en tiempo real, es decir, en cuando existe disponibilidad de datos estos se procesan de manera secuencial. Se establecen unos flujos de datos infinitos y sin l\u00edmites de tiempo.</p> </li> </ul> <p>Dentro de estos dos escenarios se pueden definir diferentes estrategias de tolerancia a fallos como se explica a continuaci\u00f3n.</p>"},{"location":"UD5%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html#561-tolerancia-a-fallos-en-batch","title":"5.6.1 Tolerancia a fallos en batch","text":"<p>En el modelo de c\u00e1lculo por lotes de Hadoop, las aplicaciones de dos principales mecanismos para hacer frente a los fallos son la replica de datos y el mecanismo de rollback (reversi\u00f3n).</p> <ul> <li> <p>Replica de datos (Data Replication): En el mecanismo de replica los datos estar\u00e1n en varios nodos de datos diferentes. Cuando se necesita la replica de datos, cualquier nodo de datos, cuya comunicaci\u00f3n no est\u00e9 ocupada puede copiar los datos. La principal ventaja de esta tecnolog\u00eda es que puede recuperarse instant\u00e1neamente de un fallo. El inconveniente es que se produce un consumo de una alta cantidad de recursos y la posibilidad de que los datos sean inconsistentes.</p> </li> <li> <p>Mecanismo de reversi\u00f3n (Rollback Mechanism): El informe de la copia (copy report) se guardar\u00e1 periodicamente. Si se produce un fallo el sistema se limita a volver a un punto de control (checkpoint) y, a continuaci\u00f3n, vuelve a iniciar la operaci\u00f3n desde ese punto. El m\u00e9todo adopta el concepto de rollback, es decir, el sistema volver\u00e1 al trabajo anterior. Este m\u00e9todo aumenta el tiempo de ejecuci\u00f3n de todo el sistema, porque el rollback necesita hacer una copia de seguridad y comprobar que se guarda un estado consistente. En comparaci\u00f3n con la replica supone m\u00e1s tiempo pero menos recursos.</p> </li> </ul>"},{"location":"UD5%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html#562-tolerancia-a-fallos-en-streaming","title":"5.6.2 Tolerancia a fallos en Streaming","text":"<p>En el sistema de streaming, hay tres tipos de estrategia para realizar la gesti\u00f3n de los fallos: la espera pasiva, la espera activa y la copia de seguridad ascendente.</p> <ul> <li>Espera pasiva (Passive Standby): El sistema har\u00e1 regularmente una copia de seguridad del \u00faltimo estado en el nodo maestro a una copia del nodo r\u00e9plica. Cuando se produzca un fallo, el estado del sistema se restaurar\u00e1 a partir de los datos de la copia de seguridad. La estrategia de replicaci\u00f3n pasiva admite el caso de que la carga de datos sea mayor, pero el tiempo de recuperaci\u00f3n se incrementa. Los datos de copia de seguridad pueden guardarse en un sistema de almacenamiento distribuido para reducir el tiempo de recuperaci\u00f3n.</li> </ul> <p></p> <p>Figura 5.3: Esquema de nodos en streaming. Upstream Nu y downstream Nd</p> <ul> <li> <p>Espera activa (Active Standby): Cuando el sistema transmite datos para el nodo maestro, tambi\u00e9n transmite una copia de los datos para una replica del nodo al mismo tiempo. Cuando el nodo maestro falla, una de las r\u00e9plicas del nodo asume completamente el trabajo, y los nodos suplentes necesitan la asignaci\u00f3n de los mismos recursos del sistema. De esta manera, el tiempo de recuperaci\u00f3n del fallo es m\u00e1s corto, pero el rendimiento de los datos es menor. Tambi\u00e9n desperdicia m\u00e1s recursos del sistema.</p> </li> <li> <p>Copia de seguridad ascendente (Upstream Backup). Cada nodo maestro almacena su propio estado y los datos de salida en un archivo de registro. Cuando un nodo maestro falla, el nodo maestro anterior en el flujo (upstream) reproducir\u00e1 una copia de los datos en un archivo de registro al nodo correspondiente con el fin de recalcular las operaciones. Esta estrategia necesita m\u00e1s tiempo para reconstruir el estado de la recuperaci\u00f3n, por lo que el tiempo de recuperaci\u00f3n de fallos tiende a ser largo. La estrategia de copia de seguridad ascendente es una mejor opci\u00f3n en unas circunstancias de escasez de recursos.</p> </li> </ul> <p>ver presentaci\u00f3n BDA5.4</p>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html","title":"UD 6 - Apache Hadoop","text":"<p>Apache Hadoop es un framework de software que soporta aplicaciones distribuidas bajo una licencia libre. Permite a las aplicaciones trabajar con miles de nodos y petabytes de datos.</p> <p>Fue inicialmente concebido para resolver un problema de escalabilidad en Nutch, un motor de b\u00fasqueda Open Source que pretend\u00eda indexar mil millones de p\u00e1ginas web.</p> Figura 6.1: Figura6.1_Componentes b\u00e1sicos de Hadoop <p>Al mismo tiempo, Google hab\u00eda publicado los documentos que describ\u00edan su novedoso sistema de archivos distribuidos, el Google File System (GFS), y MapReduce, un framework de computaci\u00f3n para procesamiento paralelo. La exitosa implementaci\u00f3n de estos conceptos en Nutch result\u00f3 en dos proyectos separados, el segundo se convirti\u00f3 en Hadoop, un proyecto Apache de primera clase.</p> <p>Hadoop</p> <p>El nombre de Hadoop no es ning\u00fan acr\u00f3nimo, sino un nombre inventado. Su creador se lo puso por un elefante amarillo de peluche que ten\u00eda su hijo. Pens\u00f3 que un nombre corto, relativamente f\u00e1cil de deletrear y pronunciar ser\u00eda adecuado</p> <p>Si Big Data es la filosof\u00eda de trabajo para grandes vol\u00famenes de datos, Apache Hadoop es la tecnolog\u00eda catalizadora. Hadoop puede escalar hasta miles de ordenadores creando un cl\u00faster con un almacenamiento del orden de petabytes de informaci\u00f3n.</p> <p>\u00bfQu\u00e9 es Hadoop?</p> <p>Apache Hadoop es una plataforma opensource que ofrece la capacidad de almacenar y procesar, a \"bajo\" coste, grandes vol\u00famenes de datos, sin importar su estructura, en un entorno distribuido, escalable y tolerante a fallos, basado en la utilizaci\u00f3n de hardware commodity y en un paradigma del procesamiento a los datos. </p> <p>Hadoop es una plataforma, lo que significa que es la base sobre la que construir aplicaciones. Se podr\u00eda hacer el s\u00edmil de Hadoop como una caja de herramientas que proporciona un conjunto de herramientas con las que construir una gran variedad de aplicaciones que requieran almacenar y procesar grandes vol\u00famenes de datos. La selecci\u00f3n de qu\u00e9 herramienta utilizar para cada aplicaci\u00f3n la realizaremos en funci\u00f3n de las necesidades de cada caso de uso.</p> <p>Otras soluciones, como MongoDB u otras bases de datos NoSQL no se consideran plataformas, ya que tienen un \u00fanico prop\u00f3sito y ofrecen un tipo de funcionalidad.</p> <p>Hadoop aglutina una serie de herramientas para el procesamiento distribuido de grandes conjuntos de datos a trav\u00e9s de cl\u00fasters de ordenadores utilizando modelos de programaci\u00f3n sencillos.</p> <p>Sus caracter\u00edsticas son:</p> <ul> <li>Confiable: Crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento.</li> <li>Tolerante a fallos: Tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. Cuando un componente se recupera, vuelve a formar parte del cl\u00faster. En Hadoop los fallos de hardware se tratan como una regla, no como una excepci\u00f3n.</li> <li>Heterogeneo: Los datos que pueden almacenarse y procesarse en Hadoop pueden ser de cualquier tipo: estructurados, semiestructurados o datos no estructurados.</li> </ul> Figura6.2_Hadoop: Datos heterog\u00e9neos <ul> <li>Portable: Se puede instalar en todo tipos de hardware y sistemas operativos.</li> <li>Escalable: Los datos y su procesamiento se distribuyen sobre un cl\u00faster de ordenadores (escalado horizontal), desde un \u00fanico servidor a miles de m\u00e1quinas, cada uno ofreciendo computaci\u00f3n y almacenamiento local.</li> </ul> Figura6.3_Hadoop: Escalable <ul> <li>Distribuido: Hadoop se basa en una infraestructura que tiene muchos servidores (tambi\u00e9n llamados nodos) que trabajan conjuntamente para almacenar y para procesar los datos, a diferencia de los sistemas centralizados, donde todo se realiza en un \u00fanico servidor.</li> </ul>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#61-componentes-y-ecosistema","title":"6.1 Componentes y Ecosistema","text":"Figura6.4_Ecosistema Hadoop <p>El n\u00facleo de Hadoop se compone de:</p> <ul> <li>Hadoop Common: un conjunto de utilidades comunes</li> <li>HDFS (Hadoop Distributed File System): un sistema de ficheros distribuidos (capa de almacenamiento) que almacena los datos en una estructura basada en espacios de nombres (directorios, subdirectorios, etc)</li> <li>YARN: un gestor de recursos (capa de procesamiento) para el manejo del cl\u00faster y la planificaci\u00f3n de procesos, que permite ejecutar aplicaciones sobre los datos almacenados en HDFS</li> <li>MapReduce: un sistema para procesamiento paralelo de grandes conjuntos de datos, con aplicaciones que lo utilizan de forma transparente.</li> </ul> <p>Note</p> <p>Sin embargo, normalmente se identifica el nombre Hadoop con todo el ecosistema de componentes independientes que suelen incluirse para dotar a Hadoop de funcionalidades necesarias en proyectos Big Data empresariales, como puede ser la ingesta de informaci\u00f3n, el acceso a datos con lenguajes est\u00e1ndar, o las capacidades de administraci\u00f3n y monitorizaci\u00f3n. Estos componentes suelen ser proyectos opensource de Apache.. </p> <p>Estos elementos permiten trabajar casi de la misma forma que si tuvi\u00e9ramos un sistema de fichero local en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores.</p> <p>Las aplicaciones se desarrollan a alto nivel, sin tener constancia de las caracter\u00edsticas de la red. De esta manera, los cient\u00edficos de datos se centran en la anal\u00edtica y no en la programaci\u00f3n distribuida.</p> <p>Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop.</p> <ul> <li>Accumulo: Base de datos NoSQL que ofrece funcionalidades de acceso aleatorio y at\u00f3mico.</li> <li>Ambari: Herramienta utilizada para instalar, configurar, mantener y monitorizar Hadoop.</li> <li>Atlas: Herramienta de gobierno de datos de Hadoop.</li> <li>Phoenix: Capa que permite acceder a los datos de HBase mediante interfaz SQL.</li> <li>Flume: Servicio distribuido y altamente eficiente para distribuir, agregar y recolectar grandes cantidades de informaci\u00f3n procedentes de sistemas real-time en Hadoop. Es \u00fatil para cargar y mover informaci\u00f3n como ficheros de logs, datos de Twitter/Reddit, etc. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables</li> <li>HBase: Es el sistema de almacenamiento NoSQL basado en columnas para Hadoop.<ul> <li>Es una base de datos de c\u00f3digo abierto, distribuida y escalable para el almacenamiento de Big Data.</li> <li>Escrita en Java, implementa y proporciona capacidades similares sobre Hadoop y HDFS.</li> <li>El objetivo de este proyecto es el de trabajar con grandes tablas, de miles de millones de filas y columnas, sobre un cl\u00faster Hadoop.</li> </ul> </li> <li>Hive: Permite acceder a ficheros de datos estructurados o semiestructurados que est\u00e1n en HDFS como si fueran una tabla de una base de datos relacional, utilizando un lenguaje similar a SQL (HiveSQL). Simplifica enormemente el desarrollo y la gesti\u00f3n con Hadoop.</li> <li>Impala: Herramienta con funcionalidad similar a Hive (tratamiento de los datos de HDFS mediante SQL) pero con un rendimiento elevado (tiempos de respuesta menores).</li> <li>Kafka: Sistema de mensajer\u00eda que permite recoger eventos en tiempo real as\u00ed como su procesamiento.</li> <li>Mahout: Conjunto de librer\u00edas para desarrollo y ejecuci\u00f3n de modelos de machine learning utilizando las capacidades de computaci\u00f3n de Hadoop.</li> <li>Oozie: Herramienta que permite definir flujos de trabajo en Hadoop as\u00ed como su orquestaci\u00f3n y planificaci\u00f3n.</li> <li>Pig: Lenguaje de alto de nivel para analizar grandes vol\u00famenes de datos. Trabaja en paralelo, lo que permite gestionar gran cantidad de informaci\u00f3n. Realmente es un compilador que genera comandos MapReduce, mediante el lenguaje textual denominado Pig Latin.</li> <li>Spark: Aunque habitualmente no se asocia al ecosistema Hadoop, Apache Spark ha sido el mejor complemento de Hadoop en los \u00faltimos a\u00f1os. Apache Spark es un motor de procesamiento masivo de datos muy eficiente a gran escala que ofrece funcionalidades para ingenier\u00eda de datos, machine learning, grafos, etc. Implementa procesamiento en tiempo real al contrario que MapReduce, lo que provoca que sea m\u00e1s r\u00e1pido. Para ello, en vez de almacenar los datos en disco, trabaja de forma masiva en memoria. Puede trabajar de forma aut\u00f3noma, sin necesidad de Hadoop.</li> <li>Sqoop: Componente para importar o exportar datos estructurados desde bases de datos relacionales a Hadoop y viceversa.</li> <li>Storm: Sistema de procesamiento real-time de eventos con baja latencia.</li> <li>Zeppelin: Aplicaci\u00f3n web de notebooks que permite a los Data Scientists realizar an\u00e1lisis y evaluar c\u00f3digo de forma sencilla, as\u00ed como la colaboraci\u00f3n entre equipos.</li> <li>ZooKeeper: Herramienta t\u00e9cnica que permite sincronizar el estado de los diferentes servicios distribuidos de Hadoop.</li> </ul>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#62-distribuciones-hadoop","title":"6.2 Distribuciones Hadoop","text":"<p>No te preocupes si ves muchos componentes y piensas que es imposible dominar todos. En la realidad, los proyectos suelen utilizar s\u00f3lo una peque\u00f1a parte de los componentes dependiendo de las necesidades. En negrita encuentras los m\u00e1s utilizados, adem\u00e1s de los componentes core: HDFS y YARN.</p> <p>Cada componente es un proyecto Apache independiente, lo que impacta, entre otros a:</p> <ul> <li>Pol\u00edtica de versionado (periodicidad, identificaci\u00f3n, \u2026): cada componente sigue su propio camino en cuanto a cu\u00e1ndo se publican las nuevas versiones, qu\u00e9 mejoras o evoluciones incluyen, etc.</li> <li>Dependencias del proyecto con otras versiones de componentes del ecosistema y librer\u00edas externas: los componentes suelen tener dependencias entre ellos. Por ejemplo, Hive tiene dependencia de HDFS, o Phoenix de HBase. Las dependencias suelen ser dif\u00edciles de gestionar, por ejemplo, porque una versi\u00f3n de Phoenix requiere una versi\u00f3n espec\u00edfica de HBase.</li> <li>Roadmap y estrategia del proyecto: al tener grupos de trabajo diferentes, cada proyecto tiene su propia estrategia en cuanto a c\u00f3mo evolucionar la soluci\u00f3n, cu\u00e1ndo adaptarse a cambios externos, etc. y no siempre est\u00e1n alineados.</li> <li>Commiters / desarrolladores: los desarrolladores de cada proyecto son diferentes.</li> </ul> <p>Por este motivo, realizar una instalaci\u00f3n de toda una plataforma Hadoop con sus componentes asociados de forma independiente (lo que se denomina Hadoop Vanila) resulta muy complicado. Por ejemplo, al instalar la versi\u00f3n X de Phoenix necesitas la versi\u00f3n Y de HBase, pero otro componente (Hive, por ejemplo), requiere la versi\u00f3n Z de HBase.</p> <p>La misma dificultad ocurre para la resoluci\u00f3n de incidencias que puedan ocurrir en la plataforma cuando se ejecuta en producci\u00f3n.</p> <p>Para solventar las dificultades mencionadas, surgen las distribuciones comerciales de Hadoop, que contienen en un \u00fanico paquete la mayor parte de componentes del ecosistema, resolviendo dependencias, a\u00f1adiendo incluso utilidades, e incorporando la posibilidad de contratar soporte empresarial 24x7</p> <p>Distribuciones: Cloudera</p> <p>Note</p> <p>Cloudera es la principal distribuci\u00f3n que existe actualmente (hubo otras, como MAPR y HortonWorks, pero desaparece MAPR y Hortonworks se une a Cloudera)</p> <p>Utiliza la mayor parte de componentes de Apache, en alg\u00fan caso realizando algunas modificaciones, y a\u00f1ade alg\u00fan componente propietario (Cloudera Manager, Cloudera Navigator, etc.). CDN Cloudera y como descargarlo</p> Distribuci\u00f3n Cloudera. Fuente: Cloudera <p>CDH Cloudera</p> <p>CDH (Cloudera\u2019s Distribution including Apache Hadoop) es la distribuci\u00f3n de Cloudera con Apache Hadoop orientada a empresas. La \u00faltima versi\u00f3n es Cloudera 6 (CDH 6). Est\u00e1 disponible como paquetes RPM y paquetes para Debian, Ubuntu o Suse. Cloudera proporciona CDH en varias modalidades.</p> <p>La versi\u00f3n m\u00e1s completa y empresarial es Cloudera Enterprise, que incluye suscripciones por cada nodo del cl\u00faster, Cloudera Manager y el soporte t\u00e9cnico. Por otro lado, Cloudera Express es una versi\u00f3n m\u00e1s sencilla, sin actualizaciones o herramientas de disaster recovery. Por \u00faltimo, existe una versi\u00f3n gratuita de CDH: Cloudera Community. Permite desplegar un cl\u00faster con un n\u00famero de nodos limitado.</p> <p>Es posible ejecutar Cloudera desde un contenedor Docker. Proporciona una imagen Docker con CDH y Cloudera Manager que sirve como entorno para aprender Hadoop y su ecosistema de una forma sencilla y sin necesidad de Hardware potente. Tambi\u00e9n es \u00fatil para desarrollar aplicaciones o probar sus funcionalidades.</p> <p>Soluciones como servicio (SaaS)</p> <ul> <li>Amazon Elastic Map Reduce (EMR)<ul> <li>Sitio Oficial</li> <li>\u00bfQu\u00e9 es Amazon EMR?</li> </ul> </li> <li>Microsoft Azure HDInsight<ul> <li>Sitio Oficial</li> <li>\u00bfQu\u00e9 es Azure HDInsight?</li> </ul> </li> <li>Google Dataproc<ul> <li>Sitio Oficial</li> <li>\u00bfQu\u00e9 es Dataproc?</li> </ul> </li> </ul> <p>Principales Servicios de Datos en Cloud</p> <p>Para comprender mejor los servicios que ofrecen los proveedores cloud, podemos clasificarlos en varios grupos en funci\u00f3n de sus caracter\u00edsticas:</p> <ul> <li>Ingesta</li> <li>Procesamiento</li> <li>Almacenamiento</li> <li>An\u00e1lisis</li> </ul> <p>En la siguiente tabla se muestran estos grupos de servicios, con la categor\u00eda a la que pertenecen, las alternativas en los principales proveedores de servicios cloud (HDInsight, AWS y GCP) y su opci\u00f3n Open Source.</p> GRUPO CATEGOR\u00cdA AZURE AWS GCP OPEN SOURCE Ingesta ETL Data Factory Glue Dataprep Apache NiFi Ingesta Message Queue Streaming EventHub Kinesis Pub/Sub Ingesta Scheduling Logic Apps, Batch Stepfunctions, Cloudwatch, EventBridge Cloud Scheduler, Cloud Batch, Cloud Workflows, Cloud Composer Apache Airflow Procesamiento Procesamiento Functions, Data Factory, Databricks Lambda, EMR, Glue, Databricks Cloud Functions, DataProc, DataFlow Apache Flink, Apache Spark, Apache Hadoop Almacenamiento Relacional Azure SQL DB RDS CloudSQL, BigTable MySQL, MariaDB, PostgreSQL Almacenamiento NoSQL CosmosDB DynamoDB, DocumentDB Datastore, Firestore Elasticsearch, Apache Cassandra, MongoDB Almacenamiento OLAP, Data Warehouse Synapse Analytics, Snowflake Redshift, Athena, Snowflake BigQuery, Snowflake Apache Druid, Apache Hive, Presto Almacenamiento Objetos Blob Storage S3 Cloud Storage Minio, Ceph Almacenamiento Cache Cache for Redis ElastiCache Memorystore Memcached, Redis Almacenamiento Grafo CosmosDB Neptune Vertex AI Neo4j An\u00e1lisis Machine Learning Azure ML SageMaker Cloud datalab, Vertex AI Tensorflow, Keras, PyTorch An\u00e1lisis Business Intelligence PowerBI Quicksight Data Studio, Looker Grafana, Apache Superset"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#63-arquitectura","title":"6.3 Arquitectura","text":"<p>Hadoop se basa en un modelo de despliegue distribuido y est\u00e1 dise\u00f1ado para ejecutar sistemas de procesamiento en el mismo cl\u00faster que almacena los datos (data local computing). </p> <p>Pese a que hay un conjunto de servidores trabajando en paralelo y de forma conjunta, para un usuario externo todos ellos act\u00faan como si fuera una sola m\u00e1quina, es decir, un usuario del sistema de ficheros (HDFS) ver\u00e1 la estructura de directorios, subdirectorios y ficheros, pero no tendr\u00e1 que conocer en qu\u00e9 servidores est\u00e1 cada fichero (lo mismo ocurre con cualquier otro componente que se ejecuta en toda la infraestructura)</p> <p>Su filosof\u00eda es almacenar todos los datos en un lugar y procesarlos en el mismo lugar, esto es, mover el procesamiento al almac\u00e9n de datos y no mover los datos al sistema de procesamiento.</p> <p>Note</p> <p>Cluster y Nodo: Al conjunto de servidores que trabajan para implementar las funcionalidades de Apache Hadoop se le denomina cl\u00faster, y a cada uno de los servidores que forman parte del cl\u00faster se le denomina nodo. A partir de ahora, cuando usemos la palabra \"cl\u00faster de Hadoop\" debes pensar en el conjunto de servidores que forman la plataforma que est\u00e1 en ejecuci\u00f3n, y cuando usemos la palabra \"nodo\" debes pensar en cada uno de los servidores que componen el cl\u00faster.</p> <p>Esto lo logra mediante un entorno distribuido de datos y procesos. El procesamiento se realiza en paralelo a trav\u00e9s de nodos de datos en un sistema de ficheros distribuidos (HDFS), donde se distingue entre:</p> <ul> <li>Nodos worker: Realizan los trabajos. Tratan con los datos locales y los procesos de aplicaci\u00f3n. Por ejemplo, para el almacenamiento, cada worker se ocupar\u00e1 de almacenar una parte, mientras que para la ejecuci\u00f3n de trabajos, cada worker realiza una parte del trabajo. Su n\u00famero depender\u00e1 de las necesidad de nuestros sistemas, pero pueden estar comprendido entre 4 y 10.000. Su hardware es relativamente barato (commodity hardware) mediante servidores X86.</li> <li>Nodos master: Encargados de los procesos de gesti\u00f3n global, es decir, controlar la ejecuci\u00f3n o el almacenamiento de los trabajos y/o datos. Son los nodos que controlan el trabajo que realizan los nodos worker, por ejemplo, asignando a cada worker una parte del proceso o de los datos a almacenar, vigilando que est\u00e1n realizando el trabajo y no est\u00e1n ca\u00eddos, rebalanceando el trabajo a otros nodos en caso de que un worker tenga problemas, etc. Normalmente se necesitan 3.</li> <li>Nodos edge: Hacen de puente entre el cl\u00faster y la red exterior y proporcionan interfaces, ya que normalmente un cl\u00faster Hadoop no tiene conexi\u00f3n con el resto de servidores e infraestructura de la empresa, por lo que toda la comunicaci\u00f3n desde el exterior hacia el cl\u00faster se canaliza a trav\u00e9s de los nodos frontera, que adem\u00e1s, ofrecen las APIs para poder invocar a servicios del cl\u00faster.</li> </ul> Arquitectura Hadoop <p>El hardware t\u00edpico donde se ejecuta un cluster Hadoop ser\u00eda:</p> <p>Note</p> <p>Commodity Hardware: A veces el concepto hardware commodity suele confundirse con hardware dom\u00e9stico, cuando lo que hace referencia es a hardware no espec\u00edfico, que no tiene unos requerimientos en cuanto a disponibilidad o resiliencia exigentes</p> <ul> <li>Nodos master: 12 HDs x 2-3 TB JBOD (Just a bunch of disks = s\u00f3lo un mont\u00f3n de discos) - 2CPUs x 8 cores - 256 GB RAM</li> <li>Nodos worker: 2 HDs x 2-3 TB RAID - 2CPUs x 8 cores - 256 GB RAM</li> <li>Nodos edge: 2 HDs x 2-3 TB RAID - 2CPUs x 8 cores - 256 GB RAM</li> </ul>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#64-uso-de-hadoop","title":"6.4 Uso de Hadoop","text":"<p>Es importante analizar y tener en cuenta en que casos reales es aconsejable el uso de Hadoop y cuando no lo es.</p>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#cuando-usar-hadoop","title":"\u00bfCu\u00e1ndo Usar Hadoop?","text":"<ul> <li>Cuando el volumen de datos es mayor que la capacidad de los sistemas tradicionales (no cabe en una m\u00e1quina).</li> <li>Cuando hay un problema de variedad de datos, porque son diversos o porque cambian frecuentemente.</li> <li>Cuando se requiere una escalabilidad que no pueden ofrecer los sistemas tradicionales, por volumen, por velocidad de proceso, por rendimiento global, y no se requiere un nivel de transaccionalidad elevado.</li> <li>Cuando se pretende tener una plataforma con la capacidad de almacenamiento y procesamiento de un gran volumen de datos para cubrir diferentes casos de uso (con la misma plataforma).</li> </ul>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#cuando-no-usar-hadoop","title":"\u00bfCu\u00e1ndo no usar Hadoop?","text":"<ul> <li>Cuando los sistemas tradicionales son capaces de dar soporte a los casos de uso y cuando los formatos/tipos de datos son fijos o no cambian apenas.</li> <li>Cuando se tiene requisitos de transaccionalidad muy estrictos, es decir, cuando se pretende cubrir la operativa de una empresa (por ejemplo, en un banco: las transferencias, movimientos, pagos, etc.).</li> <li>Cuando s\u00f3lo se requiere resolver un caso de uso \"Big Data\" muy espec\u00edfico.</li> </ul>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#65-hdfs","title":"6.5 HDFS","text":"<p>Hadoop Distributed File System (HDFS) Es el componente principal del ecosistema Hadoop. Hace posible almacenar conjuntos de datos masivos con tipos de datos estructurados, semi-estructurados y no estructurados como im\u00e1genes, v\u00eddeo, datos de sensores, etc.</p> <p>Es un sistema de almacenamiento distribuido y tolerante a fallos que puede almacenar gran cantidad de datos, escalar de forma incremental y sobrevivir a fallos de hardware sin perder datos. Se basa en el paper que public\u00f3 Google detallando su Google File System en 2003.</p> <p>Est\u00e1 optimizado para obtener un alto rendimiento y trabajar con m\u00e1xima eficiencia cuando se leen archivos grandes. Para obtener este rendimiento, utiliza tama\u00f1os de bloque inusualmente grandes y optimizaci\u00f3n de localizaci\u00f3n de los datos para reducir la E/S de red.</p> <p>Con el fin de ofrecer una visi\u00f3n de los recursos como una sola unidad crea una capa de abstracci\u00f3n como un sistema de ficheros \u00fanico. Est\u00e1 basado en la idea de que mover el procesamiento es mucho m\u00e1s r\u00e1pido, f\u00e1cil y eficiente que mover grandes cantidades de datos, que pueden producir altas latencias y congesti\u00f3n en la red.</p>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#651-caracteristicas-hdfs","title":"6.5.1 Caracter\u00edsticas HDFS","text":"<ul> <li>Es un sistema de ficheros distribuido, es decir, se ejecuta sobre diferentes nodos que trabajan en conjunto ofreciendo a los usuarios y aplicaciones que utilizan el sistema, un interfaz como si s\u00f3lo hubiera un \u00fanico servidor por detr\u00e1s.</li> <li>Est\u00e1 dise\u00f1ado para ejecutarse sobre hardware commodity, es decir, no requiere unos servidores espec\u00edficos o costosos. Esto conlleva la necesidad de poder sobreponerse a los fallos que pudieran tener los servidores o algunas partes de los servidores.</li> <li>Est\u00e1 optimizado para almacenar ficheros de gran tama\u00f1o y para hacer operaciones de lectura o escritura masivas. Su objetivo es cubrir los casos de uso de anal\u00edtica masiva, no los casos de uso que dan soporte a las operaciones de las empresas.</li> <li>Tiene capacidad para escalar horizontalmente hasta vol\u00famenes de Petabytes y miles de nodos, y est\u00e1 dise\u00f1ado para poder dar soporte a m\u00faltiples clientes con acceso concurrente. La escalabilidad se consigue a\u00f1adiendo m\u00e1s servidores</li> <li>No establece ninguna restricci\u00f3n sobre los tipos de datos que se almacenan en el sistema, ya que \u00e9stos pueden ser estructurados, semiestructurados o no disponer de ninguna estructura, como el caso de im\u00e1genes o v\u00eddeos.</li> <li>HDFS tiene una orientaci\u00f3n \"write-once, read many\", que significa \"se escribe una vez, se lee muchas veces\", es decir, asume que un archivo una vez escrito en HDFS no se modificar\u00e1, aunque se puede acceder a \u00e9l muchas veces. As\u00ed pues, los datos, una vez escritos en HDFS son immutables. Cada fichero de HDFS solo permite a\u00f1adir contenido (append-only). Una vez se ha creado y escrito en \u00e9l, solo podemos a\u00f1adir contenido o eliminarlo. Es decir, a priori, no podemos modificar los datos.</li> </ul> <p>Recuerda las caracter\u00edsticas con esta imagen</p> Figura6.7_Caracter\u00edsticas HDFS"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#652-bloques","title":"6.5.2 Bloques","text":"<p>Un bloque es la cantidad m\u00ednima de datos que puede ser le\u00edda o escrita. En HDFS, los ficheros se dividen en bloques, como en la mayor\u00eda de sistemas de ficheros. Sin embargo, el tama\u00f1o de un bloque en HDFS es muy grande, de 128 megabytes por defecto. En el sistema operativo de un PC (Windows, Linux, etc.), el tama\u00f1o suele ser de 512 bytes o 4 kilobytes, es decir, unas 50.000 veces m\u00e1s peque\u00f1o que en HDFS.</p> <p>El bloque es la unidad m\u00ednima de lectura, lo que significa que aunque tengamos un fichero que ocupa 1 kilobyte, tendremos que leer o escribir 128 megabytes cada vez que queramos operar con el fichero. Para ficheros grandes, por ejemplo, de 500 gigabytes, la ventaja que aporta es que hay que buscar y leer o escribir muchos menos bloques. Esta caracter\u00edstica explica por qu\u00e9 Hadoop est\u00e1 dise\u00f1ado para ficheros grandes y lecturas masivas, y por qu\u00e9 tiene un mal rendimiento para operaciones peque\u00f1as.</p> <p>Por lo tanto, cuando queremos escribir un fichero en HDFS, lo primero que se hace es dividir el fichero en bloques. A continuaci\u00f3n, los bloques son almacenados en diferentes nodos, no siendo necesario que los bloques de un mismo fichero est\u00e9n en un mismo nodo. Adem\u00e1s, un aspecto importante es que cada bloque se replica (se copia) en m\u00e1s de un nodo, lo que se conoce como el factor de replicaci\u00f3n. El factor de replicaci\u00f3n por defecto en HDFS es 3, lo que significa que cada bloque tiene 3 copias almacenadas en 3 nodos diferentes. La replicaci\u00f3n es el mecanismo con el que se consigue, entre otras cosas, la tolerancia a fallos.</p> <p>Al tener varias r\u00e9plicas de cada bloque en diferentes nodos, en caso de que un nodo se caiga, o que un disco de un nodo se corrompa, HDFS dispondr\u00e1 de otras copias, por lo que no se perder\u00e1n los datos. </p> Figura6.8_Bloques HDFS <p>En el ejemplo anterior, si se cayera el nodo 3, HDFS dispondr\u00eda de otras dos copias por cada bloque que almacena del fichero.</p> Figura6.9_Factor Replicaci\u00f3n HDFS <p>El factor de replicaci\u00f3n puede configurarse a nivel de fichero o directorio, es decir, podemos elegir un factor de replicaci\u00f3n diferente para los ficheros o directorios que consideremos. Cuanto mayor sea el factor de replicaci\u00f3n, m\u00e1s dif\u00edcil ser\u00e1 que perdamos los datos e incluso mejorar\u00e1 el rendimiento en las lecturas, porque para leer un bloque, HDFS podr\u00e1 utilizar cualquier nodo. Sin embargo, un factor de replicaci\u00f3n alto hace que las escrituras tengan peor rendimiento, al tener que hacer muchas copias en cada escritura, y adem\u00e1s, consumir\u00e1 m\u00e1s espacio real en disco.</p>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#653-arquitectura-hdfs","title":"6.5.3 Arquitectura HDFS","text":"<p>La arquitectura de HDFS consta de distintos servicios y tipos de nodo, aunque fundamentalmente son tres tipos:</p> <ul> <li>NameNode(NN): Nodo de Nombres.</li> <li>Secondary NameNode(SNN): Nodo de Nombres Secundario.</li> <li>DataNode(DN): Nodos de Datos</li> </ul> <p>NameNode</p> <p>El nodo Namenode act\u00faa de maestro, manteniendo la metainformaci\u00f3n de todo el sistema de ficheros, esto es:</p> <ul> <li>Almacena el espacio de nombres HDFS</li> <li>La estructura de directorios, subdirectorios y los ficheros</li> <li>La informaci\u00f3n de los ficheros: tama\u00f1o, fecha de modificaci\u00f3n, propietario, permisos, etc.</li> <li>El factor de replicaci\u00f3n de cada fichero.</li> <li>Los bloques que componen cada fichero.</li> <li>La ubicaci\u00f3n de los distintos bloques (en qu\u00e9 nodo se encuentran).</li> </ul> <p>Note</p> <p>La informaci\u00f3n es almacenada tanto en disco, para garantizar la durabilidad en caso de una ca\u00edda del servidor, como en memoria, para poder acceder a la informaci\u00f3n lo m\u00e1s r\u00e1pido posible y optimizar el rendimiento.</p> <p>NameNode se compone principalmente de dos ficheros:</p> <ul> <li>FsImage: Contiene la estructura de directorio completa (espacio de nombres) de HDFS con detalles sobre la ubicaci\u00f3n de los datos en los bloques de datos y qu\u00e9 bloques est\u00e1n almacenados en qu\u00e9 nodo. NameNode utiliza este archivo cuando se inicia.</li> <li>EditLog: Es un registro de transacciones que registra los cambios en el sistema de archivos HDFS o cualquier acci\u00f3n realizada en el cl\u00faster HDFS, como la adici\u00f3n de un nuevo bloque, la replicaci\u00f3n, la eliminaci\u00f3n, etc. En resumen, registra los cambios desde que se cre\u00f3 la \u00faltima FsImage</li> </ul> <p>Cuando se inicia un NameNode, lee el estado HDFS de un archivo de imagen, fsimage, y luego aplica las ediciones del archivo de registro de ediciones. Seguidamente escribe un nuevo estado HDFS en el fsImage y comienza la operaci\u00f3n normal con un archivo de edici\u00f3n vac\u00edo.</p> <p>Example</p> <p>Por ejemplo, la creaci\u00f3n de un nuevo archivo en HDFS hace que NameNode inserte un registro en EditLog para indicarlo. De manera similar, cambiar el factor de replicaci\u00f3n de un archivo hace que se inserte un nuevo registro en EditLog. El NameNode utiliza un archivo en su sistema de archivos del sistema operativo anfitri\u00f3n local para almacenar el EditLog. Todo el espacio de nombres del sistema de archivos, incluida la asignaci\u00f3n de bloques a archivos y las propiedades del sistema de archivos, se almacena en un archivo denominado FsImage. La FsImage tambi\u00e9n se almacena como un archivo en el sistema de archivos local de NameNode.</p> <p>Adem\u00e1s de gestionar la metainformaci\u00f3n, coordina todas las lecturas y escrituras, y controla el funcionamiento de los Datanodes, es decir, detecta si hay alg\u00fan fallo en alg\u00fan nodo y toma las acciones necesarias en caso de que alguno est\u00e9 ca\u00eddo o con fallos.</p> <p>Es importante que el Namenode sea robusto y no tenga ca\u00eddas. Por este motivo, se utiliza hardware m\u00e1s resiliente que en el caso de los Datanodes.</p> <p>Secondary NameNode</p> <p>Para mejorar la tolerancia a fallos, suele existir un nodo secundario del maestro, denominado Secondary Namenode.</p> <p>El NameNode es el \u00fanico punto de fallo en HDFS ya que, si el Namenode falla, se pierde todo el sistema de archivos HDFS. Para reducir este riesgo esto, Hadoop implement\u00f3 el Secondary Namenode en la versi\u00f3n 3.</p> <p>Secondary Namenode no es un nodo de respaldo. Su principal funci\u00f3n es almacenar una copia de los ficheros fsimage y editlog. Comprueba los metadatos del sistema de archivos almacenados en NameNode. Esto es lo que se llama checkpointing. El proceso que sigue el NameNode secundario para fusionar peri\u00f3dicamente los archivos fsimage y edits log es el siguiente:</p> <ol> <li>El NameNode secundario obtiene los \u00faltimos archivos fsImage y editLog del NameNode primario.</li> <li>El NameNode secundario aplica cada transacci\u00f3n del archivo editLog a fsImage para crear un nuevo archivo FsImage fusionado.</li> <li>El archivo fsImage fusionado se transfiere de nuevo al NameNode primario.</li> </ol> Figura6.10_Relaci\u00f3n entre NameNode y DataNode <p>DataNode</p> <p>Los Datanodes son los servicios que se encuentran en los nodos worker, y su labor principal es almacenar o leer los bloques que componen los ficheros que est\u00e1n almacenados en HDFS, con las siguientes particularidades:</p> <ul> <li>Habr\u00e1 m\u00e1s de uno en cada cl\u00faster. Por cada Namenode podemos tener miles de Datanodes.</li> <li>Almacena y lee bloques de datos. El Datanode s\u00f3lo conoce los bloques que contiene, pero no sabe a qu\u00e9 fichero pertenecen o d\u00f3nde se encuentran el resto de bloques del fichero. Toda esta informaci\u00f3n s\u00f3lo est\u00e1 en el Namenode. Por eso es cr\u00edtico para HDFS.</li> <li>Env\u00edan al Namenode la lista de los bloques que almacenan, para que el Namenode pueda tener una lista actualizada de los bloques y su ubicaci\u00f3n.</li> <li>Almacenan un checksum por cada bloque para detectar si el bloque est\u00e1 corrupto y garantizar su integridad.</li> <li>Env\u00eda un latido (heartbeat) al Namenode, que es un mensaje corto indicando que est\u00e1 levantado</li> </ul> <p>Resumen</p> Figura6.11_Resumen Nodos HDFS"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#654-funcionamiento-lectura-y-escritura","title":"6.5.4 Funcionamiento (Lectura y Escritura)","text":"<p>Los datos que se escriben en HDFS son immutables, es decir, no pueden ser modificados.</p> <p>Esto significa que HDFS s\u00f3lo permite a\u00f1adir contenido a los ficheros, as\u00ed que por ejemplo, si en un fichero de 256 megabytes se pretende modificar un car\u00e1cter, HDFS crear\u00e1 un nuevo bloque con el cambio y lo escribir\u00e1 por completo, borrando el bloque anterior. </p> <p>Esto, junto con la caracter\u00edstica del tama\u00f1o de bloque de 128 megabytes, que es la unidad m\u00ednima de lectura, hace que el rendimiento de HDFS para operaciones sencillas sobre registros aleatorios sea muy pobre. Recuerda que HDFS est\u00e1 pensado para ficheros grandes y lecturas masivas. </p> <p>HDFS proporciona dos tipos de operaciones b\u00e1sicas con los ficheros: leer y escribir un fichero</p> <p>Lectura</p> <p>En el caso de las lecturas, un esquema simplificado de esta operaci\u00f3n ser\u00eda:</p> Figura6.12_Lectura HDFS <ol> <li>El cliente que desea leer un fichero de HDFS, mediante una librer\u00eda instalada en su equipo, realiza una llamada al Namenode para conocer qu\u00e9 bloques forman un fichero (llamemos X al fichero), as\u00ed como los Datanodes que contienen cada uno de los bloques.</li> <li>El Namenode retorna dicha informaci\u00f3n, y ordena para cada bloque los Datanodes que contienen dicho bloque en funci\u00f3n de la distancia al cliente (un algoritmo eval\u00faa la distancia entre el cliente y cada Datanode). El objetivo de esta lista ordenada es intentar reducir el tiempo de acceso a cada Datanode desde el cliente.</li> <li>Con la informaci\u00f3n recibida del Namenode, el cliente se comunica directamente con el Datanode 1 para solicitarle el primer bloque.</li> <li>El cliente se comunica con el Datanode 2 para obtener el bloque 2.</li> <li>El cliente se comunica con el Datanode 1 para obtener el bloque 3. </li> </ol> <p>Info</p> <p>Es preciso indicar que durante la operaci\u00f3n, la \u00fanica responsabilidad del Namenode es devolver al cliente la lista de bloques y la ubicaci\u00f3n de los mismos, pero no interviene en las lecturas. Es decir, para realizar las lecturas de cada bloque, el cliente se comunica directamente con los Datanodes, sin que los datos pasen por el Namenode. Esto hace que el Namenode no sea cuello de botella del proceso, y pueda atender m\u00faltiples peticiones en paralelo, ya que no le supone mucho esfuerzo de computaci\u00f3n atender las diferentes solicitudes de los clientes.</p> <p>Escritura</p> <p>En el caso de las escrituras, un esquema simplificado de esta operaci\u00f3n ser\u00eda:</p> Figura6.13_Escritura HDFS <ol> <li>El cliente, que desea escribir un fichero, invoca a un servicio del Namenode para solicitar la creaci\u00f3n del fichero, indic\u00e1ndole en la llamada el nombre y la ruta en la que desea guardarlo.</li> <li>El Namenode realiza una serie de verificaciones, como los permisos del usuario/cliente en el directorio, si el fichero ya existe, etc. En caso de que todas las verificaciones sean correctas, devuelve un OK, en caso contrario un KO.</li> <li>El cliente comienza a generar los bloques en los que se dividir\u00e1 el fichero utilizando una librer\u00eda de HDFS.</li> <li>Para cada bloque que desea escribir el cliente, se invoca al Namenode para obtener el Datanode en el que escribir el bloque.</li> <li>El Namenode devuelve la lista de Datanodes en los que escribir el bloque, y el cliente escribe dicho bloque en el primer Datanode obtenido, realizando una comunicaci\u00f3n directamente con dicho Datanode.</li> <li>Una vez escrito el bloque en el primer Datanode, \u00e9ste es responsable de comunicarse con el siguiente Datanode en la cadena para que escriba una copia del bloque. Una vez todos los Datanodes han escrito la r\u00e9plica, se devuelve un \"Ok\" al cliente para que escriba el siguiente bloque.</li> </ol> <p>Info</p> <p>Al igual que en el caso de la lectura, es importante se\u00f1alar que el Namenode no recibe en ning\u00fan momento los datos del fichero, sino que se limita a resolver las cuestiones relacionadas con la ubicaci\u00f3n de cada bloque. De esta manera, liberando al Namenode de la operativa de escritura, permite optimizar el funcionamiento y que el Namenode no se convierta en el cuello de botella de HDFS en las escrituras de fichero.</p>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#655-factor-de-replicacion","title":"6.5.5 Factor de replicaci\u00f3n","text":"<p>Como sabemos, la replicaci\u00f3n es un concepto muy importante en HDFS, ya que nos permite tener una mayor tolerancia a fallos, pero tiene otras implicaciones en cuanto al rendimiento como acabamos de ver.</p> <p>Sin embargo, tiene una implicaci\u00f3n directa en la capacidad de almacenamiento. Ve\u00e1moslo.</p> <p>En un cl\u00faster, la capacidad de almacenamiento total viene dado por la suma de la capacidad de todos los discos que hay en los Datanodes. Por ejemplo, en un cl\u00faster de 20 nodos, con 12 discos de 3 terabytes por nodo, tendremos una capacidad de 36 terabytes por nodo, y 720 terabytes en total.</p> <p>Ahora bien, si todos los ficheros de HDFS van a tener un factor de replicaci\u00f3n 3 significar\u00e1 que cada fichero ocupar\u00e1 el triple, al haber 3 copias para cada datos. Esto hace que la capacidad total del cl\u00faster baje hasta 240 terabytes.</p> <p>Adem\u00e1s, cuando calculamos la capacidad real de un cl\u00faster, hay que dejar otro espacio para que las aplicaciones o los usuarios puedan guardar datos parciales de sus operaciones, logs, etc. Normalmente se reserva un 30 o 40% para este prop\u00f3sito, as\u00ed que nuestro cl\u00faster de 20 nodos y 36 terabytes por nodo, tendr\u00e1 una capacidad real de unos 150 terabytes. Sigue siendo una capacidad alta, pero est\u00e1 lejos de los 720 terabytes iniciales.</p> <p>Con esto, podemos afirmar por lo tanto que:</p> <ul> <li>Un factor de replicaci\u00f3n alto:</li> <li>Mejora la tolerancia a fallos.</li> <li>Mejora la velocidad de lectura porque se pueden utilizar m\u00e1s Datanodes para recuperar un bloque.</li> <li>Reduce la velocidad de las escrituras porque cada bloque hay que almacenarlo en m\u00e1s Datanodes.</li> <li> <p>Reduce la capacidad total de almacenamiento de un cl\u00faster.</p> </li> <li> <p>Un factor de replicaci\u00f3n bajo:</p> </li> <li>Incrementa el riesgo de perder alg\u00fan dato si se corrompen los Datanodes que almacenan un bloque.</li> <li>Reduce la velocidad de lectura porque hay que leer cada bloque de uno o pocos Datanodes que lo contienen, y a lo mejor esos Datanodes est\u00e1n ocupados con otras operaciones.</li> <li>Incrementa la velocidad de escritura, al tener que escribir cada bloque en pocos Datanodes.</li> <li>Incrementa (o mejor dicho, reduce menos) la capacidad total de almacenamiento del cl\u00faster.</li> </ul> <p>Con estos puntos enumerados, normalmente se aplican estas reglas para calcular el factor de replicaci\u00f3n \u00f3ptimo:</p> <ul> <li>Para datos temporales, que se van a escribir y quiz\u00e1s no se lean nunca, y que no son cr\u00edticos, el factor de replicaci\u00f3n suele ser bajo (1 \u00f3 2).</li> <li>Para datos cr\u00edticos, que es importante que no se puedan perder, y que suelen ser accedidos muchas veces, como por ejemplo una tabla maestra, el factor de replicaci\u00f3n suele ser alto (incluso teniendo una copia por cada Datanode si es accedida muchas veces y no ocupa mucho).</li> <li>Para el resto de ficheros, se suele dejar el factor de replicaci\u00f3n por defecto.</li> </ul>"},{"location":"UD6%20-%20Apache%20Hadoop/index.html#656-manejo-y-uso-de-hdfs","title":"6.5.6 Manejo y uso de HDFS","text":"<p>Example</p> <p>Para una primera aproximaci\u00f3n y para empezar a familiarizarnos con Apache Hadoop y HDFS usaremos la siguiente imagen de Cloudera (Necesitas pertenecer a IES Gran Capit\u00e1n)</p> <p>HDFS soporta operaciones similares a los sistemas Unix:</p> <ul> <li>Lectura, escritura o borrado de ficheros.</li> <li>Creaci\u00f3n, listado o borrado de directorios.</li> <li>Usuarios, grupos y permisos.</li> </ul> <p>En cuanto a los interfaces con los que poder usar el sistema de ficheros, ofrece diferentes interfaces, siendo los principales los mencionados a continuaci\u00f3n:</p> <ul> <li>Cliente de l\u00ednea de comandos: HDFS dispone de un amplio n\u00famero de comandos que pueden ser ejecutados en consola. </li> <li>Java API: HDFS est\u00e1 escrito en Java de forma nativa y ofrece un API que puede ser utilizado por aplicaciones con el mismo lenguaje.</li> <li>RestFul API(WebHDFS): para poder utilizar HDFS desde otros lenguajes, HDFS ofrece su funcionalidad mediante un servicio HTTP mediante el protocolo WebHDFS. Este interfaz, sin embargo, ofrece un rendimiento inferior al API de Java al utilizar HTTP como capa de transporte, por lo que no deber\u00eda utilizarse para operaciones masivas o con alto volumen de datos.</li> <li>NFS interface (HDFS NFS Gateway): es posible montar HDFS en el sistema de archivos de un cliente local utilizando la puerta de enlace NFSv3 de Hadoop.</li> <li>Librer\u00eda C: HDFS ofrece una librer\u00eda escrita en C, llamada libhdfs, que tiene un buen rendimiento, pero que no suele ofrecer toda la funcionalidad del API Java.</li> </ul> <p>Cliente de l\u00ednea de comandos</p> <p>Una vez dentro del sistema, el comando hadoop fs nos proporcionar\u00e1 todas las funcionalidades sobre HDFS. Si se introduce s\u00f3lo el comando, nos ofrecer\u00e1 la lista de opciones o comandos disponibles. Algunos de los comandos m\u00e1s utilizados son los siguientes:</p> <p>Info</p> <p> Figura6.14_HDFS DFS </p> <p><code>hadoop fs</code> es soportado por cualquier sistema de archivos gen\u00e9rico que puede apuntar a cualquier sistema de archivos como local, HDFS, FTP, S3, etc. En cambio <code>hdfs dfs</code> es exclusivo de HDFS y es el usado en las versiones actuales. </p> <p>Note</p> <p>En la distribuci\u00f3n de cloudera, el sistema de archivos local por defecto est\u00e1 localizado en /home/cloudera y la localizaci\u00f3n por defecto de HDFS es /user/cloudera</p> <p>En el caso concreto de interactuar con el sistema de ficheros de Hadoop se utiliza el comando dfs, el cual requiere de otro argumento (empezando con un guion) el cual ser\u00e1 uno de los comandos Linux para interactuar con el shell. Pod\u00e9is consultar la lista de comandos en la documentaci\u00f3n oficial.</p> <ul> <li> <p>Listar contenidos de un directorio: Para ver los contenidos del directorio HDFS el comando es el siguiente:</p> <p><code>hdfs dfs -ls /user/cloudera</code></p> </li> <li> <p>Crear un directorio: Para crear un nuevo directorio dentro del sistema de ficheros HDFS.</p> <p><code>hdfs dfs -mkdir /user/cloudera/prueba</code></p> </li> </ul> <p>Warning</p> <p>Atenci\u00f3n a los permisos necesarios para crear directorios en diferentes puntos del sistema de archivos</p> <ul> <li> <p>Copiar un fichero del sistema de archivos local al sistema de archivos HDFS. Se podr\u00e1 verificar esa copia mediante el comando     hdfs dfs -ls o mediante la operaci\u00f3n cat que se menciona a continuaci\u00f3n.</p> <p><code>hdfs dfs -copyFromLocal /home/cloudera/fichero /user/cloudera</code></p> <p>Por defecto, el destino de cualquier operaci\u00f3n HJDFS es /user/cloudera, de manera que es opcional especificar esa parte de la ruta salvo que sea diferente a la de por defecto.</p> </li> <li> <p>Visualizaci\u00f3n del contenido de un archivo: Para ver el contenido de un archivo ubicado en el sistema de archivo HDFS la operaci\u00f3n ser\u00e1 la siguiente:</p> <p><code>hdfs dfs -cat /user/cloudera/prueba/fichero</code></p> </li> <li> <p>Extraer un fichero del sistema de archivos HDFS: Con el fin de copiar a nuestros sistema de archivos local un archivo del sistema de archivos de HDFS se utilizar\u00e1 alguno de los siguientes comandos.     <code>hdfs dfs -copyToLocal /user/cloudera/prueba/fichero</code> <code>hdfs dfs -get /user/cloudera/prueba/fichero</code></p> </li> <li> <p>Mover ficheros dentro de HDFS: Para mover ficheros almacenados en HDFS a otros directorios de HDFS se podr\u00eda utilizar el siguiente comando:</p> <p><code>hdfs dfs -mv /user/cloudera/prueba/fichero /user/cloudera/</code> $</p> <p>Se permiten m\u00faltiples or\u00edgenes de ficheros, lo que obliga a que el destino sea un directorio. El movimiento de ficheros entre diferentes sistemas de archivos no est\u00e1 permitido.</p> </li> <li> <p>Copiar ficheros dentro de HDFS: Copia un fichero entre diferentes localizaciones dentro de HDFS. Tambi\u00e9n, como mv permite copiar desde diferentes or\u00edgenes, pero siempre con un directorio de destino final.</p> <p><code>hdfs dfs -cp -f /user/cloudera/prueba/fichero /user/cloudera/</code></p> <p>la opci\u00f3n -f permitir\u00e1 sobreescribir el destino si \u00e9ste existe previamente.</p> </li> <li> <p>Put: Permite copiar uno o varios or\u00edgenes desde el sistema de archivos local al sistema de archivos distribuido.</p> <p><code>hdfs dfs -put /user/cloudera/prueba/fichero /user/cloudera/</code></p> <p>Tambi\u00e9n permite leer desde la entrada est\u00e1ndar (stdin) y escribe en el sistema de archivos destino.</p> <p><code>hdfs dfs -put - /user/cloudera/entrada</code></p> </li> <li> <p>A\u00f1adir contenido al final del fichero: A veces es necesario hacer operaciones de concatenaci\u00f3n de ficheros, etc., para ello existe la operaci\u00f3n appendToFile que permite hacer esta operaci\u00f3n.     <code>hdfs dfs -appendToFile fichero_tail /user/cloudera/fichero</code></p> </li> <li> <p>Mezcla de ficheros: se utiliza para combinar varios archivos (o directorios) del sistema de archivos distribuido y luego ponerlo en un solo archivo de salida en nuestro sistema de archivos local. Dispone de una opci\u00f3n -nl con el fin de a\u00f1adir una nueva l\u00ednea al final de cada fichero.</p> <p><code>hdfs dfs -getmerge -nl file1.txt file2.txt /home/cloudera/output.txt</code></p> </li> <li> <p>Borrado de ficheros: La operaci\u00f3n rm permitir\u00e1 borrar los ficheros especificados como argumentos. Con la opci\u00f3n -R se borrar\u00e1n el directorio y los subdirectorios de forma recursiva.</p> <p><code>hdfs dfs -rm -r /user/cloudera/prueba</code></p> </li> <li> <p>Cambio de permisos a los ficheros: De la misma forma que en Linux la operaci\u00f3n chmod permitir\u00e1 realizar cambios en los permisos de uso de los ficheros. Con la opci\u00f3n -R hace que el cambio se propague recursivamente a trav\u00e9s de la estructura de directorios.</p> <p><code>hdfs dfs -chmod -R 777 /user/cloudera/prueba</code></p> </li> <li> <p>Comprobar uso de disco: Servir\u00e1 para comprobar cuando espacio de disco se est\u00e1 usando en HDFS. Si estamos interesados \u00fanicamente en el uso de disco de nuestro directorio de usuario el comando ser\u00e1:</p> <p><code>hdfs dfs -du</code></p> <p>Si por el contrario queremos conocer cuando espacio de disco est\u00e1 disponible en todo el cluster, el comando ser\u00e1:</p> <p><code>hdfs dfs -df</code></p> </li> <li> <p>Contar n\u00famero de directorios: El siguiente comando permite  obtener la informaci\u00f3n del n\u00famero de directorios , ficheros y tama\u00f1o de los mismos.</p> <p><code>hdfs dfs -count /user/cloudera</code></p> </li> <li> <p>Crea un fichero vac\u00edo:</p> <p><code>hdfs dfs -touchz /user/cloudera/emptyfile</code></p> </li> <li> <p>setrep: Modifica el factor de replicaci\u00f3n de un fichero o un directorio. Ya sabes que el factor de replicaci\u00f3n por defecto es 3. Con este comando se puede modificar para un fichero o directorio concreto.</p> <p><code>hdfs dfs -setrep 6 /user/cloudera/changerepfile</code></p> </li> </ul> <p>Recuerda</p> <p>Recuerda diferencia entre trabajar con HDFS o trabajar con el disco local de la m\u00e1quina en la que tienes abierto un terminal, que suele ser el nodo frontera. Este esquema te permitir\u00e1 ver la diferencia:</p> <p> Figura6.15_Comandos HDFS </p> <p>Cuando accedemos por terminal a una m\u00e1quina, que suele ser la m\u00e1quina frontera, y navegamos por su sistema de ficheros, lo estaremos haciendo sobre el disco o los discos que tiene esa m\u00e1quina. Cuando ejecutamos el comando <code>hdfs dfs</code> , \u00e9ste se ejecutar\u00e1 sobre el sistema de ficheros de HDFS, que es diferente al de la m\u00e1quina en la que estamos.</p> <p>Cuando queremos subir un fichero a HDFS, lo habitual es copiarlo primero en la m\u00e1quina frontera, y posteriormente subirlo a Hadoop con el comando put.</p>"}]}