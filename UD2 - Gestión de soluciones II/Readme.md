# UD 2 - Gesti√≥n de Soluciones II

En este cap√≠tulo se profundizar√° en los almacenes de datos o Data Warehouses (DW) como soluci√≥n propia de la inteligencia de negocio o Business Intelligence (BI). Esta soluci√≥n aparece como respuesta al crecimiento de datos digitales almacenados por las empresas y la necesidad de extraer informaci√≥n y generar conocimiento a partir de ellos para optimizar el proceso de toma de decisiones en cada uno de los departamentos o divisiones de la empresa. Para ello, en este cap√≠tulo se aborda el concepto de almac√©n de datos, su arquitectura as√≠ como su dise√±o e implementaci√≥n.

## 2.1. Sistemas de ayuda a la decisi√≥n

En una empresa u organizaci√≥n, los datos generados a diario son, principalmente, aquellos derivados de las operaciones rutinarias de la empresa. Estos datos, tradicionalmente, se almacenaban en **bases de datos relacionales** y su manipulaci√≥n se correspond√≠a con **transacciones** realizadas sobre la base de datos. Sin embargo, el objetivo de cualquier organizaci√≥n es seleccionar esos datos para realizar estudios y an√°lisis que permitan generar informes que, a su vez, permitan a la empresa **extraer informaci√≥n para tomar decisiones estrat√©gicas** que conduzcan a la organizaci√≥n al √©xito.


El crecimiento exponencial de los datos manejados por una organizaci√≥n ha hecho que los computadores sean las √∫nicas herramientas capaces de procesar estos datos para obtener informaci√≥n y ofrecer ayuda en la toma de decisiones. En este contexto, aparecen los **sistemas de ayuda a la decisi√≥n** o _Decision Support Systems (DSS)_ que ayudan a quienes ocupan puestos de gesti√≥n a tomar decisiones o elegir entre diferentes alternativas. 

>üìÑ **Sistema de ayuda a la decisi√≥n**: Conjunto de t√©cnicas y herramientas tecnol√≥gicas desarrolladas para procesar y analizar datos para ofrecer soporte en la toma decisiones a quienes ocupan puestos de gesti√≥n o direcci√≥n en una organizaci√≥n. Para ello, el sistema combina los recursos de los gestores junto con los recursos computacionales para optimizar el proceso de toma de decisiones.

Mientras que las bases de datos relacionales han sido tradicionalmente el componente del _back-end_ en el dise√±o de sistemas de ayuda a la decisi√≥n, los almacenes de datos se han convertido en una opci√≥n mucho m√°s competitiva como elemento _back-end_ al mejorar el rendimiento de √©stas.

Los **campos de aplicaci√≥n** de los almacenes de datos no se reducen √∫nicamente al √°mbito empresarial, sino que cubren multitud de dominios como las **ciencias naturales, demograf√≠a, epidemiolog√≠a o educaci√≥n, entre otros muchos**. La propiedad com√∫n a todos estos campos y que hace de los almacenes de datos una adecuada soluci√≥n en estos √°mbitos **es la necesidad de almacenamiento y herramientas de an√°lisis que permitan obtener en tiempos razonables informaci√≥n y conocimiento √∫tiles para mejorar el proceso de toma de decisiones**.

_[ver presentaci√≥n BDA2.1](https://moodle.iesgrancapitan.org/pluginfile.php/58143/mod_folder/content/0/BDA_UD2_01.pdf?forcedownload=1)_

## 2.2. Almacenes de datos: Concepto

La aparici√≥n de los **almacenes de datos** est√° ligada, principalmente, a una serie de retos que es necesario abordar para **convertir los datos transaccionales** con los que trabaja una base de datos relacional en **informaci√≥n para generar conocimiento y dar soporte al proceso de toma de decisiones**

   - **Accesibilidad**: Desde cualquier dispositivo, a cualquier tipo de usuario y a gran cantidad de informaci√≥n que no puede ser almacenada de forma centralizada. La accesibilidad, en este sentido, debe hacer frente al problema de la **escalabilidad** del sistema y de los datos que este maneja.
  
   - **Integraci√≥n**: Referente a la gesti√≥n de datos heterog√©neos, con distintos formatos, y provenientes de distintos √°mbitos de la organizaci√≥n. Una correcta integraci√≥n debe garantizar a su vez la correcci√≥n y completitud de los datos integrados.
  
   - **Consultas mejoradas**: Permitiendo incluir operadores avanzados y dar soporte a herramientas y procedimientos que posibiliten obtener el m√°ximo partido de los datos existentes. De este modo, ser√° posible obtener i**nformaci√≥n precisa para realizar un an√°lisis eficiente**.
  
   - **Representaci√≥n multidimensional**: Proporciona herramientas para analizar de forma multi-dimensional los datos del sistema, incluyendo datos de **diferentes unidades** de la organizaci√≥n con el objetivo de proporcionar herramientas de **an√°lisis y visualizaci√≥n multi-dimensional** para mejorar el proceso de toma de decisiones.

A continuaci√≥n, se muestra una **definici√≥n de almac√©n de datos** muy extendida, dada por W. Inmon, quien es conocido por ser el ‚Äúpadre‚Äù del concepto de almac√©n de datos.


>üìÑ **Almac√©n de datos (Data Warehouse)**: Colecci√≥n de datos orientados a temas, integrados, variante en el tiempo y no vol√°til que da soporte al proceso de toma de decisiones de la direcci√≥n.

Para entender correctamente esta definici√≥n, es necesario ahondar en las caracter√≠sticas que incluye la misma.

- **Orientados a temas**: Es decir, no orientado a procesos (transacciones), sino a entidades de mayor nivel de abstracci√≥n como ‚Äúart√≠culo‚Äù o ‚Äúpedido‚Äù.

- **Integrados**: Almacenados en un formato uniforme y consistente, lo que implica depurar o limpiar los datos para poder integrarlos.

- **Variante en el tiempo**: Asociados a un instante de tiempo (mes, trimestre, a√±o...)

- **No vol√°tiles**: Se trata de datos persistentes que no cambian una vez se incluyen en el almac√©n de datos.

El dise√±o y funcionamiento de los almacenes de datos se basa en el sistema de procesamiento anal√≠tico en-l√≠nea, **OLAP**. Este sistema se encarga del an√°lisis, interpretaci√≥n y toma de decisiones acerca del negocio, en contraposici√≥n a los sistemas de procesamiento de transacciones en l√≠nea, **OLTP**.

As√≠ pues, los sistemas **OLTP est√°n dirigidos por la tecnolog√≠a y orientados a automatizar las operaciones del d√≠a a d√≠a** de la organizaci√≥n, mientras que los sistemas **OLAP est√°n dirigidos por el negocio y proporcionan herramientas para tomar decisiones a largo plazo**, mejorando la estrategia y la competitividad de la organizaci√≥n. La tabla 2.1 muestra una comparativa entre las principales caracter√≠sticas de las bases de datos operacionales (OLTP) y los almacenes de datos (OLAP).


| Caracter√≠stica | **BBDD Operacionales(OLTP)** | **Almac√©n Datos(OLAP)** |
| -- | -- | -- |
| **Objetivo** | Depende de la aplicaci√≥n | Toma de decisiones |
| **Usuarios** | Miles | Cientos |
| **Trabajo con...** | Transacciones predefinidas | Consultas y an√°lisis espec√≠ficos |
| **Acceso** | Lectura y escritura a cientos de registros | Principalmente lecutra. Miles de registros |
| **Datos** | Detallados, num√©ricos y alfanum√©ricos | Agregados, principalmente num√©ricos |
| **Integraci√≥n** | En funci√≥n de la aplicaci√≥n | Basados en temas, con mayor nivel de abstracci√≥n |
| **Calidad** | Medida en t√©rminos de integridad | Medida en t√©rminos de consistencia |
| **Temporalidad Datos** | Solo datos actuales | Datos actuales e hist√≥ricos |
| **Actualizaciones** | Continuas | Peri√≥dicas |
| **Modelo** | Normalizado | Desnormalizado, multidimensional |
| **Optimizaci√≥n** | Para acceso OLTP a parte de la BBDD | Para acceso OLAP a gran parte de la BBDD |

_Tabla 2.1. Diferencias entre BBDD Operacionales y Almacenes de Datos_


## 2.3 Almacenes de datos: Arquitectura

Las arquitecturas disponibles para el **dise√±o de almacenes de datos** se basan, principalmente, en garantizar que el sistema cumpla una serie de propiedades esenciales para su √≥ptimo funcionamiento

- **Separaci√≥n**: De los datos transaccionales y los datos estrat√©gicos que sirven como punto de partida a la toma de de decisiones.

- **Escalabilidad**: A nivel hardware y software, para actualizarse y garantizar el correcto funcionamiento del sistema a medida que el n√∫mero de datos y usuarios aumenta.

- **Extensiones**: Permitiendo integrar e incluir nuevas aplicaciones sin necesidad de redise√±ar el sistema completo.

- **Seguridad**: Monitorizando el acceso a los datos estrat√©gicos guardados en el almac√©n de datos.

>üìÑ Las arquitecturas de **almacenes de datos** se clasifican, fundamentalmente, en dos tipos: arquitecturas orientadas a la estructura y arquitecturas orientadas a la empresa.

### 2.3.1 Arquitecturas orientadas a la estructura

Las arquitecturas orientadas a la estructura reciben su nombre debido a que est√°n dise√±adas poniendo especial √©nfasis en el **n√∫mero de capas y elementos que componen la arquitectura del sistema de almac√©n de datos**. De acuerdo con este criterio, es posible distinguir las siguientes arquitecturas.

***Arquitectura de una capa***

El objetivo principal de esta arquitectura, poco utilizada en la pr√°ctica, es **minimizar la cantidad de datos almacenados eliminando para ello los datos redundantes**. La [figura2.1][figura2.1] muestra un esquema de este tipo de arquitectura. En ella, el almac√©n de datos creado es virtual, existiendo un middleware que interpreta los datos operacionales y ofrece una vista multidimensional de ellos.

**El principal inconveniente** de esta arquitectura es que su simplicidad hace que el sistema **no cumpla la propiedad de separaci√≥n**, ya que los procesos de an√°lisis se realizan sobre los datos operacionales.

![figura2.1][figura2.1]

_Figura 2.1. Almac√©n de datos. Arquitectura de una capa._

***Arquitectura de dos capas***

Fue dise√±ada con el objetivo de solucionar el problema de la separaci√≥n que presentaba la arquitectura de una capa. Este esquema consigue **subrayar la separaci√≥n entre los datos disponibles y el almac√©n de datos** a trav√©s de los siguientes componentes (ver [figura2.2][figura2.2):

- **Capa de origen (fuente)**: Se corresponde con los or√≠genes y fuentes de los datos heterog√©neos que se pretenden incorporar al almac√©n de datos.

- **Puesta a punto**: Proceso por el cual se utilizan herramientas de Extracci√≥n, Transformaci√≥n y Carga (ETL) para extraer, limpiar, filtrar, validar y cargar datos en el almac√©n de datos.

- **Capa de almac√©n de datos**: Almacenamiento centralizado de la informaci√≥n en el almac√©n de datos, el cual puede ser utilizado para crear _data marts_ o repositorios de metadatos.

- **An√°lisis**: Conjunto de procesos a partir de los cuales los datos son eficientemente y flexiblemente analizados, generando informes y simulando escenarios hipot√©ticos para dar soporte a la toma de decisiones.

>üìã **Data mart** es un subconjunto o agregaci√≥n de los datos almacenados en un almac√©n de datos primario que incluye informaci√≥n relevante sobre un √°rea espec√≠fica del negocio.


![figura2.2][figura2.2]

_Figura 2.2. Almac√©n de datos. Arquitectura de dos capas._

***Arquitectura de tres capas***

Este tercer tipo de arquitectura incluye una capa llamada de **datos reconciliados** o almac√©n de datos operativos. Con esta capa, los datos operativos obtenidos tras la limpieza y depuraci√≥n son integrados y validados, proporcionando un modelo de datos de referencia para toda la organizaci√≥n.

De este modo, **el almac√©n de datos no se nutre de los datos de origen directamente, sino de los datos reconciliados generados**, los cuales tambi√©n son utilizados para realizar de forma m√°s eficiente tareas operativas, como la realizaci√≥n de informes o la alimentaci√≥n de datos a procesos operativos.

Esta capa de datos reconciliados tambi√©n puede implementarse de forma virtual en una arquitectura de dos capas, ya que se define como una vista integrada y coherente de los datos de origen. La [figura2.3][figura2.3] muestra de forma gr√°fica este tipo de arquitectura

![figura2.3][figura2.3]

_Figura 2.3. Almac√©n de datos. Arquitectura de tres capas._

### 2.3.2. Arquitecturas orientadas a la empresa

Esta clasificaci√≥n distingue **cinco tipos de arquitecturas** que combinan las capas mencionadas en la primera clasificaci√≥n para dise√±ar almacenes de datos.


***1. Arquitectura de data marts independientes***

Arquitectura preliminar en la que **los distintos data marts son dise√±ados de forma independiente y construidos de forma no integrada**. Suele utilizarse en los inicios de implementaci√≥n de proyectos de almacenes de datos y reemplazada a medida que el proyecto va creciendo.


***2. Arquitectura en bus***

Similar a la anterior, **asegura la integraci√≥n l√≥gica de los data marts creados**, ofreciendo una visi√≥n amplia de los datos de la empresa y permitiendo realizar an√°lisis rigurosos de los procesos que en ella se llevan a cabo.


***3. Arquitectura hub-and-spoke (centro y radio)***

Esta arquitectura es **muy utilizada en almacenes de datos de tama√±os medio y grande**. Su dise√±o pone especial √©nfasis en garantizar la escalabilidad del sistema y permitir a√±adir extensiones al mismo.

Para ello, **los datos se almacenan de forma at√≥mica y normalizada en una capa de datos reconciliados que alimenta a los data marts** construidos que contienen, a su vez, los datos agregados de forma multidimensional. Los usuarios acceden a los data marts, si bien es cierto que tambi√©n pueden hacer consultas directamente sobre los datos reconciliados.


***4. Arquitectura centralizada***

Se trata de un caso particular de la arquitectura hub-and-spoke. En ella, **la capa de datos reconciliados y los data marts se almacenan en un √∫nico repositorio f√≠sico**.


***5. Arquitectura federada***

Se trata de un tipo de arquitectura **muy utilizada en entornos din√°micos, cuando se pretende integrar almacenes de datos o data marts existentes con otros para ofrecer un entorno √∫nico e integrado de soporte a la toma de decisiones**. De esta forma, cada almac√©n de datos y cada data mart es integrado virtual o f√≠sicamente con lo dem√°s. Para ello, se utilizan una serie de t√©cnicas y herramientas avanzadas como son las ontolog√≠as, consultas distribuidas e interoperabilidad de metadatos, entre otras.

### 2.4. Almacenes de datos: Dise√±o e implementaci√≥n

En esta secci√≥n se profundizar√° en cada una de las **etapas necesarias para dise√±ar e implementar un almac√©n de datos**. Para ello, en primer lugar se describir√° el proceso de **Extracci√≥n, Transformaci√≥n y Carga (ETL)**, fundamental para construir y alimentar un almac√©n de datos. Despu√©s, se desarrollar√°n dos de los dise√±os m√°s extendidos de almac√©n de datos: el **dise√±o en estrella** y el **dise√±o en copo de nieve**.

_[ver presentaci√≥n BDA2.2](https://moodle.iesgrancapitan.org/pluginfile.php/58143/mod_folder/content/0/BDA_UD2_02.pdf?forcedownload=1)_

### 2.4.1 El proceso de Extracci√≥n, Transformaci√≥n y Carga (ETL)

El proceso ETL es el encargado de extraer, limpiar e integrar los datos provenientes de las fuentes de datos para alimentar el almac√©n de datos. Este proceso tambi√©n es el encargado de alimentar la capa de datos reconciliados en la arquitectura de tres capas. El proceso ETL tiene lugar cuando se puebla el almac√©n de datos y se lleva a cabo cada vez que el almac√©n de datos se actualiza. A continuaci√≥n, se describen detalladamente cada una de las fases de las que consta este proceso


**‚¨ÜÔ∏è Extracci√≥n**

Etapa que consiste en la **lectura de los datos de las distintas fuentes de las que provienen**. Cuando un almac√©n de datos se rellena por primera vez, se suele utilizar la t√©cnica de **extracci√≥n est√°tica**, la cual consiste en extraer una instant√°nea de los datos operacionales. A partir de entonces, se utiliza la **extracci√≥n incremental** para actualizar peri√≥dicamente los datos del almac√©n de datos, recogiendo los cambios aplicados desde la √∫ltima extracci√≥n. Para ello, se utiliza el registro mantenido por el SGBD que, por ejemplo, asocia una **marca de tiempo (timestamp)** a los datos operacionales para registrar cuando fueron modificados y agilizar el proceso de extracci√≥n.

En la actualidad, existe una gran cantidad de conjuntos de datos o _data sets_ p√∫blicos, conocidos bajo el nombre de Open Data, que abarcan una gran cantidad de dominios y con los que es posible trabajar para construir soluciones big data.  

>üìã **Open Data**: Se trata de datos que han sido generados por una fuente en particular, que abarcan un dominio tem√°tico o disciplinar y tienen atributos, dentro de los cuales est√° la frecuencia de actualizaci√≥n. Adem√°s, cuentan con una licencia espec√≠fica que indica las condiciones de reutilizaci√≥n de los mismos.

**La fuente de los datos es en muchos de los casos el estado** nacional, provincial, municipal u organizaciones comerciales. En otras ocasiones, **la fuente de los datos es fruto del estudio o medici√≥n por parte de particulares**. Los **atributos** de los conjuntos de datos deben especificar c√≥mo fueron obtenidos, incluyendo fechas de obtenci√≥n, actualizaci√≥n y validez, as√≠ como el p√∫blico involucrado, la metodolog√≠a de recogida o muestreo, etc.

Algunas de las fuentes m√°s utilizadas en la actualidad para la obtenci√≥n de datos abiertos provienen de **los centros nacionales e internacionales de estad√≠sticas**, como son el Instituto Nacional de Estad√≠stica de Espa√±a (INE) [^1] , eurostat [^2] , la oficina europea de estad√≠sticas, la Organizaci√≥n Mundial de la Salud (OMS) [^3] ... Por otra parte, existen **repositorios p√∫blicos de datos abiertos y a disposici√≥n de los usuarios como UCI4 [^4] o Kaggle [^5]** , que es una comunidad de cient√≠ficos de datos donde empresas y organizaciones suben sus datos y plantean problemas que son resueltos por los miembros de la comunidad. La figura 2.4 muestra las fuentes de datos que se acaban de mencionar.

![figura2.4][figura2.4]

_Figura 2.4: Fuentes para la obtenci√≥n de datos abiertos_

[^1]: [https://www.ine.es](https://www.ine.es)
[^2]: [https://ec.europa.eu/eurostat](https://ec.europa.eu/eurostat)
[^3]: [https://www.who.int/es/data/gho/publications/world-health-statistics](https://www.who.int/es/data/gho/publications/world-health-statistics)
[^4]: [https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)
[^5]: [https://www.kaggle.com](https://www.kaggle.com)

_[ver presentaci√≥n BDA2.3](https://moodle.iesgrancapitan.org/pluginfile.php/58143/mod_folder/content/0/BDA_UD2_03.pdf?forcedownload=1)_

**üîÑ Transformaci√≥n**

La etapa de transformaci√≥n es la fase clave para **transformar los datos operativos en datos con un formato espec√≠fico para alimentar un almac√©n de datos**. En esta etapa, **los datos se limpian y se transforman, a√±adi√©ndoles contexto y significado**. En caso de implementar un almac√©n de datos siguiendo una arquitectura de tres capas, **el proceso de transformaci√≥n es el encargado de obtener la capa de datos reconciliados**. Si bien es cierto que algunos autores separan la limpieza y la transformaci√≥n de los datos en dos etapas distintas, en este cap√≠tulo se considerar√°n ambas dentro de la fase de transformaci√≥n.

> üìÑ La etapa de **transformaci√≥n** engloba todos los procesos de limpieza y manipulaci√≥n de los datos, con el objetivo de transformar los datos operativos propios de sistemas relacionales (OLTP) en datos preparados para ser incluidos dentro del almac√©n de datos (OLAP).

La **limpieza de los datos** o _data cleaning_ engloba todos aquellos procedimientos necesarios para detectar y resolver situaciones problem√°ticas con los datos de partida que pudieran suponer problemas potenciales a la hora de analizarlos. As√≠ pues, los datos de partida pueden ser **incompletos**, es decir, pueden contener atributos sin valor o valores agregados, **incorrectos** que incluyan errores o valores sin ning√∫n significado, lo cual es com√∫n cuando los datos se introducen manualmente en el sistema, o inconsistentes cuando los cambios no son propagados a todos los m√≥dulos del sistema, los rangos de un determinado atributo son cambiantes, existen datos duplicados...


Otra tarea muy importante que se debe abordar en la fase de limpieza es la gesti√≥n de los **datos perdidos**. Se trata de **datos que no est√°n disponibles para algunas de las variables en alguna instancia**. Esto puede ser debido, por ejemplo, a que no se registraran las modificaciones sufridas por los datos, se produjera un mal funcionamiento del equipo, los datos nunca fueron rellenados o no exist√≠an en el momento en que fueron completados, se eliminaron por ser inconsistentes con la informaci√≥n registrada...

En general, se distinguen **dos tipos de situaciones cuando existen valores perdidos: datos perdidos completamente aleatorios y datos perdidos de forma no completamente aleatoria**. En el segundo caso, puede ser interesante intentar analizar la raz√≥n de la p√©rdida de los datos, la cual puede ser indicativa. En muchas ocasiones, los valores perdidos tienen relaci√≥n con un subconjunto de variables predictoras y no se encuentran aleatoriamente distribuidos por todas ellas. Por todo ello, las aproximaciones m√°s comunes a la hora de gestionar datos perdidos son:

- **Eliminaci√≥n de instancias que contengan valores perdidos**: consiste en establecer un porcentaje umbral de tal forma que, si una instancia contiene un n√∫mero de valores perdidos que supera este porcentaje, la instancia se descarta. Se trata de una t√©cnica √∫til cuando el conjunto de datos es grande y el n√∫mero de instancias con valores perdidos no es muy elevado. No obstante, esta estrategia se debe utilizar con precauci√≥n, ya que puede suponer una excesiva p√©rdida de informaci√≥n si el conjunto de datos no es muy grande, el n√∫mero de instancias con valores perdidos es alto o el porcentaje umbral es muy peque√±o.

- **Asignaci√≥n de valores fijos**: consiste en asignar un valor fijo a todos los valores perdidos de todas las variables. Este valor puede ser el n√∫mero 0 o incluso un valor desconocido _Unknown_ o no num√©rico (NaN, Not a Number) en funci√≥n del lenguaje de programaci√≥n utilizado.

- **Asignaci√≥n de valores de referencia**: asigna un valor de referencia a los valores perdidos para cada variable. Estos valores de referencia suelen ser medidas de centralizaci√≥n como la media o la mediana de los valores de cada variable.

- **Imputaci√≥n de valores perdidos**: consiste en la aplicaci√≥n de t√©cnicas m√°s sofisticadas, como pueden ser t√©cnicas estad√≠sticas o de aprendizaje autom√°tico para predecir o averiguar los valores que se han perdido.

Finalmente, la etapa de limpieza de datos tambi√©n se encarga de la **detecci√≥n de valores an√≥malos o outliers**. Se trata de valores que se han introducido de forma err√≥nea o bien a una deformaci√≥n en la distribuci√≥n de valores. El proceso de detecci√≥n de anomal√≠as consiste, fundamentalmente, en dos etapas: en primer lugar, **asumir que existe un modelo generador de datos**, como podr√≠a ser una distribuci√≥n de probabilidad. En segundo lugar, considerar que **las anomal√≠as representan un modelo generador distinto**, que no coincide con el original. Existen multitud de t√©cnicas para detectar y descartar o imputar valores an√≥malos, como lo son t√©cnicas estad√≠sticas basadas en la desviaci√≥n y el rango intercuart√≠lico o t√©cnicas de aprendizaje autom√°tico.

_[ver presentaci√≥n BDA2.4](https://moodle.iesgrancapitan.org/pluginfile.php/58143/mod_folder/content/0/BDA_UD2_04.pdf?forcedownload=1)_

Despu√©s de la etapa de limpieza, comienza la etapa de **transformaci√≥n/manipulaci√≥n**. En ella, los datos se preparan para su carga en el almac√©n de datos. Para ello, se transformar√°n los datos provenientes de sistemas transaccionales OLTP en datos preparados para su an√°lisis OLAP. A continuaci√≥n, se muestran algunos de los procesos de transformaci√≥n m√°s comunes:

- **Estandarizaci√≥n de c√≥digos y formatos de representaci√≥n**: incluye tareas como transformar la informaci√≥n EBCDIC a formato ASCII o Unicode, convertir n√∫meros cardinales en ordinales o viceversa, homogeneizaci√≥n y tratamiento de fechas, expandir codificaciones a√±adiendo textos descriptivos, unificaci√≥n de c√≥digos (sexo, estado civil, etc) y est√°ndares (medidas, monedas, etc). 

- **Conversiones y combinaciones de campos**: realizaci√≥n de agregaciones y c√°lculos simples (importe neto, beneficio, realizaci√≥n de cuentas y promedios), c√°lculos derivados con valores num√©ricos y textuales... 

- **Correcciones**: en caso de que no se pudiera hacer en el origen o en la etapa de limpieza, se deben corregir errores tipogr√°ficos, datos sin sentido ni significado, resolver conflictos de dominio de variables, aclaraci√≥n de datos ambiguos y asignaci√≥n de valores a datos nulos.

- **Integraci√≥n de varias fuentes**: implica la resoluci√≥n de claves y utilizaci√≥n de diccionarios de datos y repositorios de metadatos para el dise√±o del almac√©n de datos.

- **Eliminaci√≥n de datos y/o registros duplicados**: frecuente cuando se trabaja con distintas fuentes de datos provenientes de diferentes dimensiones o departamentos de la organizaci√≥n. Para ello, es muy √∫til el uso de funciones de b√∫squeda y agrupamiento aproximado.

- **Escalado y centrado**: para reducir los efectos adversos de contar con datos dispersos, es muy √∫til utilizar t√©cnicas para escalar y centrar los datos. Una posible transformaci√≥n consiste en restar a cada valor de cada instancia el valor medio de la variable correspondiente y despu√©s dividir los valores por la desviaci√≥n est√°ndar. De esta forma, los valores se escalan y se centran entorno al cero. Esta transformaci√≥n tambi√©n puede hacerse entorno al valor de la media o la mediana. Tambi√©n es posible escalar los valores en el intervalo [0‚àí1] o aplicar el logaritmo, para reducir la dispersi√≥n de los datos y mejorar la visualizaci√≥n.

- **Discretizaci√≥n**: la aplicaci√≥n de muchas t√©cnicas de aprendizaje autom√°tico requiere que los valores de las variables sean discreto, en lugar de continuos. La discretizaci√≥n es el proceso por el cual se divide un intervalo de valores continuos en tantos fragmentos como etiquetas o valores discretos se quieran asignar, sustituyendo los valores originales por la etiqueta o valor discreto correspondiente.

- **Selecci√≥n de caracter√≠sticas**: permite reducir el coste computacional de trabajar con todas las variables del conjunto de datos cuando, en muchas ocasiones, existen variables que no aportan informaci√≥n para el aprendizaje. Adem√°s de reducir el coste computacional, se reduce el n√∫mero de dimensiones del conjunto de datos, mejorando la visualizaci√≥n y mejorando el rendimiento de los modelos no solo en t√©rminos de tiempo, sino tambi√©n de rendimiento, puesto que existen m√©todos de aprendizaje cuyo rendimiento es peor cuando se ejecutan utilizando variables no significativas.

_[ver presentaci√≥n BDA2.5](https://moodle.iesgrancapitan.org/pluginfile.php/58143/mod_folder/content/0/BDA_UD2_05.pdf?forcedownload=1)_

**üîÄ Carga**

Se trata de la √∫ltima fase de cara a incluir datos en el almac√©n de datos. **La carga inicial de los datos puede requerir bastante tiempo al cargar de forma progresiva todos los datos hist√≥ricos**, por lo que es normal realizarla en horas de baja carga de los sistemas. Una vez que el almac√©n de datos ha sido inicialmente cargado, las sucesivas cargas de datos se pueden realizar de dos formas:

- **Refresco**: El almac√©n de datos se reescribe completamente, de forma que se reemplazan los datos antiguos. El refresco **es utilizado habitualmente junto con la extracci√≥n est√°tica** y suele ser una estrategia muy utilizada para **la carga inicial** del almac√©n de datos, aunque puede tambi√©n realizarse a posteriori.

- **Actualizaci√≥n**: **Se a√±aden al almac√©n de datos solamente aquellos datos nuevos que se pretenden incluir, sin eliminar ni modificar los datos ya existentes**. Esta t√©cnica es **utilizada frecuentemente junto con la extracci√≥n incremental** para actualizar regularmente el almac√©n de datos. Se trata de una estrategia de carga **compleja de dise√±ar**, ya que requiere que sea eficiente computacionalmente. Para ello, se requieren mecanismos de detecci√≥n del cambio como aquellos basados en la marca de tiempo (timestamp) o la utilizaci√≥n de tablas de log, entre otros.

_[ver presentaci√≥n BDA2.6](https://moodle.iesgrancapitan.org/pluginfile.php/58143/mod_folder/content/0/BDA_UD2_06.pdf?forcedownload=1)_
 
**‚¨ÜÔ∏èüîÑüîÄ Frameworks ETL**

Recientemente, han aparecido m√∫ltiples _frameworks_ que ofrecen herramientas tecnol√≥gicas para dar un soporte integrado al proceso ETL. A continuaci√≥n, se describen algunos de los m√°s utilizados:

- **Bubbles [http://bubbles.databrewery.org](http://bubbles.databrewery.org)**: Se trata de un framework ETL escrito en Python, aunque es posible utilizarlo desde otros lenguajes. Ofrece un marco de trabajo sencillo, donde el proceso ETL se describe como una secuencia de nodos conectados que representan los datos y las distintas operaciones que se pueden realizar con ellos. De esta forma, es posible construir un grafo que implemente el proceso ETL completo para un conjunto de datos.

- **Apache Camel [http://bubbles.databrewery.org](https://camel.apache.org)**: Este framework escrito en lenguaje Java y de acceso abierto se enfoca en facilitar la integraci√≥n entre distintas fuentes de datos, haciendo el proceso m√°s accesible a los desarrolladores, ofreciendo disitntas herramientas para dar soporte al proceso ETL.

- **Keetle [https://help.hitachivantara.com/Documentation/Pentaho/9.3/Products/Pentaho_Data_Integration](https://help.hitachivantara.com/Documentation/Pentaho/9.3/Products/Pentaho_Data_Integration)**: Es el framework de Pentaho para dar soporte al proceso ETL. Pentaho es una suite o conjunto de programas para construir soluciones de inteligencia de negocio y Keetle es su entorno de trabajo ETL. Similar a Bubbles, ofrece un entorno de trabajo sencillo para implementar un proceso ETL a trav√©s de nodos y conexiones entre ellos. Adem√°s, Keetle es de acceso abierto.

### 2.4.2 Dise√±o en Estrella

A la hora de dise√±ar un almac√©n de datos, existen dos alternativas ampliamente utilizadas: el **dise√±o en estrella**, que promueve el dise√±o directo de estructuras l√≥gicas sobre el modelo relacional, y el **dise√±o en copo de nieve**, como variante del dise√±o en estrella.

El proceso de desarrollo de un almac√©n de datos siguiendo el dise√±o en estrella puede estructurarse seg√∫n las siguientes fases:

1. **Elegir un proceso de negocio a modelar**: cualquier proceso de negocio puede ser modelado como un almac√©n de datos. No obstante, ser√°n de especial inter√©s aquellos procesos **necesarios para la toma de decisiones y que involucran a diferentes unidades de la organizaci√≥n**.

2. **Escoger la granularidad del proceso de negocio**: Los datos almacenados en el almac√©n de datos **deben expresarse siempre al mismo nivel de detalle**. Por este motivo es necesario escoger un nivel de granularidad al comienzo del dise√±o del almac√©n de datos.

3. **Selecci√≥n de medidas/hechos**: indican **qu√© se necesita medir o evaluar enel proceso de negocio** con el fin de dar respuesta a las necesidades de informaci√≥n y toma de decisiones por las cuales se pretende dise√±ar el almac√©nde datos. Por ejemplo, en el √°mbito log√≠stico las medidas podr√≠an ser las unidades aceptadas odevueltas, mientras que en el √°mbito del control de calidad podr√≠an ser lasunidades producidas, defectuosas, costo de la producci√≥n.

4. **Elegir las dimensiones que se aplicar√°n a cada medida o hecho**: las dimensiones especifican **las distintas propiedades de las medidas o hechos que se pretenden almacenar e integrar dentro del almac√©n de datos**. Por ejemplo, si las medidas seleccionadas est√°n relacionadas con las ventas de una determinada empresa, las dimensiones podr√≠an ser: fecha de la venta, vendedor ,cliente, producto...

A la hora de dise√±ar el almac√©n de datos siguiendo el dise√±o en estrella, se crea una **tabla central, tambi√©n llamada tabla de hechos, tabla factual o fact table**. Esta tabla es la que posee los datos (medidas o hechos) sobre las diferentes combinaciones de las dimensiones. En algunas ocasiones, un dise√±o en estrella presenta m√°s de una tabla de hechos. Los hechos introducidos pueden ser de distintos tipos: **completamente aditivos** (total de ventas de un departamento), **no aditivos** (m√°rgen de beneficios expresado como porcentaje) o **semiaditivos** (como datos intermedios que pueden agregarse con datos de otras dimensiones).

Rodeando a la tabla de hechos, se encuentra u**na tabla por cada una de las dimensiones definidas**. La clave primaria de la tabla de hechos se crea combinando las claves primarias de sus dimensiones relacionadas, de forma que est√° compuesta por las claves ajenas de las tablas de dimensiones que relaciona. De esta forma, la tabla de hechos presenta una relaci√≥n del tipo ‚Äúmuchos a muchos‚Äù entre todas las tablas de dimensiones que relaciona, a la vez que una relaci√≥n ‚Äúmuchos a uno‚Äù con cada tabla de dimensi√≥n por separado. Este esquema con forma de estrella da nombre a este tipo de dise√±o.

**Ejemplo**

Sea un almac√©n de datos en el que se pretende almacenar **las ventas llevadas a cabo en una organizaci√≥n**. La tabla central de hechos representa los datos de cada venta, mientras que las dimensiones que rodean a la tabla central recogen datos sobre los productos vendidos, la fecha de la venta, el canal de distribuci√≥n y el lugar de entrega. La [figura2.5][figura2.5] muestra un ejemplo de este esquema de almac√©n de datos.

![figura2.5][figura2.5]

_Figura 2.5: Ejemplo de almac√©n de datos dise√±ado en estrella._

**Ventajas e incovenientes**

Entre las **ventajas del dise√±o en estrella**, destaca ser un dise√±o **f√°cil de modificar que proporciona tiempos r√°pidos de respuesta**, simplificando la navegaci√≥n de los medatadatos. Adem√°s, este dise√±o **permite simular vistas de los datos que obtendr√°n los usuarios finales y facilita la interacci√≥n con otras herramientas**.

Sin embargo, el dise√±o en estrella presenta algunos **inconvenientes** que surgen, por ejemplo, cuando **se combinan dimensiones con distintos niveles de detalle**. Adem√°s, cuando se pretende limitar los niveles de una dimensi√≥n se debe a√±adir un campo de nivel o utilizar un modelo de tipo constelaci√≥n, donde se incluyen diferentes estrellas que almacenan tablas de hechos agregadas, lo cual **a√±ade complejidad al esquema**. 

**Resumen**

üìÑ El **dise√±o en estrella** permite crear un almac√©n de datos con una estructura centralizada. El dise√±o se compone de una tabla de hechos central, que recoge los valores de las medidas del proceso de negocio que se pretende modelar. Rodeando a la tabla de hechos, se incluyen tantas tablas como dimensiones se hayan especificado en el modelo, las cuales se relacionan entre s√≠ a trav√©s de la tabla de hechos.

_[ver presentaci√≥n BDA2.7](https://moodle.iesgrancapitan.org/pluginfile.php/58143/mod_folder/content/0/BDA_UD2_07.pdf?forcedownload=1)_

### 2.4.3 Dise√±o en Copo de Nieve

Otro de los dise√±os m√°s extendidos a la hora de implementar un almac√©n de datos es el **dise√±o en copo de nieve**. De hecho, el dise√±o de copo de nieve es considerado una variante o derivado del dise√±o en estrella y, por tanto, puede construirse a partir de este siguiendo las mismas etapas que se describieron en la secci√≥n anterior. 

Este dise√±o es muy utilizado cuando se dispone de **tablas de dimensi√≥n con una gran cantidad de atributos**. En esta circunstancia, el dise√±o de copo de nieve permite **normalizar las tablas de dimensi√≥n**, ofreciendo un mejor rendimiento cuando las consultas realizadas sobre el almac√©n de datos requieren del uso de operadores de agregaci√≥n. De esta forma, la tabla de hechos deja de ser la √∫nica tabla que presenta relaciones con las dem√°s, apareciendo nuevas relaciones que se dan entre las tablas normalizadas que componen las tablas de dimensiones. Este esquema, que incluye ramificaciones en cada dimensi√≥n que se corresponden con las tablas necesarias para su normalizaci√≥n, guarda un gran parecido con la estructura de un copo de nieve, lo cual da nombre a este dise√±o.

**La diferencia entre los dise√±os en estrella y copo de nieve radica fundamentalmente en la modelizaci√≥n de las tablas de dimensiones**. Para obtener un dise√±o en copo de nieve, basta con partir de un dise√±o en estrella en el que, tras un proceso de normalizaci√≥n, se obtienen varias tablas de dimensi√≥n a partir de una tabla desnormalizada. Dentro del dise√±o de copo de nieve, es posible distinguir entre **copo de nieve parcial**, en el que solo algunas de las dimensiones son normalizadas y **copo de nieve completo**, en el que todas las dimensiones del esquema son normalizadas.

Siguiendo con el ejemplo anterior, en el que se mostraba un almac√©n de datos para representar las ventas realizadas por una organizaci√≥n, la [figura2.6][figura2.6] muestra un dise√±o para un almac√©n de datos siguiendo un esquema de copo de nieve parcial, en el que las dimensiones geogr√°fica y producto han sido normalizadas. 

![figura2.6][figura2.6]

_Figura 2.6: Ejemplo de almac√©n de datos dise√±ado en copo de nieve._

**Ventajas e Inconvenientes**

La normalizaci√≥n de las tablas de dimensiones proporciona al dise√±o de copo de nieve la **mejora de la eficiencia en la realizaci√≥n de consultas complejas que requieren el uso de operadores de agregaci√≥n**. Sin embargo, este dise√±o es conceptualmente m√°s dif√≠cil de implementar, ya que **incluye un mayor n√∫mero de tablas** y requiere de realizar muchos joins entre ellas, lo que **incrementa los tiempos de consulta**.


En ocasiones, se requiere dise√±ar un almac√©n de datos que contenga m√°s de una tabla de hechos. Para ello, el **esquema o dise√±o en constelaci√≥n** permite crear un almac√©n de datos de forma similar al dise√±o en estrella, con la diferencia de que este modelo incluye m√°s de una tabla de hechos. Adem√°s, cada tabla de hechos puede vincularse con todas o solo algunas tablas de dimensiones. Este esquema contribuye a la reutilizaci√≥n de las tablas de dimensiones, las cuales se pueden relacionar con m√°s de una tabla de hechos.

**Resumen**

üìÑ El **dise√±o en copo de nieve** permite crear un almac√©n de datos a partir de un dise√±o en estrella. Para ello, las tablas de dimensiones se normalizan, generando m√°s de una tabla para cada una de las dimensiones, mejorando la eficiencia de consultas complejas que requieren el uso de operadores avanzados.

_[ver presentaci√≥n BDA2.8](https://moodle.iesgrancapitan.org/pluginfile.php/58143/mod_folder/content/0/BDA_UD2_08.pdf?forcedownload=1)_




[figura2.1]: images/Figura2.1_Almac√©n%20de%20datos.%20Arquitectura%20de%20una%20capa.png "Figura 2.1: Almac√©n de datos. Arquitectura de una capa."
[figura2.2]: images/Figura2.2_Almac√©n%20de%20datos.%20Arquitectura%20de%20dos%20capas.png "Figura 2.1: Almac√©n de datos. Arquitectura de una capa."
[figura2.3]: images/Figura2.3_Almac√©n%20de%20datos.%20Arquitectura%20de%20tres%20capas.png "Figura 2.1: Almac√©n de datos. Arquitectura de una capa."
[figura2.4]: images/Figura2.4_Fuentes%20para%20la%20obtenci√≥n%20de%20datos%20abiertos.jpg "Fuentes para la obtenci√≥n de datos abiertos.jpg"
[figura2.5]: images/Figura2.5_Ejemplo%20de%20almac√©n%20de%20datos%20dise√±ado%20en%20estrella.jpg "Figura2.5_Ejemplo de almac√©n de datos dise√±ado en estrella.jpg"
[figura2.6]: images/Figura2.6_Ejemplo%20de%20almac√©n%20de%20datos%20dise√±ado%20en%20copo%20de%20nieve.jpg "Figura2.6_Ejemplo de almac√©n de datos dise√±ado en copo de nieve.jpg"





